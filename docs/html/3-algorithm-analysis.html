<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Cliff Shaffer" />
  <meta name="author" content="Peter Ljunglöf" />
  <meta name="author" content="Nick Smallbone" />
  <title>DSABook – Algorithm Analysis</title>
  <link rel="stylesheet" href="../css/main.css" />
  <link rel="stylesheet" href="../css/book.css" />
  <link rel="stylesheet" href="../css/code.css" />
  <link rel="stylesheet" href="../css/quiz.css" />
  <link rel="stylesheet" href="../css/mobile.css" />
  <link rel="stylesheet" href="../css/print.css" />
  
  <link rel="stylesheet" href="../lib/JSAV.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaMOD.css" type="text/css" />
  <link rel="stylesheet" href="../lib/jquery.ui.min.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaStyle.css" type="text/css" />
  <link rel="stylesheet" href="../lib/ChalmersGU-interactive.css" type="text/css" />

  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:    './',
      VERSION:     '0.4.1',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
      },
      "HTML-CSS": {
        scale: "80",
      }
    });
  </script>

  <script type="text/javascript" src="../lib/jquery.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.migrate.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/javascript" src="../lib/localforage.min.js"></script>
  <script type="text/javascript" src="../lib/accessibility.js"></script>
  <script type="text/javascript" src="../lib/jquery.ui.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.transit.js"></script>
  <script type="text/javascript" src="../lib/raphael.js"></script>
  <script type="text/javascript" src="../lib/JSAV.js"></script>
  <script type="text/javascript" src="../lib/config.js"></script>
  <script type="text/javascript" src="../lib/timeme.js"></script>
  <script type="text/javascript" src="../lib/odsaUtils.js"></script>
  <script type="text/javascript" src="../lib/odsaMOD.js"></script>
  <script type="text/javascript" src="../lib/d3.min.js"></script>
  <script type="text/javascript" src="../lib/d3-selection-multi.v1.min.js"></script>
  <script type="text/javascript" src="../lib/dataStructures.js"></script>
  <script type="text/javascript" src="../lib/conceptMap.js"></script>

  <script>
    ODSA.SETTINGS.MODULE_SECTIONS = [
    'internal-variables', 
    'getting-and-setting-values', 
    'adding-elements', 
    'add-practice-exericse', 
    'removing-elements', 
    'remove-practice-exericise', 
    'static-array-based-list-summary-questions', 
    'static-array-based-list:-full-code',
    ];
    ODSA.SETTINGS.MODULE_NAME = "DSABook";
    ODSA.SETTINGS.MODULE_LONG_NAME = "Data Structures and Algorithms";
    JSAV_OPTIONS['lang']='en';
    JSAV_EXERCISE_OPTIONS['code']='pseudo';
  </script>

  
  <script type="text/javascript" src="../scripts/quizhandler.js"></script>
</head>

<body>

<header>
<nav class="sitenav">
<div class="navlink"></div>
<h1 class="navlink navbutton"><a href="index.html" accesskey="t" rel="top">Data Structures and Algorithms</a></h1>
<div class="navlink"></div>
</nav>
<nav class="sitenav">
<div class="navlink">
<a href="2.15-sorting-summary-exercises.html" class="navbutton">&lt;&lt;</a>
<a href="2.15-sorting-summary-exercises.html" accesskey="p" rel="previous">Sorting: Summary Exercises</a>
</div>
<div class="navlink">
<a href="3.1-problems-algorithms-and-programs.html" accesskey="n" rel="next">Problems, Algorithms, and Programs</a>
<a href="3.1-problems-algorithms-and-programs.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</header>

<main>
<section id="algorithm-analysis" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span>
Algorithm Analysis</h1>
<p>How long will a program take when I run it on a dataset ten times as
large? If a particular program is slow, is it badly implemented or is it
solving a hard problem? What order of improvement can I expect if I
switch to a better algorithm? Questions like these ask us to consider
the difficulty of a problem and the efficiency of approaches to solving
it.</p>
<p>This chapter introduces the motivation, basic notation, and
fundamental techniques of algorithm analysis. We focus on a methodology
known as <a href="10-glossary.html#asymptotic-algorithm-analysis"
class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">asymptotic
algorithm analysis</a>, or simply <a
href="10-glossary.html#asymptotic-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">asymptotic
analysis</a>.</p>
<p>Asymptotic analysis estimates the resource consumption of an
algorithm, called its <a href="10-glossary.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>.
Here, resource consumption can mean runtime, memory use, API calls, or
any other measure. Instead of computing this resource consumption
exactly, asymptotic analysis is only interested in its <a
href="10-glossary.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-Oh notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> (also called <a href="10-glossary.html#order-of-growth"
class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-Oh notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">order
of growth</a>). The growth rate is what determines the resource
consumption for large inputs. Thankfully, growth rate expressions are
relatively easy to compare. This allows us decide which of two
algorithms is better at solving the same problem. Asymptotic analysis
also gives algorithm designers a tool for estimating whether a proposed
solution is likely to meet the resource constraints for a problem before
they implement an actual program.</p>
<p>After reading this chapter, you should understand:</p>
<ul>
<li>The concept of <a href="10-glossary.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an algorithm, the resource usage of an algorithm as a function of an
input parameter. Different kinds of complexity such as <a
href="10-glossary.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst-case</a>
and <a href="10-glossary.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average-case</a>.</li>
<li>The concept of <a href="10-glossary.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-Oh notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> or <a href="10-glossary.html#order-of-growth" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-Oh notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">order
of growth</a> of a (mathematical) function. How to compute and compare
growth rates of functions. Notations such as <a
href="10-glossary.html#big-oh-notation" class="term"
title="For growth rates $f$ and $g$, we write $f \in O(g)$ to say that $g$ is an upper bound for $f$. The notation can be made sense of by defining $O(g)$ as the set of functions with growth rate less than or equal to that of $g$. The notation is often somewhat imprecisely used as $f(n) \in O(g(n))$ or even $f(n) = O(g(n))$.">big-Oh</a>
to describe upper and lower bounds of growth rates.</li>
<li>The <a href="10-glossary.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortized complexity.">asymptotic
complexity</a> of an algorithm, which is the growth rate of its
complexity. Sometimes, this is just called the growth rate of the
algorithm.</li>
<li>The difference between the asymptotic complexity of an <a
href="10-glossary.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
(or program) and that of a <a href="10-glossary.html#problem"
class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
The latter is the best asymptotic complexity over all algorithms that
solve the problem.</li>
</ul>
<p>The chapter concludes with a brief discussion of the practical
difficulties encountered when empirically measuring the cost of a
program, and some principles for code tuning to improve program
efficiency.</p>
</section>
</main>

<footer>
<nav class="sitenav">
<div class="navlink">
<a href="2.15-sorting-summary-exercises.html" class="navbutton">&lt;&lt;</a>
<a href="2.15-sorting-summary-exercises.html" accesskey="p" rel="previous">Sorting: Summary Exercises</a>
</div>
<div class="navlink">
<a href="3.1-problems-algorithms-and-programs.html" accesskey="n" rel="next">Problems, Algorithms, and Programs</a>
<a href="3.1-problems-algorithms-and-programs.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</footer>

</body>
</html>

