<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Peter Ljunglöf" />
  <meta name="author" content="Alex Gerdes" />
  <meta name="author" content="(editors)" />
  <title>DSABook – Analysing problems</title>
  <link rel="stylesheet" href="../css/main.css" />
  <link rel="stylesheet" href="../css/book.css" />
  <link rel="stylesheet" href="../css/code.css" />
  <link rel="stylesheet" href="../css/math.css" />
  <link rel="stylesheet" href="../css/quiz.css" />
  <link rel="stylesheet" href="../css/mobile.css" />
  <link rel="stylesheet" href="../css/print.css" />
  
  <link rel="stylesheet" href="../lib/JSAV.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaMOD.css" type="text/css" />
  <link rel="stylesheet" href="../lib/jquery.ui.min.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaStyle.css" type="text/css" />

  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:    './',
      VERSION:     '0.4.1',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  <script type="text/javascript" src="../lib/jquery.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.migrate.min.js"></script>
  <script type="text/javascript" src="../lib/localforage.min.js"></script>
  <script type="text/javascript" src="../lib/accessibility.js"></script>
  <script type="text/javascript" src="../lib/jquery.ui.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.transit.js"></script>
  <script type="text/javascript" src="../lib/raphael.js"></script>
  <script type="text/javascript" src="../lib/JSAV.js"></script>
  <script type="text/javascript" src="../lib/config.js"></script>
  <script type="text/javascript" src="../lib/timeme.js"></script>
  <script type="text/javascript" src="../lib/odsaUtils.js"></script>
  <script type="text/javascript" src="../lib/odsaMOD.js"></script>
  <script type="text/javascript" src="../lib/d3.min.js"></script>
  <script type="text/javascript" src="../lib/d3-selection-multi.v1.min.js"></script>
  <script type="text/javascript" src="../lib/dataStructures.js"></script>
  <script type="text/javascript" src="../lib/conceptMap.js"></script>

  <script>
    ODSA.SETTINGS.MODULE_SECTIONS = [
    'internal-variables', 
    'getting-and-setting-values', 
    'adding-elements', 
    'add-practice-exericse', 
    'removing-elements', 
    'remove-practice-exericise', 
    'static-array-based-list-summary-questions', 
    'static-array-based-list:-full-code',
    ];
    ODSA.SETTINGS.MODULE_NAME = "DSABook";
    ODSA.SETTINGS.MODULE_LONG_NAME = "Data Structures and Algorithms";
    JSAV_OPTIONS['lang']='en';
    JSAV_EXERCISE_OPTIONS['code']='pseudo';
  </script>

  
  <script type="text/javascript" src="../scripts/quizhandler.js"></script>
<link href="../interactive/Sorting/SortingLowerBoundCON.css" rel="stylesheet" type="text/css"/>
</head>

<body>

<header>
<nav class="sitenav">
<div></div>
<h1><a href="index.html" class="navbutton" accesskey="t" rel="top">Data Structures and Algorithms</a></h1>
<div></div>
</nav>
<nav class="sitenav">
<div>
<a href="section-5.2.html" class="navbutton">&lt;&lt;</a>
<a href="section-5.2.html" accesskey="p" rel="previous">Lower bounds and tight bounds</a>
</div>
<div>
<a href="section-5.4.html" accesskey="n" rel="next">Common misunderstandings</a>
<a href="section-5.4.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</header>

<main>
<section id="sec:analysing-problems" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span>
Analysing problems</h2>
<p>You most often use the techniques of algorithm analysis to analyse an
<a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
(or the instantiation of an algorithm as a <a
href="section-14.html#program" class="term"
title="An instance, or concrete representation, of an algorithm in some programming language.">program</a>).
But you can also use these same techniques to analyse the cost of a <a
href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
The key question that we want to ask is: How hard is a problem?</p>
<p>Certainly we should expect that in some sense, the problem of sorting
a list of records is harder than the problem of searching a list of
records for a given key value. Certainly the algorithms that we know for
sorting some records seem to be more expensive than the algorithms that
we know for searching those same records.</p>
<p>What we need are useful definitions for the upper bound and lower
bound of a <em>problem</em>, instead of an algorithm.</p>
<p>One might start by thinking that the upper bound for a problem is how
hard any algorithm can be for the problem. But we can make algorithms as
bad as we want, so that is not useful. Instead, what is useful is to say
that a problem is only as hard as what we <em>can</em> do. In other
words, we should define the upper bound for a problem to be the
<em>best</em> algorithm that we know for the problem.</p>
<p>But what does it then mean to give a lower bound for a problem? Lower
bound refers to the minimum that any algorithm <em>must</em> cost. For
example, when searching an unsorted list, we <em>must</em> look at every
record. When sorting a list, we <em>must</em> look at every record (to
even know if it is sorted).</p>
<p>So, how do upper and lower bounds relate to the key question – how
hard is a problem? As we have argued, the upper bound relies on our
knowledge of the currently best algorithm. But how can we be certain
that this algorithm is as good as it can be? To know this we have to
know about the lower bound of the problem, i.e., the lower bound tells
us how hard a problem is.</p>
<p>As a rule of thumb we can say:</p>
<ul>
<li>when we analyse an algorithm, we are interested in the <em>upper
bound</em>,
big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math></li>
<li>when we analyse a problem, we are instead interested in the
<em>lower bound</em>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></li>
</ul>
<p>It is much easier to show that an algorithm (or program) is in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(f)</annotation></semantics></math>
than it is to show that a problem is in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(f)</annotation></semantics></math>.
For a problem to be in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(f)</annotation></semantics></math>
means that <em>every</em> algorithm that solves the problem is in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(f)</annotation></semantics></math>,
even algorithms that we have not thought of! In other words,
<em>every</em> algorithm <em>must</em> have at least this cost. So, to
prove a lower bound, we need an argument that is true, even for
algorithms that we don’t know.</p>
<p>So far all of our examples of algorithm analysis give “obvious”
results, with
big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>
always matching
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>.
To understand how
big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>
notations are properly used to describe our understanding of a problem
or an algorithm, it is best to consider an example where you do not
already know a lot about the problem.</p>
<p>Let us look ahead to analysing the problem of sorting to see how this
process works. What is the least possible cost for any sorting algorithm
in the worst case? The algorithm must at least look at every element in
the input, just to determine that the input is truly sorted. Thus, any
sorting algorithm must take at least
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
time. For many problems, this observation that each of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
inputs must be looked at leads to an easy
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
lower bound.</p>
<p>In the previous chapters about sorting, you learned about some
sorting algorithms whose running time is in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
– Bubble sort, Selection sort and Insertion sort. But you also learned
about the linearithmic sorting algorithms Quicksort and Mergesort with a
running time in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n\log n)</annotation></semantics></math>.
Thus, the problem of sorting can be said to have an upper bound in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n\log n)</annotation></semantics></math>.
How do we close the gap between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n\log n)</annotation></semantics></math>?
Can there be even better sorting algorithms than Mergesort and
Quicksort? If you can think of no algorithm whose worst-case growth rate
is better than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n\log n)</annotation></semantics></math>,
and if you have discovered no analysis technique to show that the least
cost for the problem of sorting in the worst case is greater than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>,
then you cannot know for sure whether or not there is a better
algorithm.</p>
<p>Should we search for a faster algorithm? Many have tried, without
success. Fortunately (or perhaps unfortunately?), we can prove that
<em>any</em> sorting algorithm must have running time in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
in the worst case. So, the problem of sorting has a linearithmic lower
bound, which is the same as the upper bounds for the best sorting
algorithms. Thus, we can conclude that the problem of sorting is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Theta(n \log n)</annotation></semantics></math>
in the worst case, because the upper and lower bounds have met.</p>
<p>Knowing the lower bound for a problem does not give you a good
algorithm. But it does help you to know when to stop looking. If the
lower bound for the problem matches the upper bound for the algorithm
(within a constant factor), then we know that we can find an algorithm
that is better only by a constant factor.</p>
<p>So, to summarise: The upper bound for a problem is the best that you
<em>can</em> do, while the lower bound for a problem is the least work
that you <em>must</em> do. If those two are the same, then we can say
that we really understand our problem.</p>
<section id="sec:lower-bounds-for-sorting" class="level3"
data-number="5.3.1">
<h3 data-number="5.3.1"><span class="header-section-number">5.3.1</span>
Case study: Lower bounds for sorting</h3>
<p>By now you have seen many analyses for algorithms. These analyses
generally define the worst-case upper bounds. For many of the algorithms
presented so far, analysis has been quite easy. This section considers a
more difficult task: An analysis for the cost of a <em>problem</em> as
opposed to an <em>algorithm</em>.</p>
<div class="online">
<p>As we explained earlier, the lower bound defines the best possible
cost for <em>any</em> algorithm that solves the problem, including
algorithms not yet invented. Once we know the lower bound for the
problem, we know that no future algorithm can possibly be
(asymptotically) more efficient.</p>
<p>A simple estimate for a problem’s lower bound can be obtained by
measuring the size of the input that must be read and the output that
must be written. Certainly no algorithm can be more efficient than the
problem’s I/O time. From this we see that the sorting problem cannot be
solved by <em>any</em> algorithm in less than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
time because it takes at least
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
steps to read and write the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
values to be sorted. Alternatively, any sorting algorithm must at least
look at every input value to recognise whether the input values are in
sorted order. So, based on our current knowledge of sorting algorithms
and the size of the input, we know that the <em>problem</em> of sorting
is bounded by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>.</p>
<p>This section presents one of the most important and most useful
proofs in computer science: No sorting algorithm based on key
comparisons can possibly be faster than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
in the worst case. This proof is important for three reasons. First,
knowing that widely used sorting algorithms are asymptotically optimal
is reassuring. In particular, it means that you need not bang your head
against the wall searching for an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
sorting algorithm. (Or at least not one that is in any way based on key
comparisons. But it is hard to imagine how to sort without any
comparisons.) Second, this proof is one of the few non-trivial
lower-bounds proofs that we have for any problem; that is, this proof
provides one of the relatively few instances where our lower bound is
tighter than simply measuring the size of the input and output. As such,
it provides a useful model for proving lower bounds on other problems.
Finally, knowing a lower bound for sorting gives us a lower bound in
turn for other problems whose solution could be made to work as the
basis for a sorting algorithm. The process of deriving asymptotic bounds
for one problem from the asymptotic bounds of another is called a <a
href="section-14.html#reduction" class="term"
title="In algorithm analysis, the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another. In particular, if problem A can be used to solve problem B, and problem A is proved to be in $O(f(n))$, then problem B must also be in $O(f(n))$. Reductions are often used to show that certain problems are at least as expensive as sorting, or that certain problems are NP-Complete.">reduction</a>.</p>
<p>All of the sorting algorithms we have studied make decisions based on
the direct comparison of two key values. For example, <a
href="section-14.html#insertion-sort" class="term"
title="A sorting algorithm with $O(n^2)$ average and worst case cost, and $O(n)$ best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted.">Insertion
sort</a> sequentially compares the value to be inserted into the sorted
list until a comparison against the next value in the list fails.</p>
<p>The proof that any comparison sort requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
comparisons in the worst case is structured as follows:</p>
<ol type="1">
<li>First, comparison-based decisions can be modeled as the branches in
a tree. This means that any sorting algorithm based on comparisons
between records can be viewed as a binary tree whose nodes correspond to
the comparisons, and whose branches correspond to the possible
outcomes.</li>
<li>Next, the minimum number of leaves in the resulting tree is shown to
be the factorial of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</li>
<li>Finally, the minimum depth of a tree with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math>
leaves is shown to be in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>.</li>
</ol>
<p>Before presenting the proof of an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
lower bound for sorting, we first must define the concept of a <a
href="section-14.html#decision-tree" class="term"
title="A theoretical construct for modeling the behaviour of algorithms. Each point at which the algorithm makes a decision (such as an if statement) is modeled by a branch in the tree that represents the algorithms behaviour. Decision trees can be used in lower bounds proofs, such as the proof that sorting requires $\Omega(n \log n)$ comparisons in the worst case.">decision
tree</a>. A decision tree is a binary tree that can model the processing
for any algorithm that makes binary decisions. Each (binary) decision is
represented by a branch in the tree. For the purpose of modeling sorting
algorithms, we count all comparisons of key values as decisions. If two
keys are compared and the first is less than the second, then this is
modeled as a left branch in the decision tree. In the case where the
first value is greater than the second, the algorithm takes the right
branch.</p>
<div class="dsvis">
<details open="true">
<summary>
<p>Here is a visualisation that illustrates decision trees and the
sorting lower bound proof.</p>
</summary>
<p>
<div id="SortingLowerBoundCON" class="ssAV" data-short-name="SortingLowerBoundCON" data-long-name="Sorting Lower Bound Slideshow" alt="Sorting Lower Bound Slideshow" tabIndex="-1">
<span class="jsavcounter"></span>
<div class="jsavcontrols"></div>
<p class="jsavoutput jsavline"></p>
<div class="jsavcanvas"></div>
</div>
</p>
</details>
</div>
<p>Any sorting algorithm requiring
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
comparisons in the worst case requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
running time in the worst case. Because any sorting algorithm requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
running time, the problem of sorting also requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
time. We already know of sorting algorithms with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>
running time, so we can conclude that the problem of sorting requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Theta(n \log n)</annotation></semantics></math>
time. As a corollary, we know that no comparison-based sorting algorithm
can improve on existing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Theta(n \log n)</annotation></semantics></math>
time sorting algorithms by more than a constant factor.</p>
</div>
</section>
</section>
</main>

<footer>
<nav class="sitenav">
<div class="navlink">
<a href="section-5.2.html" class="navbutton">&lt;&lt;</a>
<a href="section-5.2.html" accesskey="p" rel="previous">Lower bounds and tight bounds</a>
</div>
<div class="navlink">
<a href="section-5.4.html" accesskey="n" rel="next">Common misunderstandings</a>
<a href="section-5.4.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</footer>

<script type="text/javascript" src="../interactive/Sorting/SortingLowerBoundCON.js"></script>
</body>
</html>

