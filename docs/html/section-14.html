<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Peter Ljunglöf" />
  <meta name="author" content="Alex Gerdes" />
  <meta name="author" content="(editors)" />
  <title>DSABook – Glossary</title>
  <link rel="stylesheet" href="../css/main.css" />
  <link rel="stylesheet" href="../css/book.css" />
  <link rel="stylesheet" href="../css/code.css" />
  <link rel="stylesheet" href="../css/math.css" />
  <link rel="stylesheet" href="../css/quiz.css" />
  <link rel="stylesheet" href="../css/mobile.css" />
  <link rel="stylesheet" href="../css/print.css" />
  
  <link rel="stylesheet" href="../lib/JSAV.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaMOD.css" type="text/css" />
  <link rel="stylesheet" href="../lib/jquery.ui.min.css" type="text/css" />
  <link rel="stylesheet" href="../lib/odsaStyle.css" type="text/css" />

  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:    './',
      VERSION:     '0.4.1',
      COLLAPSE_INDEX: false,
      FILE_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  <script type="text/javascript" src="../lib/jquery.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.migrate.min.js"></script>
  <script type="text/javascript" src="../lib/localforage.min.js"></script>
  <script type="text/javascript" src="../lib/accessibility.js"></script>
  <script type="text/javascript" src="../lib/jquery.ui.min.js"></script>
  <script type="text/javascript" src="../lib/jquery.transit.js"></script>
  <script type="text/javascript" src="../lib/raphael.js"></script>
  <script type="text/javascript" src="../lib/JSAV.js"></script>
  <script type="text/javascript" src="../lib/config.js"></script>
  <script type="text/javascript" src="../lib/timeme.js"></script>
  <script type="text/javascript" src="../lib/odsaUtils.js"></script>
  <script type="text/javascript" src="../lib/odsaMOD.js"></script>
  <script type="text/javascript" src="../lib/d3.min.js"></script>
  <script type="text/javascript" src="../lib/d3-selection-multi.v1.min.js"></script>
  <script type="text/javascript" src="../lib/dataStructures.js"></script>
  <script type="text/javascript" src="../lib/conceptMap.js"></script>

  <script>
    ODSA.SETTINGS.MODULE_SECTIONS = [
    'internal-variables', 
    'getting-and-setting-values', 
    'adding-elements', 
    'add-practice-exericse', 
    'removing-elements', 
    'remove-practice-exericise', 
    'static-array-based-list-summary-questions', 
    'static-array-based-list:-full-code',
    ];
    ODSA.SETTINGS.MODULE_NAME = "DSABook";
    ODSA.SETTINGS.MODULE_LONG_NAME = "Data Structures and Algorithms";
    JSAV_OPTIONS['lang']='en';
    JSAV_EXERCISE_OPTIONS['code']='pseudo';
  </script>

  
  <script type="text/javascript" src="../scripts/quizhandler.js"></script>
</head>

<body>

<header>
<nav class="sitenav">
<div></div>
<h1><a href="index.html" class="navbutton" accesskey="t" rel="top">Data Structures and Algorithms</a></h1>
<div></div>
</nav>
<nav class="sitenav">
<div>
<a href="section-13.11.html" class="navbutton">&lt;&lt;</a>
<a href="section-13.11.html" accesskey="p" rel="previous">Review questions</a>
</div>
<div>
<a href="section-15.html" accesskey="n" rel="next">Bibliography</a>
<a href="section-15.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</header>

<main>
<section id="sec:glossary" class="level1 html" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span>
Glossary</h1>
<div class="glossary">
<dl>
<dt><dfn id="2-3-tree">2-3 tree</dfn></dt>
<dd>
<p>A specialised form of the <a href="section-14.html#b-tree"
class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>
where each internal node has either 2 children or 3 children. Key values
are ordered to maintain the <a
href="section-14.html#binary-search-tree-property" class="term"
title="The defining relationship between the key values for nodes in a BST. All nodes stored in the left subtree of a node whose key value is $K$ have key values less than or equal to $K$. All nodes stored in the right subtree of a node whose key value is $K$ have key values greater than $K$.">binary
search tree property</a>. The 2-3 tree is always height balanced, and
its insert, search, and remove operations all have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>
cost.</p>
</dd>
<dt><dfn id="80-20-rule">80/20 rule</dfn></dt>
<dd>
<p>Given a typical application where there is a collection of records
and a series of search operations for records, the 80/20 rule is an
empirical observation that 80% of the record accessess typically go to
20% of the records. The exact values varies between data collections,
and is related to the concept of <a
href="section-14.html#locality-of-reference" class="term"
title="The concept that accesses within a collection of records is not evenly distributed. This can express itself as some small fraction of the records receiving the bulk of the accesses (80/20 rule). Alternatively, it can express itself as an increased probability that the next or future accesses will come close to the most recent access. This is the fundamental property for success of caching.">locality
of reference</a>.</p>
</dd>
<dt><dfn id="abstract-data-type">abstract data type</dfn> (<dfn
id="adt">ADT</dfn>)</dt>
<dd>
<p>Abbreviated ADT. The specification of a <a
href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
type</a> within some language, independent of an implementation. The <a
href="section-14.html#interface" class="term"
title="An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members.">interface</a>
for the ADT is defined in terms of a <a href="section-14.html#type"
class="term" title="A collection of values.">type</a> and a set of
operations on that type. The behaviour of each operation is determined
by its inputs and outputs. An ADT does not specify <em>how</em> the data
type is implemented. These implementation details are hidden from the
user of the ADT and protected from outside access, a concept referred to
as <a href="section-14.html#encapsulation" class="term"
title="In programming, the concept of hiding implementation details from the user of an ADT, and protecting data members of an object from outside access.">encapsulation</a>.</p>
</dd>
<dt><dfn id="accept">accept</dfn></dt>
<dd>
<p>When a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a> executes on a string and terminates in an <a
href="section-14.html#accepting-state" class="term"
title="Part of the definition of a finite automata is to designate some states as accepting states. If the finite automata executes on an input string and completes the computation in an accepting state, then the machine is said to accept the string.">accepting
state</a>, it is said to accept the string. The finite automata is said
to accept the language that consists of all strings for which the finite
automata completes execution in an accepting state.</p>
</dd>
<dt><dfn id="accepting-state">accepting state</dfn></dt>
<dd>
<p>Part of the definition of a <a href="section-14.html#finite-automata"
class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a> is to designate some <a href="section-14.html#state"
class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">states</a>
as accepting states. If the finite automata executes on an input string
and completes the computation in an accepting state, then the machine is
said to <a href="section-14.html#accept" class="term"
title="When a finite automata executes on a string and terminates in an accepting state, it is said to accept the string. The finite automata is said to accept the language that consists of all strings for which the finite automata completes execution in an accepting state.">accept</a>
the string.</p>
</dd>
<dt><dfn id="activation-record">activation record</dfn></dt>
<dd>
<p>The entity that is stored on the <a
href="section-14.html#runtime-stack" class="term"
title="The place where an activation record is stored when a subroutine is called during a program&#39;s runtime.">runtime
stack</a> during program execution. It stores any active <a
href="section-14.html#local-variable" class="term"
title="A variable declared within a function or method. It exists only from the time when the function is called to when the function exits. When a function is suspended (due to calling another function), the function&#39;s local variables are stored in an activation record on the runtime stack.">local
variable</a> and the return address from which a new subroutine is being
called, so that this information can be recovered when the subroutine
terminates.</p>
</dd>
<dt><dfn id="acyclic-graph">acyclic graph</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a graph that contains no <a href="section-14.html#cycle"
class="term"
title="In graph terminology, a cycle is a path of length three or more that connects some vertex $v_1$ to itself.">cycles</a>.</p>
</dd>
<dt><dfn id="address">address</dfn></dt>
<dd>
<p>A location in memory.</p>
</dd>
<dt><dfn id="adjacency-list">adjacency list</dfn></dt>
<dd>
<p>An implementation for a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
that uses an (array-based) <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>
to represent the <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> of the graph,
and each vertex is in turn represented by a (linked) list of the
vertices that are <a href="section-14.html#neighbour" class="term"
title="In a graph, a node $w$ is said to be a neighbour of node $v$ if there is an edge from $v$ to $w$.">neighbours</a>.</p>
</dd>
<dt><dfn id="adjacency-matrix">adjacency matrix</dfn></dt>
<dd>
<p>An implementation for a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
that uses a 2-dimensional <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
where each row and each column corresponds to a <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> in the <a
href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.
A given row and column in the matrix corresponds to an edge from the <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> corresponding to
the row to the vertex corresponding to the column.</p>
</dd>
<dt><dfn id="adjacent">adjacent</dfn></dt>
<dd>
<p>Two <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
of a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
or two <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> of a <a
href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
are said to be adjacent if they have an <a href="section-14.html#edge"
class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
connecting them. If the edge is directed from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>,
then we say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
is adjacent to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is adjacent from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>.</p>
</dd>
<dt><dfn>ADT</dfn></dt>
<dd>
<p>See <a href="section-14.html#abstract-data-type" class="term"
title="Abbreviated ADT. The specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behaviour of each operation is determined by its inputs and outputs. An ADT does not specify *how* the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation.">abstract
data type</a></p>
</dd>
<dt><dfn id="adversary-argument">adversary argument</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#lower-bounds-proof" class="term"
title="A proof regarding the lower bound, with this term most typically referring to the lower bound for any possible algorithm to solve a given problem. Many problems have a simple lower bound based on the concept that the minimum amount of processing is related to looking at all of the problem&#39;s input. However, some problems have a higher lower bound than that. For example, the lower bound for the problem of sorting ($\Omega(n \log n)$) is greater than the input size to sorting ($n$). Proving such &#39;non-trivial&#39; lower bounds for problems is notoriously difficult.">lower
bounds proof</a> for a problem where a (fictional) “adversary” is
assumed to control access to an algorithm’s input, and which yields
information about that input in such a way that will drive the cost for
any proposed algorithm to solve the problem as high as possible. So long
as the adversary never gives an answer that conflicts with any previous
answer, it is permitted to do whatever necessary to make the algorithm
require as much cost as possible.</p>
</dd>
<dt><dfn id="adversary">adversary</dfn></dt>
<dd>
<p>A fictional construct introduced for use in an <a
href="section-14.html#adversary-argument" class="term"
title="A type of lower bounds proof for a problem where a (fictional) &#39;adversary&#39; is assumed to control access to an algorithm&#39;s input, and which yields information about that input in such a way that will drive the cost for any proposed algorithm to solve the problem as high as possible. So long as the adversary never gives an answer that conflicts with any previous answer, it is permitted to do whatever necessary to make the algorithm require as much cost as possible.">adversary
argument</a>.</p>
</dd>
<dt><dfn>algorithm analysis</dfn></dt>
<dd>
<p>See <a href="section-14.html#asymptotic-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">asymptotic
analysis</a></p>
</dd>
<dt><dfn id="algorithm">algorithm</dfn></dt>
<dd>
<p>A method or a process followed to solve a <a
href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.</p>
</dd>
<dt><dfn id="alias">alias</dfn></dt>
<dd>
<p>Another name for something. In programming, this usually refers to
two <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">references</a>
that refer to the same object.</p>
</dd>
<dt><dfn id="all-pairs-shortest-paths-problem">all-pairs shortest paths
problem</dfn></dt>
<dd>
<p>Given a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
with <a href="section-14.html#weight" class="term"
title="A cost or distance most often associated with an edge in a graph.">weights</a>
or distances on the <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>,
find the shortest paths between every pair of vertices in the graph. One
approach to solving this problem is <a
href="section-14.html#floyds-algorithm" class="term"
title="An algorithm to solve the all-pairs shortest paths problem. It uses the dynamic programming algorithmic technique, and runs in $O(n^3)$ time. As with any dynamic programming algorithm, the key issue is to avoid duplicating work by using proper bookkeeping on the algorithm&#39;s progress through the solution space. The basic idea is to first find all the direct edge costs, then improving those costs by allowing paths through vertex 0, then the cheapest paths involving paths going through vertices 0 and 1, and so on.">Floyd’s
algorithm</a>, which uses the <a
href="section-14.html#dynamic-programming" class="term"
title="An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem.">dynamic
programming</a> algorithmic technique.</p>
</dd>
<dt><dfn id="allocated-allocation">allocated allocation</dfn></dt>
<dd>
<p>Reserving memory for an object in the heap memory.</p>
</dd>
<dt><dfn id="alphabet-trie">alphabet trie</dfn></dt>
<dd>
<p>A <a href="section-14.html#trie" class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>
data structure for storing variable-length strings. Level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the tree corresponds to the letter in position
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the string. The root will have potential branches on each intial
letter of string. Thus, all strings starting with “a” will be stored in
the “a” branch of the tree. At the second level, such strings will be
separated by branching on the second letter.</p>
</dd>
<dt><dfn id="alphabet">alphabet</dfn></dt>
<dd>
<p>The characters or symbols that strings in a given language may be
composed of.</p>
</dd>
<dt><dfn id="amortised-analysis">amortised analysis</dfn></dt>
<dd>
<p>Analysing the <a href="section-14.html#amortised-complexity"
class="term"
title="A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortised cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials.">amortised
complexity</a> of an <a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
or <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.</p>
</dd>
<dt><dfn id="amortised-complexity">amortised complexity</dfn></dt>
<dd>
<p>A modification to the notion of <a href="section-14.html#complexity"
class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
for operations on a data structure where, for each fixed input size, one
does not just look at the cost of a single run of the operation, but its
<a href="section-14.html#amortised-cost" class="term"
title="The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortised analysis, gives rise to the notion of amortised complexity.">amortised
cost</a> over sufficiently long series of operations of the same kind.
This can be made precise without considering averages by introducing <a
href="section-14.html#potential" class="term"
title="A concept in amortised complexity for operations on a data structure. We choose a *potential function* that associates an arbitrary non-negative value of *stored cost* (stored energy) with each state of the data structure. We then define the amortised cost of a run of the operation to be its cost as given by the the cost model plus the change in potential. The complexity modified this way is called amortised complexity. An example is adding an element to a dynamic array. When the dynamic array is not full, adding an element is quick and we store some of that saved cost by increasing the potential. When the dynamic array is full capacity, we perform an expensive reallocation, but compensate that cost by resetting the potential from a high value to zero. Let us define the potential of a dynamic array with capacity $c$ and size $n$ to be $max(2n-c,0)$. Assuming we double the capacity on reallocation, the operation of adding an element then has constant amortised complexity. The concept comes from potential energy in physics. For example, in the graviational field of the earth, kinetic energy may be stored as potential energy.">potentials</a>.</p>
</dd>
<dt><dfn id="amortised-cost">amortised cost</dfn></dt>
<dd>
<p>The average cost of an operation in a sufficiently long series of
operations of the same kind. This is as opposed to considering every
individual operation to independently have its own cost, which might
lead to an overestimate for the total cost of the series. This can be
made precise without considering averages by introducing <a
href="section-14.html#potential" class="term"
title="A concept in amortised complexity for operations on a data structure. We choose a *potential function* that associates an arbitrary non-negative value of *stored cost* (stored energy) with each state of the data structure. We then define the amortised cost of a run of the operation to be its cost as given by the the cost model plus the change in potential. The complexity modified this way is called amortised complexity. An example is adding an element to a dynamic array. When the dynamic array is not full, adding an element is quick and we store some of that saved cost by increasing the potential. When the dynamic array is full capacity, we perform an expensive reallocation, but compensate that cost by resetting the potential from a high value to zero. Let us define the potential of a dynamic array with capacity $c$ and size $n$ to be $max(2n-c,0)$. Assuming we double the capacity on reallocation, the operation of adding an element then has constant amortised complexity. The concept comes from potential energy in physics. For example, in the graviational field of the earth, kinetic energy may be stored as potential energy.">potentials</a>.</p>
<p>In <a href="section-14.html#amortised-analysis" class="term"
title="Analysing the amortised complexity of an algorithm or problem.">amortised
analysis</a>, gives rise to the notion of <a
href="section-14.html#amortised-complexity" class="term"
title="A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortised cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials.">amortised
complexity</a>.</p>
</dd>
<dt><dfn id="ancestor">ancestor</dfn></dt>
<dd>
<p>In a tree, for a given node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
any node on a <a href="section-14.html#path" class="term"
title="In tree or graph terminology, a sequence of vertices $v_1, v_2, ..., v_n$ forms a path of length $n-1$ if there exist edges from $v_i$ to $v_{i+1}$ for $1 \leq i &lt; n$.">path</a>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
up to the root is an ancestor of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="antisymmetric">antisymmetric</dfn></dt>
<dd>
<p>In set notation, relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
is antisymmetric if whenever
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">aRb</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>R</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">bRa</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a = b</annotation></semantics></math>,
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>∈</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">a, b \in \mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="approximation-algorithm">approximation algorithm</dfn></dt>
<dd>
<p>An algorthm for an <a href="section-14.html#optimisation-problem"
class="term"
title="Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the *traveling salesman problem*, where visiting $n$ cities in some order has a cost, and the goal is to visit in the cheapest order.">optimisation
problem</a> that finds a good, but not necessarily cheapest,
solution.</p>
</dd>
<dt><dfn id="arm">arm</dfn></dt>
<dd>
<p>In the context of an <a href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a>, this attaches the sensor on the I/O head to the <a
href="section-14.html#boom" class="term"
title="In the context of an I/O head, is the central structure to which all of the I/O heads are attached. Thus, the all move together during a seek operation.">boom</a>.</p>
</dd>
<dt><dfn id="array-based-list">array-based list</dfn></dt>
<dd>
<p>An implementation for the <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>
ADT that uses an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
to store the list elements. Typical implementations fix the array size
at creation of the list, and the <a href="section-14.html#overhead"
class="term"
title="All information stored by a data structure aside from the actual data. For example, the pointer fields in a linked list or BST, or the unused positions in an array-based list.">overhead</a>
is the number of array positions that are presently unused.</p>
</dd>
<dt><dfn id="array-based-queue">array-based queue</dfn></dt>
<dd>
<p>Analogous to an <a href="section-14.html#array-based-list"
class="term"
title="An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused.">array-based
list</a>, this uses an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
to store the elements when implementing the <a
href="section-14.html#queue" class="term"
title="A list-like structure in which elements are inserted only at one end, and removed only from the other one end.">queue</a>
ADT.</p>
</dd>
<dt><dfn id="array-based-stack">array-based stack</dfn></dt>
<dd>
<p>Analogous to an <a href="section-14.html#array-based-list"
class="term"
title="An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused.">array-based
list</a>, this uses an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
to store the elements when implementing the <a
href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>
ADT.</p>
</dd>
<dt><dfn id="array">array</dfn></dt>
<dd>
<p>A <a href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
type</a> that is used to store elements in consecutive memory locations
and refers to them by an index.</p>
</dd>
<dt><dfn id="ascii-character-coding">ASCII character coding</dfn></dt>
<dd>
<p><em>American Standard Code for Information Interchange</em>. A
commonly used method for encoding characters using a binary code.
Standard ASCII uses an 8-bit code to represent upper and lower case
letters, digits, some punctuation, and some number of non-printing
characters (such as carrage return). Now largely replaced by UTF-8
encoding.</p>
</dd>
<dt><dfn id="assembly-code">assembly code</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#intermediate-code" class="term"
title="A step in a typical compiler is to transform the original high-level language into a form on which it is easier to do other stages of the process. For example, some compilers will transform the original high-level source code into assembly code on which it can do code optimisation, before translating it into its final executable form.">intermediate
code</a> created by a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that is easy to convert into the final form that the computer can
execute. An assembly language is typically a direct mapping of one or a
few instructions that the CPU can execute into a mnemonic form that is
relatively easy for a human to read.</p>
</dd>
<dt><dfn>asymptotic algorithm analysis</dfn></dt>
<dd>
<p>See <a href="section-14.html#asymptotic-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">asymptotic
analysis</a></p>
</dd>
<dt><dfn id="asymptotic-analysis">asymptotic analysis</dfn> (<dfn
id="asymptotic-algorithm-analysis">asymptotic algorithm analysis</dfn>,
<dfn id="algorithm-analysis">algorithm analysis</dfn>)</dt>
<dd>
<p>A method for estimating the efficiency of an algorithm or computer
program by identifying its <a
href="section-14.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortised complexity.">asymptotic
complexity</a>, the <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> of its complexity function. Asymptotic analysis also gives a
way to define the inherent difficulty of a <a
href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
We frequently use the term algorithm analysis to mean the same
thing.</p>
</dd>
<dt><dfn id="asymptotic-complexity">asymptotic complexity</dfn></dt>
<dd>
<p>The <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> or <a href="section-14.html#order-of-growth" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">order
of growth</a> of the <a href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an algorithm or problem. There are several independent categories of
qualifiers for (asymptotic) complexity:</p>
<ul>
<li><a href="section-14.html#time-complexity" class="term"
title="The complexity of an algorithm or problem with a cost model that approximates runtime.">time
complexity</a> (default), <a href="section-14.html#space-complexity"
class="term"
title="The complexity of an algorithm or problem with a cost model that approximates memory/storage usage.">space
complexity</a>, complexity in some other cost,</li>
<li><a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a> (default), <a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average
case</a>, <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best
case</a>,</li>
<li>whether to use <a href="section-14.html#amortised-analysis"
class="term"
title="Analysing the amortised complexity of an algorithm or problem.">amortised
complexity</a>.</li>
</ul>
</dd>
<dt><dfn>attribute</dfn></dt>
<dd>
<p>See <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
member</a></p>
</dd>
<dt><dfn>automata</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn>automatic variable</dfn></dt>
<dd>
<p>See <a href="section-14.html#local-variable" class="term"
title="A variable declared within a function or method. It exists only from the time when the function is called to when the function exits. When a function is suspended (due to calling another function), the function&#39;s local variables are stored in an activation record on the runtime stack.">local
variable</a></p>
</dd>
<dt><dfn id="average-case">average case</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, specifically <a href="section-14.html#complexity"
class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an algorithm, the average of the costs for all <a
href="section-14.html#problem-instance" class="term"
title="A specific selection of values for the parameters to a problem. In other words, a specific set of inputs to a problem. A given problem instance has a size under some cost model.">problem
instances</a> of a given input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.
If not all problem instances have equal probability of occurring, then
the average case must be calculated using a weighted average that is
specified with the problem (for example, every input may be equally
likely). Every input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
has its own average case. We <strong>never</strong> consider the average
case as removed from input size.</p>
</dd>
<dt><dfn id="average-seek-time">average seek time</dfn></dt>
<dd>
<p>Expected (average) time to perform a <a href="section-14.html#seek"
class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operation on a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, assuming that the seek is between two randomly selected
tracks. This is one of two metrics commonly provided by disk drive
vendors for disk drive performance, with the other being <a
href="section-14.html#track-to-track-seek-time" class="term"
title="Expected (average) time to perform a seek operation from a random track to an adjacent track. Thus, this can be viewed as the minimum possible seek time for the disk drive. This is one of two metrics commonly provided by disk drive vendors for disk drive performance, with the other being average seek time.">track-to-track
seek time</a>.</p>
</dd>
<dt><dfn id="avl-tree">AVL tree</dfn></dt>
<dd>
<p>A variant implementation for the <a href="section-14.html#bst"
class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>,
which differs from the standard BST in that it uses modified insert and
remove methods in order to keep the tree <a
href="section-14.html#balanced-tree" class="term"
title="A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree.">balanced</a>.
Similar to a <a href="section-14.html#splay-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a> in that it uses the concept of <a
href="section-14.html#rotation" class="term"
title="In the AVL tree and splay tree, a rotation is a local operation performed on a node, its children, and its grandchildren that can result in reordering their relationship. The goal of performing a rotation is to make the tree more balanced.">rotations</a>
in the insert and remove operations.</p>
</dd>
<dt><dfn id="b-star-tree">B* tree</dfn></dt>
<dd>
<p>A variant on the <a href="section-14.html#b-plus-tree" class="term"
title="The most commonly implemented form of B-tree. A B+ tree does not store data at the internal nodes, but instead only stores search key values as direction finders for the purpose of searching through the tree. Only the leaf nodes store a reference to the actual data records.">B+
tree</a>. The B* tree is identical to the B+ tree, except for the rules
used to split and merge nodes. Instead of splitting a node in half when
it overflows, the B* tree gives some records to its neighbouring
sibling, if possible. If the sibling is also full, then these two nodes
split into three. Similarly, when a node underflows, it is combined with
its two siblings, and the total reduced to two nodes. Thus, the nodes
are always at least two thirds full.</p>
</dd>
<dt><dfn id="b-plus-tree">B+ tree</dfn></dt>
<dd>
<p>The most commonly implemented form of <a
href="section-14.html#b-tree" class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>.
A B+ tree does not store data at the <a
href="section-14.html#internal-node" class="term"
title="In a tree, any node that has at least one non-empty child is an internal node.">internal
nodes</a>, but instead only stores <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> values as direction finders for the purpose of searching through
the tree. Only the <a href="section-14.html#leaf-node" class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaf
nodes</a> store a <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to the actual data records.</p>
</dd>
<dt><dfn id="b-tree">B-tree</dfn></dt>
<dd>
<p>A method for <a href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">indexing</a>
a large collection of records. A B-tree is a <a
href="section-14.html#balanced-tree" class="term"
title="A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree.">balanced
tree</a> that typically has high branching factor (commonly as much as
100 <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>
per <a href="section-14.html#internal-node" class="term"
title="In a tree, any node that has at least one non-empty child is an internal node.">internal
node</a>), causing the tree to be very shallow. When stored on disk, the
node size is selected to be same as the desired unit of I/O (hence some
multiple of the disk <a href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>
size). This makes it easy to gain access to the record associated with a
given <a href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> stored in the tree with few <a
href="section-14.html#disk-access" class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
accesses</a>. The most commonly implemented variant of the B-tree is the
<a href="section-14.html#b-plus-tree" class="term"
title="The most commonly implemented form of B-tree. A B+ tree does not store data at the internal nodes, but instead only stores search key values as direction finders for the purpose of searching through the tree. Only the leaf nodes store a reference to the actual data records.">B+
tree</a>.</p>
</dd>
<dt><dfn id="backing-storage">backing storage</dfn></dt>
<dd>
<p>In the context of a <a href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">caching</a>
system or <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>, backing storage is the relatively large but slower source of
data that needs to be cached. For example, in a <a
href="section-14.html#virtual-memory" class="term"
title="A memory management technique for making relatively fast but small memory appear larger to the program. The large &#39;virtual&#39; data space is actually stored on a relatively slow but large backing storage device, and portions of the data are copied into the smaller, faster memory as needed by use of a buffer pool. A common example is to use RAM to manage access to a large virtual space that is actually stored on a disk drive. The programmer can implement a program as though the entire data content were stored in RAM, even if that is larger than the physical RAM available making it easier to implement.">virtual
memory</a>, the disk drive would be the backing storage. In the context
of a web browser, the Internet might be considered the backing
storage.</p>
</dd>
<dt><dfn id="backtracking">backtracking</dfn></dt>
<dd>
<p>A <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
for brute-force search of a solution space. It is essentially a <a
href="section-14.html#depth-first-search" class="term"
title="A graph traversal algorithm. Whenever a $v$ is visited during the traversal, DFS will recursively visit all of $v$ &#39;s unvisited neighbours.">depth-first
search</a> of the solution space. This can be improved using a <a
href="section-14.html#branch-and-bounds-algorithm" class="term"
title="A variation on backtracking that applies to optimisation problems. We traverse the solution tree as with backtracking. Proceeding deeper in the solution tree generally requires additional cost. We remember the best-cost solution found so far. If the cost of the current branch in the tree exceeds the best tour cost found so far, then we know to stop pursuing this branch of the tree. At this point we can immediately back up and take another branch.">branch-and-bounds
algorithm</a>.</p>
</dd>
<dt><dfn id="bad-reference">bad reference</dfn></dt>
<dd>
<p>A reference is referred to as a bad reference if it is allocated but
not initialised.</p>
</dd>
<dt><dfn id="bag">bag</dfn></dt>
<dd>
<p>In set notation, a bag is a collection of elements with no order
(like a set), but which allows for duplicate-valued elements (unlike a
set).</p>
</dd>
<dt><dfn id="balanced-tree">balanced tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
where the <a href="section-14.html#subtree" class="term"
title="A subtree is a subset of the nodes of a binary tree that includes some node $R$ of the tree as the subtree root along with all the descendants of $R$.">subtrees</a>
meet some criteria for being balanced. Two possibilities are that the
tree is <a href="section-14.html#height-balanced" class="term"
title="The condition the depths of each subtree in a tree are roughly the same.">height
balanced</a>, or that the tree has a roughly equal number of <a
href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
in each subtree.</p>
</dd>
<dt><dfn id="base-case">base case</dfn></dt>
<dd>
<p>In <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>
or <a href="section-14.html#proof-by-induction" class="term"
title="A mathematical proof technique similar to recursion. It is used to prove a parameterised theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction.">proof
by induction</a>, the base case is the termination condition. This is a
simple input or value that can be solved (or proved in the case of
induction) without resorting to a recursive call (or the <a
href="section-14.html#induction-hypothesis" class="term"
title="The key assumption used in a proof by induction, that the theorem to be proved holds for smaller instances of the theorem. The induction hypothesis is equivalent to the recursive call in a recursive function.">induction
hypothesis</a>).</p>
</dd>
<dt><dfn id="base-class">base class</dfn></dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, a class from which another class <a
href="section-14.html#inherit" class="term"
title="In object-oriented programming, the process by which a subclass gains data members and methods from a base class.">inherits</a>.
The class that inherits is called a <a href="section-14.html#subclass"
class="term"
title="In object-oriented programming, any class within a class hierarchy that inherits from some other class.">subclass</a>.</p>
</dd>
<dt><dfn id="base-type">base type</dfn></dt>
<dd>
<p>The <a href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
type</a> for the elements in a set. For example, the set might consist
of the integer values 3, 5, and 7. In this example, the base type is
integers.</p>
</dd>
<dt><dfn id="base">base</dfn> (<dfn id="radix">radix</dfn>)</dt>
<dd>
<p>The number of digits in a number representation. For example, we
typically represent numbers in base (or radix) 10. Hexidecimal is base
(or radix) 16.</p>
</dd>
<dt><dfn id="basic-operation">basic operation</dfn></dt>
<dd>
<p>Examples of basic operations include inserting a data item into the
data structure, deleting a <a href="section-14.html#data-item"
class="term"
title="A piece of information or a record whose value is drawn from a type.">data
item</a> from the data structure, and finding a specified <a
href="section-14.html#data-item" class="term"
title="A piece of information or a record whose value is drawn from a type.">data
item</a>.</p>
</dd>
<dt><dfn id="best-case">best case</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, specifically <a href="section-14.html#complexity"
class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an algorithm, the <a href="section-14.html#problem-instance"
class="term"
title="A specific selection of values for the parameters to a problem. In other words, a specific set of inputs to a problem. A given problem instance has a size under some cost model.">problem
instance</a> from among all problem instances for a given input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
that has least cost. Every input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
has its own best case. We <strong>never</strong> consider the best case
as removed from input size.</p>
</dd>
<dt><dfn id="best-fit">best fit</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, best fit is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
for deciding which <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> to use
when allocating memory from a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. Best fit will always allocate from the smallest <a
href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> that is
large enough to service the memory request. The rationale is that this
will be the method that best preserves large blocks needed for unusually
large requests. The disadvantage is that it tends to cause <a
href="section-14.html#external-fragmentation" class="term"
title="A condition that arises when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests.">external
fragmentation</a> in the form of small, unusable memory blocks.</p>
</dd>
<dt><dfn>BFS</dfn></dt>
<dd>
<p>See <a href="section-14.html#breadth-first-search" class="term"
title="A graph traversal algorithm. As the name implies, all immediate neighbours for a node are visited before any more-distant nodes are visited. BFS is driven by a queue. A start vertex is placed on the queue. Then, until the queue is empty, a node is taken off the queue, visited, and and then any unvisited neighbours are placed onto the queue.">breadth-first
search</a></p>
</dd>
<dt><dfn
id="big-o-notation">big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>
notation</dfn></dt>
<dd>
<p>For <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rates</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>,
we write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f \in O(g)</annotation></semantics></math>
to say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
is an <a href="section-14.html#upper-bound" class="term"
title="An upper bound for a growth rate $f$ is any growth rate $g$ that is greater than or equal to it. Formally, there are constants $n_0 \geq 0$ and $C &gt; 0$ such that $f(n) \leq C g(n)$ for all $n \geq n_0$. We also write $f \in O(g)$ or slightly imprecisely $f(n) \in O(g(n))$ (this is big-$O$ notation). Usually, we are interested in finding an upper bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, an upper bound for an algorithm is an upper bound for the asymptotic complexity of the algorithm, the growth rate of its complexity. In practice, we are looking for the best possible upper bound that has a simple mathematical expression. For example, we may write $T(n) \in O(n^2)$ if $T$ is the (time) complexity of the algorithm to say that the complexity is quadratic, i.e. the asymptoptic complexity of the algorithm has as upper bound the growth rate given by squaring.">upper
bound</a> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.
The notation can be made sense of by defining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(g)</annotation></semantics></math>
as the set of functions with growth rate less than or equal to that of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>.
The notation is often somewhat imprecisely used as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \in O(g(n))</annotation></semantics></math>
or even
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) = O(g(n))</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="binary-heap">binary heap</dfn> (<dfn
id="heap">heap</dfn>)</dt>
<dd>
<p>The head data structure is a <a
href="section-14.html#complete-binary-tree" class="term"
title="A binary tree where the nodes are filled in row by row, with the bottom row filled in left to right. Due to this requirement, there is only one tree of $n$ nodes for any value of $n$. Since storing the records in an array in row order leads to a simple mapping from a node&#39;s position in the array to its parent, siblings, and children, the array representation is most commonly used to implement the complete binary tree. The heap data structure is a complete binary tree with partial ordering constraints on the node values.">complete
binary tree</a> with the requirement that every <a
href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
has a value greater than its <a href="section-14.html#child"
class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>
(called a <a href="section-14.html#max-heap" class="term"
title="A heap where every node has a key value greater than its children. As a consequence, the node with maximum key value is at the root.">max
heap</a>), or else the requirement that every node has a value less than
its children (called a <a href="section-14.html#min-heap" class="term"
title="A heap where every node has a key value less than its children. As a consequence, the node with minimum key value is at the root.">min
heap</a>). Since it is a complete binary tree, a heap is nearly always
implemented using an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
rather than an explicit tree structure. To add a new value to a heap, or
to remove the extreme value (the max value in a max-heap or min value in
a min-heap) and update the heap, takes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>
time in the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>. However, if given all of the values in an unordered array, the
values can be re-arranged to form a heap in only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
time. Due to its space and time efficiency, the heap is a popular choice
for implementing a <a href="section-14.html#priority-queue" class="term"
title="An ADT whose primary operations of insert of records, and deletion of the greatest (or, in an alternative implementation, the least) valued record. Most often implemented using the heap data structure. The name comes from a common application where the records being stored represent tasks, with the ordering values based on the priorities of the tasks.">priority
queue</a>.</p>
<p>Uncommonly, <em>heap</em> is a synonym for <a
href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>.</p>
</dd>
<dt><dfn id="binary-insert-sort">binary insert sort</dfn></dt>
<dd>
<p>A variation on <a href="section-14.html#insertion-sort" class="term"
title="A sorting algorithm with $O(n^2)$ average and worst case cost, and $O(n)$ best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted.">Insertion
sort</a> where the position of the value being inserted is located by
binary search, and then put into place. In normal usage this is not an
improvement on standard Insertion sort because of the expense of moving
many items in the <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>.
But it is directly useful if the cost of comparison is high compared to
that of moving an element, or is theoretically useful if we only care to
count the cost of comparisons.</p>
</dd>
<dt><dfn id="binary-search-tree-property">binary search tree
property</dfn></dt>
<dd>
<p>The defining relationship between the <a href="section-14.html#key"
class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values for <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
in a <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>.
All nodes stored in the left subtree of a node whose key value is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
have key values less than or equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>.
All nodes stored in the right subtree of a node whose key value is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
have key values greater than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="binary-search-tree">binary search tree</dfn> (<dfn
id="bst">BST</dfn>)</dt>
<dd>
<p>A binary tree that imposes the following constraint on its node
values: The <a href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> value for any node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
must be greater than the (key) values for all nodes in the left <a
href="section-14.html#subtree" class="term"
title="A subtree is a subset of the nodes of a binary tree that includes some node $R$ of the tree as the subtree root along with all the descendants of $R$.">subtree</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
and less than the key values for all nodes in the right subtree of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
Some convention must be adopted if multiple nodes with the same key
value are permitted, typically these are required to be in the right
subtree.</p>
</dd>
<dt><dfn id="binary-search">binary search</dfn></dt>
<dd>
<p>A standard <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursive</a>
algorithm for finding the <a href="section-14.html#record" class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">record</a>
with a given <a href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> value within a sorted list. It runs in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>
time. At each step, look at the middle of the current sublist, and throw
away the half of the records whose keys are either too small or too
large.</p>
</dd>
<dt><dfn id="binary-tree">binary tree</dfn></dt>
<dd>
<p>A finite set of nodes which is either empty, or else has a root node
together two binary trees, called the left and right <a
href="section-14.html#subtree" class="term"
title="A subtree is a subset of the nodes of a binary tree that includes some node $R$ of the tree as the subtree root along with all the descendants of $R$.">subtrees</a>,
which are <a href="section-14.html#disjoint" class="term"
title="Two parts of a data structure or two collections with no objects in common are disjoint. This term is often used in conjunction with a data structure that has nodes (such as a tree). Also used in the context of sets, where two subsets are disjoint if they share no elements.">disjoint</a>
from each other and from the <a href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>.</p>
</dd>
<dt><dfn id="binary-trie">binary trie</dfn></dt>
<dd>
<p>A <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a> whose structure is that of a <a href="section-14.html#trie"
class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>.
Generally this is an implementation for a <a
href="section-14.html#search-tree" class="term"
title="A tree data structure that makes search by key value more efficient. A type of container, it is common to implement an index using a search tree. A good search tree implementation will guarentee that insertion, deletion, and search operations are all $O(\log n)$.">search
tree</a>. This means that the <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> values are thought of a binary digits, with the digit in the
position corresponding to this a node’s <a href="section-14.html#level"
class="term"
title="In a tree, all nodes of depth $d$ are at level $d$ in the tree. The root is the only node at level 0, and its depth is 0.">level</a>
in the tree indicating a left branch if it is “0”, or a right branch if
it is “1”. Examples include the <a
href="section-14.html#huffman-coding-tree" class="term"
title="A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie.">Huffman
coding tree</a> and the <a href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">Bintree</a>.</p>
</dd>
<dt><dfn id="binning">binning</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
binning is a type of <a href="section-14.html#hash-function"
class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>. Say we are given keys in the range 0 to 999, and have a
hash table of size 10. In this case, a possible hash function might
simply divide the key value by 100. Thus, all keys in the range 0 to 99
would hash to slot 0, keys 100 to 199 would hash to slot 1, and so on.
In other words, this hash function “bins” the first 100 keys to the
first slot, the next 100 keys to the second slot, and so on. This
approach tends to make the hash function dependent on the distribution
of the high-order bits of the keys.</p>
</dd>
<dt><dfn id="binsort">binsort</dfn></dt>
<dd>
<p>A sort that works by taking each record and placing it into a bin
based on its value. The bins are then gathered up in order to sort the
list. It is generally not practical in this form, but it is the
conceptual underpinning of the <a href="section-14.html#radix-sort"
class="term"
title="A sorting algorithm that works by processing records with $k$ digit keys in $k$ passes, where each pass sorts the records according to the current digit. At the end of the process, the records will be sorted. This can be efficient if the number of digits is small compared to the number of records. However, if the $n$ records all have unique key values, than at least $\Omega(\log n)$ digits are required, leading to an $\Omega(n \log n)$ sorting algorithm that tends to be much slower than other sorting algorithms like Quicksort or Mergesort.">radix
sort</a>.</p>
</dd>
<dt><dfn id="bintree">bintree</dfn></dt>
<dd>
<p>A <a href="section-14.html#spatial-data-structure" class="term"
title="A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree.">spatial
data structure</a> in the form of binary <a href="section-14.html#trie"
class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>,
typically used to store point data in two or more dimensions. Similar to
a <a href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a> except that at each level, it splits one dimension in half.
Since many leaf nodes of the PR quadtree will contain no data points,
implementation often makes use of the <a
href="section-14.html#flyweight" class="term"
title="A design pattern that is meant to solve the following problem: You have an application with many objects. Some of these objects are identical in the information that they contain, and the role that they play. But they must be reached from various places, and conceptually they really are distinct objects. Because there is so much duplication of the same information, we want to reduce memory cost by sharing that space. For example, in document layout, the letter &#39;C&#39; might be represented by an object that describes that character&#39;s strokes and bounding box. However, we do not want to create a separate &#39;C&#39; object everywhere in the document that a &#39;C&#39; appears. The solution is to allocate a single copy of the shared representation for &#39;C&#39; objects. Then, every place in the document that needs a &#39;C&#39; in a given font, size, and typeface will reference this single copy. The various instances of references to a specific form of &#39;C&#39; are called flyweights. Flyweights can also be used to implement the empty leaf nodes of the bintree and PR quadtree.">flyweight</a>
<a href="section-14.html#design-pattern" class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a>.</p>
</dd>
<dt><dfn>bit vector</dfn></dt>
<dd>
<p>See <a href="section-14.html#bitmap" class="term"
title="An array that stores a single bit at each position. Typically these bits represent Boolean variables associated with a collection of objects, such that the $i$ th bit is the Boolean value for the $i$ th object.">bitmap</a></p>
</dd>
<dt><dfn id="bitmap">bitmap</dfn> (<dfn id="bit-vector">bit
vector</dfn>)</dt>
<dd>
<p>An <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
that stores a single bit at each position. Typically these bits
represent <a href="section-14.html#boolean-variable" class="term"
title="A variable that takes on one of the two values `True` and `False`.">Boolean
variables</a> associated with a collection of objects, such that the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
th bit is the Boolean value for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
th object.</p>
</dd>
<dt><dfn id="block">block</dfn></dt>
<dd>
<p>A unit of storage, usually referring to storage on a <a
href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> or other <a href="section-14.html#peripheral-storage"
class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> device. A block is the basic unit of I/O for that
device.</p>
</dd>
<dt><dfn id="boolean-expression">Boolean expression</dfn></dt>
<dd>
<p>A Boolean expression is comprised of <a
href="section-14.html#boolean-variable" class="term"
title="A variable that takes on one of the two values `True` and `False`.">Boolean
variable</a> combined using the operators AND
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⋅</mi><annotation encoding="application/x-tex">\cdot</annotation></semantics></math>),
OR
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math>),
and NOT (to negate Boolean variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
we write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">¯</mo></mover><annotation encoding="application/x-tex">\overline{x}</annotation></semantics></math>).</p>
</dd>
<dt><dfn id="boolean-variable">Boolean variable</dfn></dt>
<dd>
<p>A variable that takes on one of the two values <code>True</code> and
<code>False</code>.</p>
</dd>
<dt><dfn id="boom">boom</dfn></dt>
<dd>
<p>In the context of an <a href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a>, is the central structure to which all of the I/O heads are
attached. Thus, the all move together during a <a
href="section-14.html#seek" class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operation.</p>
</dd>
<dt><dfn id="bounding-box">bounding box</dfn></dt>
<dd>
<p>A box (usually aligned to the coordinate axes of the reference
system) that contains a (potentially complex) object. In graphics and
computational geometry, complex objects might be associated with a
bounding box for use by algorithms that search for objects in a
particular location. The idea is that if the bounding box is not within
the area of interest, then neither is the object. Checking the bounding
box is cheaper than checking the object, but it does require some time.
So if enough objects are not outside the area of interest, this approach
will not save time. But if most objects are outside of the area of
interest, then checking bounding boxes first can save a lot of time.</p>
</dd>
<dt><dfn id="branch-and-bounds-algorithm">branch-and-bounds
algorithm</dfn></dt>
<dd>
<p>A variation on <a href="section-14.html#backtracking" class="term"
title="A heuristic for brute-force search of a solution space. It is essentially a depth-first search of the solution space. This can be improved using a branch-and-bounds algorithm.">backtracking</a>
that applies to <a href="section-14.html#optimisation-problem"
class="term"
title="Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the *traveling salesman problem*, where visiting $n$ cities in some order has a cost, and the goal is to visit in the cheapest order.">optimisation
problems</a>. We traverse the <a href="section-14.html#solution-tree"
class="term"
title="An ordering imposed on the set of solutions within a solution space in the form of a tree, typically derived from the order that some algorithm would visit the solutions.">solution
tree</a> as with backtracking. Proceeding deeper in the solution tree
generally requires additional cost. We remember the best-cost solution
found so far. If the cost of the current branch in the tree exceeds the
best tour cost found so far, then we know to stop pursuing this branch
of the tree. At this point we can immediately back up and take another
branch.</p>
</dd>
<dt><dfn id="breadth-first-search">breadth-first search</dfn> (<dfn
id="bfs">BFS</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
<a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
algorithm. As the name implies, all immediate <a
href="section-14.html#neighbour" class="term"
title="In a graph, a node $w$ is said to be a neighbour of node $v$ if there is an edge from $v$ to $w$.">neighbours</a>
for a <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
are <a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visited</a>
before any more-distant nodes are visited. BFS is driven by a <a
href="section-14.html#queue" class="term"
title="A list-like structure in which elements are inserted only at one end, and removed only from the other one end.">queue</a>.
A start vertex is placed on the queue. Then, until the queue is empty, a
node is taken off the queue, visited, and and then any <a
href="section-14.html#unvisited" class="term"
title="In graph algorithms, this refers to a node that has not been processed at the current point in the algorithm. This information is typically maintained by using a mark array.">unvisited</a>
neighbours are placed onto the queue.</p>
</dd>
<dt><dfn id="break-even-point">break-even point</dfn></dt>
<dd>
<p>The point at which two costs become even when measured as the
function of some variable. In particular, used to compare the space
requirements of two implementations. For example, when comparing the
space requirements of an <a href="section-14.html#array-based-list"
class="term"
title="An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused.">array-based
list</a> implementation versus a <a href="section-14.html#linked-list"
class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> implementation, the key issue is how full the list is compared
to its capacity limit (for the array-based list). The point where the
two representations would have the same space cost is the break-even
point. As the list becomes more full beyond this point, the array-based
list implementation becomes more space efficent, while as the list
becomes less full below this point, the linked list implementation
becomes more space efficient.</p>
</dd>
<dt><dfn>BST</dfn></dt>
<dd>
<p>See <a href="section-14.html#binary-search-tree" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">binary
search tree</a></p>
</dd>
<dt><dfn id="bubble-sort">Bubble sort</dfn></dt>
<dd>
<p>A simple sort that requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
time in <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best</a>,
<a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>,
and <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst</a>
cases. Even an optimised version will normally run slower than <a
href="section-14.html#insertion-sort" class="term"
title="A sorting algorithm with $O(n^2)$ average and worst case cost, and $O(n)$ best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted.">Insertion
sort</a>, so it has little to recommend it.</p>
</dd>
<dt><dfn id="bucket-hashing">bucket hashing</dfn></dt>
<dd>
<p>A method of <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>
where multiple <a href="section-14.html#slot" class="term"
title="In hashing, a position in a hash table.">slots</a> of the <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> are grouped together to form a <a
href="section-14.html#bucket" class="term"
title="In bucket hashing, a bucket is a sequence of slots in the hash table that are grouped together.">bucket</a>.
The <a href="section-14.html#hash-function" class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a> then either hashes to some bucket, or else it hashes to a
<a href="section-14.html#home-slot" class="term"
title="In hashing, this is the slot in the hash table determined for a given key by the hash function.">home
slot</a> in the normal way, but this home slot is part of some bucket.
<a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">Collision
resolution</a> is handled first by attempting to find a free position
within the same bucket as the home slot. If the bucket if full, then the
record is placed in an <a href="section-14.html#overflow-bucket"
class="term"
title="In bucket hashing, this is the bucket into which a record is placed if the bucket containing the record&#39;s home slot is full. The overflow bucket is logically considered to have infinite capacity, though in practice search and insert will become relatively expensive if many records are stored in the overflow bucket.">overflow
bucket</a>.</p>
</dd>
<dt><dfn id="bucket-sort">bucket sort</dfn></dt>
<dd>
<p>A variation on the <a href="section-14.html#binsort" class="term"
title="A sort that works by taking each record and placing it into a bin based on its value. The bins are then gathered up in order to sort the list. It is generally not practical in this form, but it is the conceptual underpinning of the radix sort.">Binsort</a>,
where each bin is associated with a range of <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values. This will require some method of sorting the records placed into
each bin.</p>
</dd>
<dt><dfn id="bucket">bucket</dfn></dt>
<dd>
<p>In <a href="section-14.html#bucket-hashing" class="term"
title="A method of hashing where multiple slots of the hash table are grouped together to form a bucket. The hash function then either hashes to some bucket, or else it hashes to a home slot in the normal way, but this home slot is part of some bucket. Collision resolution is handled first by attempting to find a free position within the same bucket as the home slot. If the bucket if full, then the record is placed in an overflow bucket.">bucket
hashing</a>, a bucket is a sequence of <a href="section-14.html#slot"
class="term" title="In hashing, a position in a hash table.">slots</a>
in the <a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> that are grouped together.</p>
</dd>
<dt><dfn id="buddy-method">buddy method</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, an alternative to using a <a
href="section-14.html#free-block-list" class="term"
title="In a memory manager, the list that stores the necessary information about the current free blocks. Generally, this is done with some sort of linked list, where each node of the linked list indicates the start position and length of the free block in the memory pool.">free
block list</a> and a <a href="section-14.html#sequential-fit"
class="term"
title="In a memory manager, the process of searching the memory pool for a free block large enough to service a memory request, possibly reserving the remaining space as a free block. Examples are first fit, circular first fit, best fit, and worst fit.">sequential
fit</a> method to seach for a suitable free block to service a <a
href="section-14.html#memory-request" class="term"
title="In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there.">memory
request</a>. Instead, the memory pool is broken down as needed into
smaller chunks by splitting it in half repeatedly until the smallest
power of 2 that is as big or bigger than the size of the memory request
is reached. The name comes from the fact that the binary representation
for the start of the block positions only differ by one bit for adjacent
blocks of the same size. These are referred to as “buddies” and will be
merged together if both are free.</p>
</dd>
<dt><dfn id="buffer-passing">buffer passing</dfn></dt>
<dd>
<p>An approach to implementing the <a href="section-14.html#adt"
class="term"
title="Abbreviated ADT. The specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behaviour of each operation is determined by its inputs and outputs. An ADT does not specify *how* the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation.">ADT</a>
for a <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>, where a pointer to a <a href="section-14.html#buffer"
class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
is passed between the client and the buffer pool. This is in contrast to
a <a href="section-14.html#message-passing" class="term"
title="A common approach to implementing the ADT for a memory manager or buffer pool, where the contents of a message to be stored is explicitly passed between the client and the memory manager. This is in contrast to a buffer passing approach.">message
passing</a> approach, it is most likely to be used for long messages or
when the message size is always the same as the buffer size, such as
when implementing a <a href="section-14.html#b-tree" class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>.</p>
</dd>
<dt><dfn id="buffer-pool">buffer pool</dfn></dt>
<dd>
<p>A collection of one or more <a href="section-14.html#buffer"
class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffers</a>.
The buffer pool is an example of a <a href="section-14.html#caching"
class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">cache</a>.
It is stored in <a href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a>, and holds data that is expected to be used in the near
future. When a data value is requested, the buffer pool is searched
first. If the value is found in the buffer pool, then <a
href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a> need not be accessed. If the value is not found in the
buffer pool, then it must be fetched from secondary storage. A number of
traditional <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristics</a>
have been developed for deciding which data to <a
href="section-14.html#flush" class="term"
title="The act of removing data from a cache, most typically because other data considered of higher future value must replace it in the cache. If the data being flushed has been modified since it was first read in from secondary storage (and the changes are meant to be saved), then it must be written back to that secondary storage.">flush</a>
from the buffer pool when new data must be stored, such as <a
href="section-14.html#least-recently-used" class="term"
title="Abbreviated LRU, it is a popular heuristic to use for deciding which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. Analogous to the move-to-front heuristic for maintaining a self-organising list.">least
recently used</a>.</p>
</dd>
<dt><dfn id="buffer">buffer</dfn></dt>
<dd>
<p>A block of memory, most often in <a
href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a>. The size of a buffer is typically one or a multiple of the
basic unit of I/O that is read or written on each access to <a
href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a> such as a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>.</p>
</dd>
<dt><dfn id="buffering">buffering</dfn></dt>
<dd>
<p>A synonym for <a href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">caching</a>.
More specifically, it refers to an arrangement where all accesses to
data (such as on a <a href="section-14.html#peripheral-storage"
class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> device) must be done in multiples of some minimum unit of
storage. On a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, this basic or smallest unit of I/O is a <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>.
It is called “buffering” because the block of data returned by such an
access is stored in a <a href="section-14.html#buffer" class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>.</p>
</dd>
<dt><dfn id="caching">caching</dfn></dt>
<dd>
<p>The concept of keeping selected data in <a
href="section-14.html#main-memory" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">main
memory</a>. The goal is to have in main memory the data values that are
most likely to be used in the near future. An example of a caching
technique is the use of a <a href="section-14.html#buffer-pool"
class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>.</p>
</dd>
<dt><dfn id="call-stack">call stack</dfn></dt>
<dd>
<p>Known also as execution stack. A stack that stores the function call
sequence and the return address for each function.</p>
</dd>
<dt><dfn id="cartesian-product">Cartesian product</dfn></dt>
<dd>
<p>For sets, this is another name for the <a
href="section-14.html#set-product" class="term"
title="Written $\mathbf{Q} \times \mathbf{P}$, the set product is a set of ordered pairs such that ordered pair $(a, b)$ is in the product whenever $a \in \mathbf{P}$ and $b \in \mathbf{Q}$. For example, when $\mathbf{P} = {2, 3, 5}$ and $\mathbf{Q} = {5, 10}$, $\mathbf{Q} \times \mathbf{P} = {(2, 5), (2, 10), (3, 5), (3, 10), (5, 5), (5, 10)}$.">set
product</a>.</p>
</dd>
<dt><dfn id="ceiling">ceiling</dfn></dt>
<dd>
<p>Written
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⌈</mo><mi>x</mi><mo stretchy="false" form="postfix">⌉</mo></mrow><annotation encoding="application/x-tex">\lceil x \rceil</annotation></semantics></math>,
for real value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
the ceiling is the least integer
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≥</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\geq x</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="child">child</dfn></dt>
<dd>
<p>In a tree, the set of <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
directly pointed to by a node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
are the <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="circular-first-fit">circular first fit</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, circular first fit is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
for deciding which <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> to use
when allocating memory from a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. Circular first fit is a minor modification on <a
href="section-14.html#first-fit" class="term"
title="In a memory manager, first fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. First fit will always allocate the first free block on the free block list that is large enough to service the memory request. The advantage of this approach is that it is typically not necessary to look at all free blocks on the free block list to find a suitable free block. The disadvantage is that it is not &#39;intelligently&#39; selecting what might be a better choice of free block.">first
fit</a> memory allocation, where the last free block allocated from is
remembered, and search for the next suitable free block picks up from
there. Like first fit, it has the advantage that it is typically not
necessary to look at all free blocks on the free block list to find a
suitable free block. And it has the advantage over first fit that it
spreads out memory allocations evenly across the <a
href="section-14.html#free-block-list" class="term"
title="In a memory manager, the list that stores the necessary information about the current free blocks. Generally, this is done with some sort of linked list, where each node of the linked list indicates the start position and length of the free block in the memory pool.">free
block list</a>. This might help to minimise <a
href="section-14.html#external-fragmentation" class="term"
title="A condition that arises when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests.">external
fragmentation</a>.</p>
</dd>
<dt><dfn id="circular-list">circular list</dfn></dt>
<dd>
<p>A <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>
ADT implementation variant where the last element of the list provides
access to the first element of the list.</p>
</dd>
<dt><dfn id="class-hierarchy">class hierarchy</dfn></dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, a set of classes and their interrelationships. One of
the classes is the <a href="section-14.html#base-class" class="term"
title="In object-oriented programming, a class from which another class inherits. The class that inherits is called a subclass.">base
class</a>, and the others are <a href="section-14.html#subclass"
class="term"
title="In object-oriented programming, any class within a class hierarchy that inherits from some other class.">subclasses</a>
that <a href="section-14.html#inherit" class="term"
title="In object-oriented programming, the process by which a subclass gains data members and methods from a base class.">inherit</a>
either directly or indirectly from the base class.</p>
</dd>
<dt><dfn id="class">class</dfn></dt>
<dd>
<p>In the <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming paradigm</a> an ADT and its implementation together make up
a class. An instantiation of a class within a program is termed an <a
href="section-14.html#object" class="term"
title="An instance of a class, that is, something that is created and takes up storage during the execution of a computer program. In the object-oriented programming paradigm, objects are the basic units of operation. Objects have state in the form of data members, and they know how to perform certain actions (methods).">object</a>.</p>
</dd>
<dt><dfn id="clause">clause</dfn></dt>
<dd>
<p>In a <a href="section-14.html#boolean-expression" class="term"
title="A Boolean expression is comprised of Boolean variable combined using the operators AND ($\cdot$), OR ($+$), and NOT (to negate Boolean variable $x$ we write $\overline{x}$).">Boolean
expression</a>, a clause is one or more <a
href="section-14.html#literal" class="term"
title="In a Boolean expression, a literal is a Boolean variable or its negation. In the context of compilers, it is any constant value. Similar to a terminal.">literals</a>
OR’ed together.</p>
</dd>
<dt><dfn id="client">client</dfn></dt>
<dd>
<p>The user of a service. For example, the object or part of the program
that calls a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a> class is the client of that memory manager. Likewise the
class or code that calls a <a href="section-14.html#buffer-pool"
class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>.</p>
</dd>
<dt><dfn id="clique">clique</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a clique is a <a href="section-14.html#subgraph"
class="term"
title="A subgraph $\mathbf{S}$ is formed from graph $\mathbf{G}$ by selecting a subset $\mathbf{V}_s$ of $\mathbf{G}$&#39;s vertices and a subset $\mathbf{E}_s$ of $\mathbf{G}$&#39;s edges such that for every edge $e \in \mathbf{E}_s$, both vertices of $e$ are in $\mathbf{V}_s$.">subgraph</a>,
defined as any <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>
of the graph’s <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> such that every
vertex in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>
has an <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
to every other vertex in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>.
The size of the clique is the number of vertices in the clique.</p>
</dd>
<dt><dfn>closed hash system</dfn></dt>
<dd>
<p>See <a href="section-14.html#open-addressing" class="term"
title="A hash system where all records are stored in slots of the hash table. This is in contrast to an open hash system.">open
addressing</a></p>
</dd>
<dt><dfn id="closed-form-solution">closed-form solution</dfn></dt>
<dd>
<p>An algebraic equation with the same value as a <a
href="section-14.html#summation" class="term"
title="The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to $n$ can be written as $\sum_{i=1}^{n} i$.">summation</a>
or <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>. The process of replacing the summation or recurrence with
its closed-form solution is known as solving the summation or
recurrence.</p>
</dd>
<dt><dfn id="closed">closed</dfn></dt>
<dd>
<p>A set is closed over a (binary) operation if, whenever the operation
is applied to two members of the set, the result is a member of the
set.</p>
</dd>
<dt><dfn id="cluster">cluster</dfn></dt>
<dd>
<p>In <a href="section-14.html#file-processing" class="term"
title="The domain with computer science that deals with processing data stored on a disk drive (in a file), or more broadly, dealing with data stored on any peripheral storage device. Two fundamental properties make dealing with data on a peripheral device different from dealing with data in main memory: (1) Reading/writing data on a peripheral storage device is far slower than reading/writing data to main memory (for example, a typical disk drive is about a million times slower than RAM). (2) All I/O to a peripheral device is typically in terms of a block of data (for example, nearly all disk drives do all I/O in terms of blocks of 512 bytes).">file
processing</a>, a collection of physically adjacent <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sectors</a>
that define the smallest allowed allocation unit of space to a disk
file. The idea of requiring space to be allocated in multiples of
sectors is that this will reduce the number of <a
href="section-14.html#extent" class="term"
title="A physically contiguous block of sectors on a disk drive that are all part of a given disk file. The fewer extents needed to store the data for a disk file, generally the fewer seek operations that will be required to process a series of disk access operations on that file.">extents</a>
required to store the file, which reduces the expected number of <a
href="section-14.html#seek" class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operations reuquired to process a series of <a
href="section-14.html#disk-access" class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
accesses</a> to the file. The disadvantage of large cluster size is that
it increases <a href="section-14.html#internal-fragmentation"
class="term"
title="A condition that occurs when more than $m$ bytes are allocated to service a memory request for $m$ bytes, wasting free storage. This is often done to simplify memory management.">internal
fragmentation</a> since any space not actually used by the file in the
last cluster is wasted.</p>
</dd>
<dt><dfn>CNF</dfn></dt>
<dd>
<p>See <a href="section-14.html#conjunctive-normal-form" class="term"
title="A Boolean expression written as a series of clauses that are AND&#39;ed together.">conjunctive
normal form</a></p>
</dd>
<dt><dfn id="code-generation">code generation</dfn></dt>
<dd>
<p>A phase in a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that transforms <a href="section-14.html#intermediate-code" class="term"
title="A step in a typical compiler is to transform the original high-level language into a form on which it is easier to do other stages of the process. For example, some compilers will transform the original high-level source code into assembly code on which it can do code optimisation, before translating it into its final executable form.">intermediate
code</a> into the final executable form of the code. More generally,
this can refer to the process of turning a parse tree (that determines
the correctness of the structure of the program) into actual
instructions that the computer can execute.</p>
</dd>
<dt><dfn id="code-optimisation">code optimisation</dfn></dt>
<dd>
<p>A phase in a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that makes changes in the code (typically <a
href="section-14.html#assembly-code" class="term"
title="A form of intermediate code created by a compiler that is easy to convert into the final form that the computer can execute. An assembly language is typically a direct mapping of one or a few instructions that the CPU can execute into a mnemonic form that is relatively easy for a human to read.">assembly
code</a>) with the goal of replacing it with a version of the code that
will run faster while performing the same computation.</p>
</dd>
<dt><dfn id="cohesion">cohesion</dfn></dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, a term that refers to the degree to which a class has a
single well-defined role or responsibility.</p>
</dd>
<dt><dfn id="collatz-sequence">Collatz sequence</dfn></dt>
<dd>
<p>For a given integer value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
the sequence of numbers that derives from performing the following
computatin on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="op">(</span>n <span class="op">&gt;</span> <span class="dv">1</span><span class="op">)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>ODD<span class="op">(</span>n<span class="op">))</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> n <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> n <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span></code></pre></div>
<p>This is famous because, while it terminates for any value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
that you try, it has never been proven to be a fact that this always
terminates.</p>
</dd>
<dt><dfn id="collision-resolution-policy">collision resolution
policy</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
the process of resolving a <a href="section-14.html#collision"
class="term"
title="In a hash system, this refers to the case where two search keys are mapped by the hash function to the same slot in the hash table. This can happen on insertion or search when another record has already been hashed to that slot. In this case, a closed hash system will require a process known as collision resolution to find the location of the desired record.">collision</a>.
Specifically in a <a href="section-14.html#closed-hash-system"
class="term"
title="A hash system where all records are stored in slots of the hash table. This is in contrast to an open hash system.">closed
hash system</a>, this is the process of finding the proper position in a
<a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> that contains the desired record if the <a
href="section-14.html#hash-function" class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a> did not return the correct position for that record due to
a <a href="section-14.html#collision" class="term"
title="In a hash system, this refers to the case where two search keys are mapped by the hash function to the same slot in the hash table. This can happen on insertion or search when another record has already been hashed to that slot. In this case, a closed hash system will require a process known as collision resolution to find the location of the desired record.">collision</a>
with another record.</p>
</dd>
<dt><dfn id="collision-resolution">collision resolution</dfn></dt>
<dd>
<p>The outcome of a <a
href="section-14.html#collision-resolution-policy" class="term"
title="In hashing, the process of resolving a collision. Specifically in a closed hash system, this is the process of finding the proper position in a hash table that contains the desired record if the hash function did not return the correct position for that record due to a collision with another record.">collision
resolution policy</a>.</p>
</dd>
<dt><dfn id="collision">collision</dfn></dt>
<dd>
<p>In a <a href="section-14.html#hash-system" class="term"
title="The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key.">hash
system</a>, this refers to the case where two search <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">keys</a>
are mapped by the <a href="section-14.html#hash-function" class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a> to the same slot in the <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. This can happen on insertion or search when another record
has already been hashed to that slot. In this case, a <a
href="section-14.html#closed-hash-system" class="term"
title="A hash system where all records are stored in slots of the hash table. This is in contrast to an open hash system.">closed
hash system</a> will require a process known as <a
href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> to find the location of the desired record.</p>
</dd>
<dt><dfn id="comparable">comparable</dfn></dt>
<dd>
<p>The concept that two objects can be compared to determine if they are
equal or not, or to determine which one is greater than the other. In
set notation, elements
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
of a set are comparable under a given relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
if either
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi>R</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">xRy</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mi>R</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">yRx</annotation></semantics></math>.
To be reliably compared for a greater/lesser relationship, the values
being compared must belong to a <a href="section-14.html#total-order"
class="term"
title="A binary relation on a set where every pair of distinct elements in the set are comparable (that is, one can determine which of the two is greater than the other).">total
order</a>. In programming, the property of a data type such that two
elements of the type can be compared to determine if they are the same
(a weaker version), or which of the two is larger (a stronger version).
<code>Comparable</code> is also the name of an <a
href="section-14.html#interface" class="term"
title="An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members.">interface</a>
in Java that asserts a comparable relationship between objects within a
class, and <code>.compareTo()</code> is the <code>Comparable</code>
interface method that implements the actual comparison between two
objects of the class.</p>
</dd>
<dt><dfn id="comparator">comparator</dfn></dt>
<dd>
<p>A function given as a parameter to a method of a library (or
alternatively, a parameter for a C++ template or a Java generic). The
comparator function concept provides a generic way to encapulate the
process of performing a comparison between two objects of a specific
type. For example, if we want to write a generic sorting routine, that
can handle any record type, we can require that the user of the sorting
routine passes in a comparator function to define how records in the
collection are to be compared.</p>
</dd>
<dt><dfn id="comparison">comparison</dfn></dt>
<dd>
<p>The act of comparing two <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">keys</a>
or <a href="section-14.html#record" class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">records</a>.
For many <a href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
types</a>, a comparison has constant time cost. The number of
comparisons required is often used as a <a
href="section-14.html#measure-of-cost" class="term"
title="When comparing two things, such as two algorithms, some event or unit must be used as the basic unit of comparison. It might be number of milliseconds needed or machine instructions expended by a program, but it is usually desirable to have a way to do comparison between two algorithms without writing a program. Thus, some other measure of cost might be used as a basis for comparison between the algorithms. For example, when comparing two sorting algorthms it is traditional to use as a measure of cost the number of comparisons made between the key values of record pairs.">measure
of cost</a> for sorting and searching algorithms.</p>
</dd>
<dt><dfn id="compile-time-polymorphism">compile-time
polymorphism</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#polymorphism" class="term"
title="An object-oriented programming term meaning *one name, many forms*. It describes the ability of software to change its behaviour dynamically. Two basic forms exist: run-time polymorphism and compile-time polymorphism.">polymorphism</a>
known as Overloading. Overloaded methods have the same names, but
different signatures as a method available elsewhere in the class.
Compare to <a href="section-14.html#run-time-polymorphism" class="term"
title="A form of polymorphism known as Overriding. Overridden methods are those which implement a new method with the same signature as a method inherited from its base class. Compare to compile-time polymorphism.">run-time
polymorphism</a>.</p>
</dd>
<dt><dfn id="compiler">compiler</dfn></dt>
<dd>
<p>A computer program that reads computer programs and converts them
into a form that can be directly excecuted by some form of computer. The
major phases in a compiler include <a
href="section-14.html#lexical-analysis" class="term"
title="A phase of a compiler or interpreter responsible for reading in characters of the program or language and grouping them into tokens.">lexical
analysis</a>, <a href="section-14.html#syntax-analysis" class="term"
title="A phase of compilation that accepts tokens, checks if program is syntactically correct, and then generates a parse tree.">syntax
analysis</a>, <a href="section-14.html#intermediate-code-generation"
class="term"
title="A phase in a compiler, that walks through a parse tree to produce simple assembly code.">intermediate
code generation</a>, <a href="section-14.html#code-optimisation"
class="term"
title="A phase in a compiler that makes changes in the code (typically assembly code) with the goal of replacing it with a version of the code that will run faster while performing the same computation.">code
optimisation</a>, and <a href="section-14.html#code-generation"
class="term"
title="A phase in a compiler that transforms intermediate code into the final executable form of the code. More generally, this can refer to the process of turning a parse tree (that determines the correctness of the structure of the program) into actual instructions that the computer can execute.">code
generation</a>. More broadly, a compiler can be viewed as <a
href="section-14.html#parser" class="term"
title="A part of a compiler that takes as input the program text (or more typically, the tokens from the scanner), and verifies that the program is syntactically correct. Typically it will build a parse tree as part of the process.">parsing</a>
the program to verify that it is syntactically correct, and then doing
<a href="section-14.html#code-generation" class="term"
title="A phase in a compiler that transforms intermediate code into the final executable form of the code. More generally, this can refer to the process of turning a parse tree (that determines the correctness of the structure of the program) into actual instructions that the computer can execute.">code
generation</a> to convert the hig-level program into something that the
computer can execute.</p>
</dd>
<dt><dfn id="complete-binary-tree">complete binary tree</dfn></dt>
<dd>
<p>A binary tree where the nodes are filled in row by row, with the
bottom row filled in left to right. Due to this requirement, there is
only one tree of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
nodes for any value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.
Since storing the records in an <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
in row order leads to a simple mapping from a node’s position in the
array to its <a href="section-14.html#parent" class="term"
title="In a tree, the node $P$ that directly links to a node $A$ is the parent of $A$. $A$ is the child of $P$.">parent</a>,
<a href="section-14.html#sibling" class="term"
title="In a tree, a sibling of node $A$ is any other node with the same parent as $A$.">siblings</a>,
and <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>,
the array representation is most commonly used to implement the complete
binary tree. The <a href="section-14.html#heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
data structure is a complete binary tree with partial ordering
constraints on the node values.</p>
</dd>
<dt><dfn id="complete-graph">complete graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
where every <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> connects to every
other vertex.</p>
</dd>
<dt><dfn id="complex-number">complex number</dfn></dt>
<dd>
<p>In mathematics, an imaginary number, that is, a number with a real
component and an imaginary component.</p>
</dd>
<dt><dfn id="complexity">complexity</dfn></dt>
<dd>
<p>After fixing a <a href="section-14.html#cost-model" class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a> for a problem, we can calculate the complexity function of an
algorithm. This sends an input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
to the cost of running the algorithm on input of that size. For each
fixed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
we consider only the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst-case</a>
input of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.
This defines the worst-case complexity of the algorithm. There is also
the <a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average-case</a>
and <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best-case</a>
complexity, which are defined similarly.</p>
<p>We speak of time complexity, space complexity, etc. depending on what
kind of cost our cost model captures. Here, time refers to runtime and
space refers to memory/storage. The case of time complexity is the
default, so we omit the word “time”.</p>
</dd>
<dt><dfn id="composite-design-pattern">composite design
pattern</dfn></dt>
<dd>
<p>Given a class hierarchy representing a set of objects, and a
container for a collection of objects, the composite <a
href="section-14.html#design-pattern" class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a> addresses the relationship between the object hierarchy and
a bunch of behaviours on the objects. In the composite design, each
object is required to implement the collection of behaviours. This is in
contrast to the procedural approach where a behaviour (such as a tree <a
href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>)
is implemented as a method on the object collection (such as a <a
href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>).
Procedural tree traversal requires that the tree have a method that
understands what to do when it encounters any of the object types (<a
href="section-14.html#internal-node" class="term"
title="In a tree, any node that has at least one non-empty child is an internal node.">internal</a>
or <a href="section-14.html#leaf-node" class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaf
nodes</a>) that the tree might contain. The composite approach would
have the tree call the “traversal” method on its root node, which then
knows how to perform the “traversal” behaviour. This might in turn
require invoking the traversal method of other objects (in this case,
the children of the root).</p>
</dd>
<dt><dfn id="composite-type">composite type</dfn></dt>
<dd>
<p>A <a href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
type</a> whose <a href="section-14.html#member" class="term"
title="In set notation, this is a synonym for element. In abstract design, a data item is a member of a type. In an object-oriented language, data members are data fields in an object.">members</a>
have subparts. For example, a typical database record.</p>
</dd>
<dt><dfn id="composition">composition</dfn></dt>
<dd>
<p>Relationships between classes based on usage rather than <a
href="section-14.html#inherit" class="term"
title="In object-oriented programming, the process by which a subclass gains data members and methods from a base class.">inheritance</a>,
i.e. a <strong>HAS-A</strong> relationship. For example, some code in
class ‘A’ has a <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to some other class ‘B’.</p>
</dd>
<dt><dfn id="computability">computability</dfn></dt>
<dd>
<p>A branch of computer science that deals with the theory of solving
problems through computation. More specificially, it deals with the
limits to what problems (functions) are computable. An example of a
famous problem that cannot in principle be solved by a computer is the
<a href="section-14.html#halting-problem" class="term"
title="The halting problem is to answer this question: Given a computer program $P$ and an input $I$, will program $P$ halt when executed on input $I$? This problem has been proved impossible to solve in the general case. Thus, it is an example of an unsolvable problem.">halting
problem</a>.</p>
</dd>
<dt><dfn id="computation">computation</dfn></dt>
<dd>
<p>In a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a>, a computation is a sequence of <a
href="section-14.html#configuration" class="term"
title="For a finite automata, a complete specification for the current condition of the machine on some input string. This includes the current state that the machine is in, and the current condition of the string, including which character is about to be processed.">configurations</a>
for some length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n \geq 0</annotation></semantics></math>.
In general, it is a series of operations that the machine performs.</p>
</dd>
<dt><dfn id="computational-complexity-theory">computational complexity
theory</dfn></dt>
<dd>
<p>A branch of the theory of computation in theoretical computer science
and mathematics that focuses on classifying computational problems
according to their inherent difficulty, and relating those classes to
each other. An example is the study of <a
href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-Complete</a>
problems.</p>
</dd>
<dt><dfn id="configuration">configuration</dfn></dt>
<dd>
<p>For a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a>, a complete specification for the current condition of the
machine on some input string. This includes the current <a
href="section-14.html#state" class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">state</a>
that the machine is in, and the current condition of the string,
including which character is about to be processed.</p>
</dd>
<dt><dfn id="conjunctive-normal-form">conjunctive normal form</dfn>
(<dfn id="cnf">CNF</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#boolean-expression" class="term"
title="A Boolean expression is comprised of Boolean variable combined using the operators AND ($\cdot$), OR ($+$), and NOT (to negate Boolean variable $x$ we write $\overline{x}$).">Boolean
expression</a> written as a series of <a href="section-14.html#clause"
class="term"
title="In a Boolean expression, a clause is one or more literals OR&#39;ed together.">clauses</a>
that are AND’ed together.</p>
</dd>
<dt><dfn id="connected-component">connected component</dfn></dt>
<dd>
<p>In an <a href="section-14.html#undirected-graph" class="term"
title="A graph whose edges do not have a direction.">undirected
graph</a>, a <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
of the <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
such that each node in the subset can be reached from any other node in
that subset.</p>
</dd>
<dt><dfn id="connected-graph">connected graph</dfn></dt>
<dd>
<p>An <a href="section-14.html#undirected-graph" class="term"
title="A graph whose edges do not have a direction.">undirected
graph</a> is a connected graph if there is at least one path from any <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> to any other.</p>
</dd>
<dt><dfn id="constant-running-time">constant running time</dfn></dt>
<dd>
<p>The cost of a function whose running time is not related to its input
size. In
big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>
notation, this is traditionally written as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="constructive-induction">constructive induction</dfn></dt>
<dd>
<p>A process for finding the <a
href="section-14.html#closed-form-solution" class="term"
title="An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence.">closed
form</a> for a <a href="section-14.html#recurrence-relation"
class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>, that involves substituting in a guess for the closed form
to replace the recursive part(s) of the recurrence. Depending on the
goal (typically either to show that the hypothesised growth rate is
right, or to find the precise constants), one then manipulates the
resulting non-recursive equation.</p>
</dd>
<dt><dfn>container class</dfn></dt>
<dd>
<p>See <a href="section-14.html#container" class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a></p>
</dd>
<dt><dfn id="container">container</dfn> (<dfn
id="container-class">container class</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#data-structure" class="term"
title="The implementation for an ADT.">data structure</a> that stores a
collection of <a href="section-14.html#record" class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">records</a>.
Typical examples are <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">arrays</a>,
<a href="section-14.html#search-tree" class="term"
title="A tree data structure that makes search by key value more efficient. A type of container, it is common to implement an index using a search tree. A good search tree implementation will guarentee that insertion, deletion, and search operations are all $O(\log n)$.">search
trees</a>, and <a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
tables</a>.</p>
</dd>
<dt><dfn id="context-free-grammar">context-free grammar</dfn></dt>
<dd>
<p>A <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>
comprised only of productions of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">A \rightarrow x</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is a <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminal</a>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is a series of one or more <a href="section-14.html#terminal"
class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminals</a>
and non-terminals. That is, the given non-terminal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
can be replaced at any time.</p>
</dd>
<dt><dfn id="context-free-language">context-free language</dfn></dt>
<dd>
<p>The set of <a href="section-14.html#language" class="term"
title="A set of strings.">languages</a> that can be defined by <a
href="section-14.html#context-sensitive-grammar" class="term"
title="A grammar comprised only of productions of the form $xAy \rightarrow xvy$ where $A$ is a non-terminal and $x$ and $y$ are each a series of one or more terminals and non-terminals. That is, the given non-terminal $A$ can be replaced only when it is within the proper context.">context-sensitive
grammars</a>.</p>
</dd>
<dt><dfn id="context-sensitive-grammar">context-sensitive
grammar</dfn></dt>
<dd>
<p>A <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>
comprised only of productions of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi>A</mi><mi>y</mi><mo>→</mo><mi>x</mi><mi>v</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">xAy \rightarrow xvy</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is a <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminal</a>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
are each a series of one or more <a href="section-14.html#terminal"
class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminals</a>
and non-terminals. That is, the given non-terminal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
can be replaced only when it is within the proper context.</p>
</dd>
<dt><dfn id="cost-model">cost model</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, a definition for the cost of each <a
href="section-14.html#basic-operation" class="term"
title="Examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and finding a specified data item.">basic
operation</a> performed by the algorithm, along with a definition for
the size of the input. Having these definitions allows us to calculate
the <a href="section-14.html#cost" class="term"
title="The amount of resources that given run of an algorithm consumes.">cost</a>
to run the algorithm on a given input, and from there determine the <a
href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of the algorithm.</p>
<p>By default, the cost model approximates the runtime of the program.
To stress this, we also speak of time complexity. It is also possible to
model other kinds of costs. In the case of memory/storage, we speak of
space complexity.</p>
<p>Looking at the <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> of the complexity function tells us the <a
href="section-14.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortised complexity.">asymptotic
complexity</a> of the algorithm. A cost model would be considered “good”
if it yields predictions that conform to our understanding of
reality.</p>
</dd>
<dt><dfn id="cost">cost</dfn></dt>
<dd>
<p>The amount of resources that given run of an algorithm consumes.</p>
</dd>
<dt><dfn>countable</dfn></dt>
<dd>
<p>See <a href="section-14.html#countably-infinite" class="term"
title="A set is countably infinite if it contains a finite number of elements, or (for a set with an infinite number of elements) if there exists a one-to-one mapping from the set to the set of integers.">countably
infinite</a></p>
</dd>
<dt><dfn id="countably-infinite">countably infinite</dfn> (<dfn
id="countable">countable</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#set" class="term"
title="A collection of distinguishable members or elements.">set</a> is
countably infinite if it contains a finite number of elements, or (for a
set with an infinite number of elements) if there exists a one-to-one
mapping from the set to the set of integers.</p>
</dd>
<dt><dfn id="cpu">CPU</dfn></dt>
<dd>
<p>Acronym for <em>Central Processing Unit</em>, the primary processing
device for a computer.</p>
</dd>
<dt><dfn id="current-position">current position</dfn></dt>
<dd>
<p>A property of some list ADTs, where there is maintained a “current
position” state that can be referred to later.</p>
</dd>
<dt><dfn id="cycle">cycle</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a <a href="section-14.html#cycle" class="term"
title="In graph terminology, a cycle is a path of length three or more that connects some vertex $v_1$ to itself.">cycle</a>
is a <a href="section-14.html#path" class="term"
title="In tree or graph terminology, a sequence of vertices $v_1, v_2, ..., v_n$ forms a path of length $n-1$ if there exist edges from $v_i$ to $v_{i+1}$ for $1 \leq i &lt; n$.">path</a>
of length three or more that connects some <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mn>1</mn></msub><annotation encoding="application/x-tex">v_1</annotation></semantics></math>
to itself.</p>
</dd>
<dt><dfn id="cylinder-index">cylinder index</dfn></dt>
<dd>
<p>In the <a href="section-14.html#isam" class="term"
title="An obsolete method for indexing data for (at the time) fast retrieval. More generally, the term is used also to generically refer to an index that supports both sequential and keyed access to data records. Today, that would nearly always be implemented using a B-tree.">ISAM</a>
system, a simple <a href="section-14.html#linear-index" class="term"
title="A form of indexing that stores key-value pairs in a sorted array. Typically this is used for an index to a large collection of records stored on disk, where the linear index itself might be on disk or in main memory. It allows for efficient search (including for range queries), but it is not good for inserting and deleting entries in the array. Therefore, it is an ideal indexing structure for when the system needs to do range queries but the collection of records never changes once the linear index has been created.">linear
index</a> that stores the lowest key value stored in each <a
href="section-14.html#cylinder" class="term"
title="A disk drive normally consists of a stack of platters. While this might not be so true today, traditionally all of the I/O heads moved together during a seek operation. Thus, when a given I/O head is positioned over a particular track on a platter, the other I/O heads are also positioned over the corresponding track on their platters. That collection of tracks is called a cylinder. A given cylinder represents all of the data that can be read from all of the platters without doing another seek operation.">cylinder</a>.</p>
</dd>
<dt><dfn id="cylinder-overflow">cylinder overflow</dfn></dt>
<dd>
<p>In the <a href="section-14.html#isam" class="term"
title="An obsolete method for indexing data for (at the time) fast retrieval. More generally, the term is used also to generically refer to an index that supports both sequential and keyed access to data records. Today, that would nearly always be implemented using a B-tree.">ISAM</a>
system, this is space reserved for storing any records that can not fit
in their respective <a href="section-14.html#cylinder" class="term"
title="A disk drive normally consists of a stack of platters. While this might not be so true today, traditionally all of the I/O heads moved together during a seek operation. Thus, when a given I/O head is positioned over a particular track on a platter, the other I/O heads are also positioned over the corresponding track on their platters. That collection of tracks is called a cylinder. A given cylinder represents all of the data that can be read from all of the platters without doing another seek operation.">cylinder</a>.</p>
</dd>
<dt><dfn id="cylinder">cylinder</dfn></dt>
<dd>
<p>A <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> normally consists of a stack of <a
href="section-14.html#platter" class="term"
title="In a disk drive, one of a series of flat disks that comprise the storage space for the drive. Typically, each surface (top and bottom) of each platter stores data, and each surface has its own I/O head.">platters</a>.
While this might not be so true today, traditionally all of the <a
href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
heads</a> moved together during a <a href="section-14.html#seek"
class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operation. Thus, when a given I/O head is positioned over a particular
<a href="section-14.html#track" class="term"
title="On a disk drive, a concentric circle representing all of the sectors that can be viewed by the I/O head as the disk rotates. The significance is that, for a given placement of the I/O head, the sectors on the track can be read without performing a (relatively expensive) seek operation.">track</a>
on a platter, the other I/O heads are also positioned over the
corresponding track on their platters. That collection of tracks is
called a cylinder. A given cylinder represents all of the data that can
be read from all of the platters without doing another seek
operation.</p>
</dd>
<dt><dfn>DAG</dfn></dt>
<dd>
<p>See <a href="section-14.html#directed-acyclic-graph" class="term"
title="A graph with no cycles. Abbreviated as DAG. Note that a DAG is not necessarily a tree since a given node might have multiple parents.">directed
acyclic graph</a></p>
</dd>
<dt><dfn>data field</dfn></dt>
<dd>
<p>See <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
member</a></p>
</dd>
<dt><dfn id="data-item">data item</dfn></dt>
<dd>
<p>A piece of information or a record whose value is drawn from a
type.</p>
</dd>
<dt><dfn id="data-member">data member</dfn> (<dfn id="data-field">data
field</dfn>, <dfn id="attribute">attribute</dfn>, <dfn
id="instance-variable">instance variable</dfn>)</dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, the variables that together define the space required
by a data item are referred to as data members. Some of the commonly
used synonyms include <em>data field</em>, <em>attribute</em>, and
<em>instance variable</em>.</p>
</dd>
<dt><dfn id="data-structure">data structure</dfn></dt>
<dd>
<p>The implementation for an <a href="section-14.html#adt" class="term"
title="Abbreviated ADT. The specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behaviour of each operation is determined by its inputs and outputs. An ADT does not specify *how* the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation.">ADT</a>.</p>
</dd>
<dt><dfn id="data-type">data type</dfn></dt>
<dd>
<p>A type together with a collection of operations to manipulate the
type.</p>
</dd>
<dt><dfn id="deallocated">deallocated</dfn> (<dfn
id="deallocation">deallocation</dfn>)</dt>
<dd>
<p>Free the memory allocated to an unused object.</p>
</dd>
<dt><dfn>deallocation</dfn></dt>
<dd>
<p>See <a href="section-14.html#deallocated" class="term"
title="Free the memory allocated to an unused object.">deallocated</a></p>
</dd>
<dt><dfn id="decision-problem">decision problem</dfn></dt>
<dd>
<p>A problem whose output is either “YES” or “NO”.</p>
</dd>
<dt><dfn id="decision-tree">decision tree</dfn></dt>
<dd>
<p>A theoretical construct for modeling the behaviour of algorithms.
Each point at which the algorithm makes a decision (such as an if
statement) is modeled by a branch in the tree that represents the
algorithms behaviour. Decision trees can be used in <a
href="section-14.html#lower-bounds-proof" class="term"
title="A proof regarding the lower bound, with this term most typically referring to the lower bound for any possible algorithm to solve a given problem. Many problems have a simple lower bound based on the concept that the minimum amount of processing is related to looking at all of the problem&#39;s input. However, some problems have a higher lower bound than that. For example, the lower bound for the problem of sorting ($\Omega(n \log n)$) is greater than the input size to sorting ($n$). Proving such &#39;non-trivial&#39; lower bounds for problems is notoriously difficult.">lower
bounds proofs</a>, such as the proof that sorting requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
comparisons in the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>.</p>
</dd>
<dt><dfn id="deep-copy">deep copy</dfn></dt>
<dd>
<p>Copying the actual content of a <a href="section-14.html#pointee"
class="term"
title="The term pointee refers to anything that is pointed to by a pointer or reference.">pointee</a>.</p>
</dd>
<dt><dfn id="degree">degree</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, the degree for a <a href="section-14.html#vertex"
class="term" title="Another name for a node in a graph.">vertex</a> is
its number of <a href="section-14.html#neighbour" class="term"
title="In a graph, a node $w$ is said to be a neighbour of node $v$ if there is an edge from $v$ to $w$.">neighbours</a>.
In a <a href="section-14.html#directed-graph" class="term"
title="A graph whose edges each are directed from one of its defining vertices to the other.">directed
graph</a>, the <a href="section-14.html#in-degree" class="term"
title="In graph terminology, the in degree for a vertex is the number of edges directed into the vertex.">in
degree</a> is the number of edges directed into the vertex, and the <a
href="section-14.html#out-degree" class="term"
title="In graph terminology, the out degree for a vertex is the number of edges directed out of the vertex.">out
degree</a> is the number of edges directed out of the vertex. In <a
href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
terminology, the degree for a <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
is its number of <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.</p>
</dd>
<dt><dfn id="delegation-mental-model-for-recursion">delegation mental
model for recursion</dfn></dt>
<dd>
<p>A way of thinking about the process of <a
href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>.
The recursive function “delegates” most of the work when it makes the
recursive call. The advantage of the delegation mental model for
recursion is that you don’t need to think about how the delegated task
is performed. It just gets done.</p>
</dd>
<dt><dfn id="dense-graph">dense graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
where the actual number of <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
is a large fraction of the possible number of edges. Generally, this is
interpreted to mean that the <a href="section-14.html#degree"
class="term"
title="In graph terminology, the degree for a vertex is its number of neighbours. In a directed graph, the in degree is the number of edges directed into the vertex, and the out degree is the number of edges directed out of the vertex. In tree terminology, the degree for a node is its number of children.">degree</a>
for any <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> in the graph is
relatively high.</p>
</dd>
<dt><dfn id="depth-first-search-tree">depth-first search tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
that can be defined by the operation of a <a
href="section-14.html#depth-first-search" class="term"
title="A graph traversal algorithm. Whenever a $v$ is visited during the traversal, DFS will recursively visit all of $v$ &#39;s unvisited neighbours.">depth-first
search</a> (DFS) on a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.
This tree would consist of the <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
of the graph and a subset of the <a href="section-14.html#edge"
class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
of the graph that was followed during the DFS.</p>
</dd>
<dt><dfn id="depth-first-search">depth-first search</dfn> (<dfn
id="dfs">DFS</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
<a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
algorithm. Whenever a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
is <a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visited</a>
during the traversal, DFS will <a href="section-14.html#recursion"
class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursively</a>
visit all of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
’s <a href="section-14.html#unvisited" class="term"
title="In graph algorithms, this refers to a node that has not been processed at the current point in the algorithm. This information is typically maintained by using a mark array.">unvisited</a>
<a href="section-14.html#neighbour" class="term"
title="In a graph, a node $w$ is said to be a neighbour of node $v$ if there is an edge from $v$ to $w$.">neighbours</a>.</p>
</dd>
<dt><dfn id="depth">depth</dfn></dt>
<dd>
<p>The depth of a node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
in a tree is the length of the path from the root of the tree to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="dequeue">dequeue</dfn></dt>
<dd>
<p>A specialised term used to indicate removing an element from a
queue.</p>
</dd>
<dt><dfn id="dereference">dereference</dfn></dt>
<dd>
<p>Accessing the value of the <a href="section-14.html#pointee"
class="term"
title="The term pointee refers to anything that is pointed to by a pointer or reference.">pointee</a>
for some <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
variable. Commonly, this happens in a language like Java when using the
“dot” operator to access some field of an object.</p>
</dd>
<dt><dfn id="derivation">derivation</dfn></dt>
<dd>
<p>In formal languages, the process of executing a series of <a
href="section-14.html#production-rule" class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production
rules</a> from a <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>.
A typical example of a derivation would be the series of productions
executed to go from the <a href="section-14.html#start-symbol"
class="term"
title="In a grammar, the designated non-terminal that is the intial point for deriving a string in the langauge.">start
symbol</a> to a given string.</p>
</dd>
<dt><dfn id="descendant">descendant</dfn></dt>
<dd>
<p>In a tree, the set of all nodes that have a node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
as an <a href="section-14.html#ancestor" class="term"
title="In a tree, for a given node $A$, any node on a path from $A$ up to the root is an ancestor of $A$.">ancestor</a>
are the descendants of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
In other words, all of the nodes that can be reached from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
by progressing downwards in tree. Another way to say it is: The <a
href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
their children, and so on.</p>
</dd>
<dt><dfn id="deserialisation">deserialisation</dfn></dt>
<dd>
<p>The process of returning a <a href="section-14.html#serialisation"
class="term"
title="The process of taking a data structure in memory and representing it as a sequence of bytes. This is sometimes done in order to transmit the data structure across a network or store the data structure in a stream, such as on disk. Deserialisation reconstructs the original data structure from the serialised representation.">serialised</a>
representation for a data structure back to its original in-memory
form.</p>
</dd>
<dt><dfn id="design-pattern">design pattern</dfn></dt>
<dd>
<p>An abstraction for describing the design of programs, that is, the
interactions of objects and classes. Experienced software designers
learn and reuse patterns for combining software components, and design
patterns allow this design knowledge to be passed on to new programmers
more quickly.</p>
</dd>
<dt><dfn id="deterministic-algorithm">deterministic algorithm</dfn></dt>
<dd>
<p>An algorithm that does not involve any element of randomness, and so
its behaviour on a given input will always be the same. This is in
contrast to a <a href="section-14.html#randomised-algorithm"
class="term"
title="An algorithm that involves some form of randomness to control its behaviour. The ultimate goal of a randomised algorithm is to improve performance over a deterministic algorithm to solve the same problem. There are a number of variations on this theme. A &#39;Las Vegas algorithm&#39; returns a correct result, but the amount of time required might or might not improve over a deterministic algorithm. A &#39;Monte Carlo algorithm&#39; is a form of probabilistic algorithm that is not guarenteed to return a correct result, but will return a result relatively quickly.">randomised
algorithm</a>.</p>
</dd>
<dt><dfn>deterministic finite acceptor</dfn></dt>
<dd>
<p>See <a href="section-14.html#deterministic-finite-automata"
class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a></p>
</dd>
<dt><dfn id="deterministic-finite-automata">deterministic finite
automata</dfn> (<dfn id="deterministic-finite-acceptor">deterministic
finite acceptor</dfn>, <dfn id="dfa">DFA</dfn>)</dt>
<dd>
<p>An <a href="section-14.html#automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">automata</a>
or abstract machine that can process an input string (shown on a tape)
from left to right. There is a control unit (with <a
href="section-14.html#state" class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">states</a>),
behaviour defined for what to do when in a given state and with a given
symbol on the current square of the tape. All that we can “do” is change
state before going to the next letter to the right.</p>
</dd>
<dt><dfn id="deterministic">deterministic</dfn></dt>
<dd>
<p>Any <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a> in which, for every pair of <a href="section-14.html#state"
class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">state</a>
and symbol, there is only a single transition. This means that whenever
the machine is in a given state and sees a given symbol, only a single
thing can happen. This is in contrast to a <a
href="section-14.html#non-deterministic" class="term"
title="In a finite automata, at least one state has multiple transitions on at least one symbol. This means that it is not deterministic about what transition to take in that situation. A non-deterministic machine is said to accept a string if it completes execution on the string in an accepting state under at least one choice of non-deterministic transitions. Generally, non-determinism can be simulated with a deterministic machine by alternating between the execution that would take place under each of the branching choices.">non-deterministic</a>
finite automata, which has at least one state with multiple transitions
on at least one symbol.</p>
</dd>
<dt><dfn>DFA</dfn></dt>
<dd>
<p>See <a href="section-14.html#deterministic-finite-automata"
class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a></p>
</dd>
<dt><dfn>DFS</dfn></dt>
<dd>
<p>See <a href="section-14.html#depth-first-search" class="term"
title="A graph traversal algorithm. Whenever a $v$ is visited during the traversal, DFS will recursively visit all of $v$ &#39;s unvisited neighbours.">depth-first
search</a></p>
</dd>
<dt><dfn>DFT</dfn></dt>
<dd>
<p>See <a href="section-14.html#discrete-fourier-transform" class="term"
title="Let $a = a_0, a_1, ..., a_{n-1}^T$ be a vector that stores the coefficients for a polynomial being evaluated. We can then do the calculations to evaluate the polynomial at the $n$ th $roots of unity &lt;nth roots of unit&gt;$ by multiplying the $A_{z}$ matrix by the coefficient vector. The resulting vector $F_{z}$ is called the discrete Fourier transform (or DFT) for the polynomial.">discrete
Fourier transform</a></p>
</dd>
<dt><dfn id="diagonalisation-argument">diagonalisation
argument</dfn></dt>
<dd>
<p>A proof technique for proving that a set is <a
href="section-14.html#uncountably-infinite" class="term"
title="An infinite set is uncountably infinite if there does not exist any mapping from it to the set of integers. This is often proved using a diagonalisation argument. The real numbers is an example of an uncountably infinite set.">uncountably
infinite</a>. The approach is to show that, no matter what order the
elements of the set are put in, a new element of the set can be
constructed that is not in that ordering. This is done by changing the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
th value or position of the element to be different from that of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
th element in the proposed ordering.</p>
</dd>
<dt><dfn id="dictionary-search">dictionary search</dfn></dt>
<dd>
<p>A close relative of an <a href="section-14.html#interpolation-search"
class="term"
title="Given a sorted array, and knowing the first and last key values stored in some subarray known to contain search key $K$, interpolation search will compute the expected location of $K$ in the subarray as a fraction of the distance between the known key values. So it will next check that computed location, thus narrowing the search for the next iteration. Given reasonable key value distribution, the average case for interpolation search will be $O(\log \log n)$, or better than the expected cost of binary search. Nonetheless, binary search is expected to be faster in nearly all practical situations due to the small difference between the two costs, combined with the higher constant factors required to implement interpolation search as compared to binary search.">interpolation
search</a>. In a classical (paper) dictionary of words in a natural
language, there are markings for where in the dictionary the words with
a given letter start. So in typical usage of such a dictionary, words
are found by opening the dictionary to some appropriate place within the
pages that contain words starting with that letter.</p>
</dd>
<dt><dfn id="dictionary">dictionary</dfn></dt>
<dd>
<p>An abstract data type or <a href="section-14.html#interface"
class="term"
title="An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members.">interface</a>
for a data structure or software subsystem that supports insertion,
search, and deletion of records.</p>
</dd>
<dt><dfn>digraph</dfn></dt>
<dd>
<p>See <a href="section-14.html#directed-graph" class="term"
title="A graph whose edges each are directed from one of its defining vertices to the other.">directed
graph</a></p>
</dd>
<dt><dfn id="dijkstras-algorithm">Dijkstra’s algorithm</dfn></dt>
<dd>
<p>An algorithm to solve the <a
href="section-14.html#single-source-shortest-paths-problem" class="term"
title="Given a graph with weights or distances on the edges, and a designated start vertex $s$, find the shortest path from $s$ to every other vertex in the graph. One algorithm to solve this problem is Dijkstra&#39;s algorithm.">single-source
shortest paths problem</a> in a <a href="section-14.html#graph"
class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.
This is a <a href="section-14.html#greedy-algorithm" class="term"
title="An algorithm that makes locally optimal choices at each step.">greedy
algorithm</a>. It is nearly identical to <a
href="section-14.html#prims-algorithm" class="term"
title="A greedy algorithm for computing the MST of a graph. It is nearly identical to Dijkstra&#39;s algorithm for solving the single-source shortest paths problem, with the only difference being the calculation done to update the best-known distance.">Prim’s
algorithm</a> for finding a <a
href="section-14.html#minimum-spanning-tree" class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">minimum
spanning tree</a>, with the only difference being the calculation done
to update the best-known distance.</p>
</dd>
<dt><dfn id="diminishing-increment-sort">diminishing increment
sort</dfn></dt>
<dd>
<p>Another name for <a href="section-14.html#shellsort" class="term"
title="A sort that relies on the best-case cost of Insertion sort to improve over $O(n^2)$ worst case cost.">Shellsort</a>.</p>
</dd>
<dt><dfn id="direct-access">direct access</dfn></dt>
<dd>
<p>A storage device, such as a disk drive, that has some ability to move
to a desired data location more-or-less directly. This is in contrast to
a <a href="section-14.html#sequential-access" class="term"
title="In file processing terminology, the requirement that all records in a file are accessed in sequential order. Alternatively, a storage device that can only access data sequentially, such as a tape drive.">sequential
access</a> storage device such as a tape drive.</p>
</dd>
<dt><dfn id="direct-proof">direct proof</dfn></dt>
<dd>
<p>In general, a direct proof is just a “logical explanation”. A direct
proof is sometimes referred to as an argument by deduction. This is
simply an argument in terms of logic. Often written in English with
words such as “if ... then”, it could also be written with logic
notation such as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>⇒</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">P \Rightarrow Q</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="directed-acyclic-graph">directed acyclic graph</dfn> (<dfn
id="dag">DAG</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
with no cycles. Abbreviated as DAG. Note that a DAG is not necessarily a
<a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
since a given <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
might have multiple <a href="section-14.html#parent" class="term"
title="In a tree, the node $P$ that directly links to a node $A$ is the parent of $A$. $A$ is the child of $P$.">parents</a>.</p>
</dd>
<dt><dfn id="directed-edge">directed edge</dfn></dt>
<dd>
<p>An <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
that goes from <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> to another. In
contrast, an <a href="section-14.html#undirected-edge" class="term"
title="An edge that connects two vertices with no direction between them. Many graph representations will represent such an edge with two directed edges.">undirected
edge</a> simply links to vertices without a direction.</p>
</dd>
<dt><dfn id="directed-graph">directed graph</dfn> (<dfn
id="digraph">digraph</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
whose <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
each are directed from one of its defining <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> to the
other.</p>
</dd>
<dt><dfn id="dirty-bit">dirty bit</dfn></dt>
<dd>
<p>Within a <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>, a piece of information associated with each <a
href="section-14.html#buffer" class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
that indicates whether the contents of the buffer have changed since
being read in from <a href="section-14.html#backing-storage"
class="term"
title="In the context of a caching system or buffer pool, backing storage is the relatively large but slower source of data that needs to be cached. For example, in a virtual memory, the disk drive would be the backing storage. In the context of a web browser, the Internet might be considered the backing storage.">backing
storage</a>. When the buffer is <a href="section-14.html#flush"
class="term"
title="The act of removing data from a cache, most typically because other data considered of higher future value must replace it in the cache. If the data being flushed has been modified since it was first read in from secondary storage (and the changes are meant to be saved), then it must be written back to that secondary storage.">flushed</a>
from the buffer pool, the buffer’s contents must be written to the
backing storage if the dirty bit is set (that is, if the contents have
changed). This means that a relatively expensive write operation is
required. In contrast, if the dirty bit is not set, then it is
unnecessary to write the contents to backing storage, thus saving time
over not keeping track of whether the contents have changed or not.</p>
</dd>
<dt><dfn id="discrete-fourier-transform">discrete Fourier
transform</dfn> (<dfn id="dft">DFT</dfn>)</dt>
<dd>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mo stretchy="false" form="prefix">[</mo><msub><mi>a</mi><mn>0</mn></msub><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><msup><mo stretchy="false" form="postfix">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">a = [a_0, a_1, ..., a_{n-1}]^T</annotation></semantics></math>
be a vector that stores the coefficients for a polynomial being
evaluated. We can then do the calculations to evaluate the polynomial at
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
th
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>&lt;</mo><mi>n</mi><mi>t</mi><mi>h</mi><mi>r</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">roots of unity &lt;nth roots of unit&gt;</annotation></semantics></math>
by multiplying the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>z</mi></msub><annotation encoding="application/x-tex">A_{z}</annotation></semantics></math>
matrix by the coefficient vector. The resulting vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>F</mi><mi>z</mi></msub><annotation encoding="application/x-tex">F_{z}</annotation></semantics></math>
is called the discrete Fourier transform (or DFT) for the
polynomial.</p>
</dd>
<dt><dfn id="discriminator">discriminator</dfn></dt>
<dd>
<p>A part of a <a href="section-14.html#multi-dimensional-search-key"
class="term"
title="A search key containing multiple parts, that works in conjunction with a multi-dimensional search structure. Most typically, a spatial search key representing a position in multi-dimensional (2 or 3 dimensions) space. But a multi-dimensional key could be used to organise data within non-spatial dimensions, such as temperature and time.">multi-dimensional
search key</a>. Certain tree data structures such as the <a
href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>
and the <a href="section-14.html#kd-tree" class="term"
title="A spatial data structure that uses a binary tree to store a collection of data records based on their (point) location in space. It uses the concept of a discriminator at each level to decide which single component of the multi-dimensional search key to branch on at that level. It uses a key-space decomposition, meaning that all data records in the left subtree of a node have a value on the corresponding discriminator that is less than that of the node, while all data records in the right subtree have a greater value. The bintree is the image-space decomposition analog of the kd tree.">kd
tree</a> operate by making branching decisions at nodes of the tree
based on a single attribute of the multi-dimensional key, with the
attribute determined by the level of the node in the tree. For example,
in 2 dimensions, nodes at the odd levels in the tree might branch based
on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
value of a coordinate, while at the even levels the tree would branch
based on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
value of the coordinate. Thus, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
coordinate is the discriminator for the odd levels, while the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
coordinate is the discriminator for the even levels.</p>
</dd>
<dt><dfn id="disjoint-sets">disjoint sets</dfn></dt>
<dd>
<p>A collection of <a href="section-14.html#set" class="term"
title="A collection of distinguishable members or elements.">sets</a>,
any pair of which share no elements in common. A collection of disjoint
sets partitions some objects such that every object is in exactly one of
the disjoint sets.</p>
</dd>
<dt><dfn id="disjoint">disjoint</dfn></dt>
<dd>
<p>Two parts of a <a href="section-14.html#data-structure" class="term"
title="The implementation for an ADT.">data structure</a> or two
collections with no objects in common are disjoint. This term is often
used in conjunction with a data structure that has <a
href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
(such as a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>).
Also used in the context of <a href="section-14.html#set" class="term"
title="A collection of distinguishable members or elements.">sets</a>,
where two <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subsets</a>
are disjoint if they share no elements.</p>
</dd>
<dt><dfn id="disk-access">disk access</dfn></dt>
<dd>
<p>The act of reading data from a disk drive (or other form of <a
href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a>). The number of times data must be read from (or written to)
a disk is often a good measure of cost for an algorithm that involves
disk I/O, since this is usually the dominant cost.</p>
</dd>
<dt><dfn id="disk-controller">disk controller</dfn></dt>
<dd>
<p>The control mechanism for a <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. Responsible for the action of reading or writing a <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>
of data.</p>
</dd>
<dt><dfn id="disk-drive">disk drive</dfn></dt>
<dd>
<p>An example of <a href="section-14.html#peripheral-storage"
class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> or <a href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a>. Data access times are typically measured in thousandths of
a second (milliseconds), which is roughly a million times slower than
access times for <a href="section-14.html#ram" class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>,
which is an example of a <a href="section-14.html#primary-storage"
class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a> device. Reads from and writes to a disk drive are always
done in terms of some minimum size, which is typically called a <a
href="section-14.html#block" class="term"
title="A unit of storage, usually referring to storage on a disk drive or other peripheral storage device. A block is the basic unit of I/O for that device.">block</a>.
The block size is 512 bytes on most disk drives. Disk drives and RAM are
typical parts of a computer’s <a href="section-14.html#memory-hierarchy"
class="term"
title="The concept that a computer system stores data in a range of storage types that range from fast but expensive (primary storage) to slow but cheap (secondary storage). When there is too much data to store in primary storage, the goal is to have the data that is needed soon or most often in the primary storage as much as possible, by using caching techniques.">memory
hierarchy</a>.</p>
</dd>
<dt><dfn id="disk-i-o">disk I/O</dfn></dt>
<dd>
<p>Refers to the act of reading data from or writing data to a <a
href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. All disk reads and writes are done in units of a <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>
or <a href="section-14.html#block" class="term"
title="A unit of storage, usually referring to storage on a disk drive or other peripheral storage device. A block is the basic unit of I/O for that device.">block</a>.</p>
</dd>
<dt><dfn id="disk-based-space-time-tradeoff">disk-based space/time
tradeoff</dfn></dt>
<dd>
<p>In contrast to the standard <a
href="section-14.html#space-time-tradeoff" class="term"
title="Many programs can be designed to either speed processing at the cost of additional storage, or reduce storage at the cost of additional processing time.">space/time
tradeoff</a>, this principle states that the smaller you can make your
disk storage requirements, the faster your program will run. This is
because the time to read information from disk is enormous compared to
computation time, so almost any amount of additional computation needed
to unpack the data is going to be less than the disk-reading time saved
by reducing the storage requirements.</p>
</dd>
<dt><dfn id="distance">distance</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
representations, a synonym for <a href="section-14.html#weight"
class="term"
title="A cost or distance most often associated with an edge in a graph.">weight</a>.</p>
</dd>
<dt><dfn id="divide-and-conquer">divide and conquer</dfn></dt>
<dd>
<p>A technique for designing algorithms where a solution is found by
breaking the problem into smaller (similar) subproblems, solving the
subproblems, then combining the subproblem solutions to form the
solution to the original problem. This process is often implemented
using <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>.</p>
</dd>
<dt><dfn id="divide-and-conquer-recurrences">divide-and-conquer
recurrences</dfn></dt>
<dd>
<p>A common form of <a href="section-14.html#recurrence-relation"
class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a> that have the form</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>T</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>a</mi><mi>T</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mi>/</mi><mi>b</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mi>c</mi><msup><mi>n</mi><mi>k</mi></msup></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mi>T</mi><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mi>c</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
T(n) &amp;= a T(n/b) + cn^k \\
T(1) &amp;= c
\end{align*}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
are constants. In general, this recurrence describes a problem of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
divided into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
subproblems of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>/</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">n/b</annotation></semantics></math>,
while
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><msup><mi>n</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">cn^k</annotation></semantics></math>
is the amount of work necessary to combine the partial solutions.</p>
</dd>
<dt><dfn id="divide-and-guess">divide-and-guess</dfn></dt>
<dd>
<p>A technique for finding a <a
href="section-14.html#closed-form-solution" class="term"
title="An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence.">closed-form
solution</a> to a <a href="section-14.html#summation" class="term"
title="The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to $n$ can be written as $\sum_{i=1}^{n} i$.">summation</a>
or <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>.</p>
</dd>
<dt><dfn id="domain">domain</dfn></dt>
<dd>
<p>The set of possible inputs to a function.</p>
</dd>
<dt><dfn id="double-buffering">double buffering</dfn></dt>
<dd>
<p>The idea of using multiple <a href="section-14.html#buffer"
class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffers</a>
to allow the <a href="section-14.html#cpu" class="term"
title="Acronym for *Central Processing Unit*, the primary processing device for a computer.">CPU</a>
to operate in parallel with a <a
href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> device. Once the first buffer’s worth of data has been read
in, the CPU can process this while the next block of data is being read
from the peripheral storage. For this idea to work, the next block of
data to be processed must be known or predicted with reasonable
accuracy.</p>
</dd>
<dt><dfn id="double-hashing">double hashing</dfn></dt>
<dd>
<p>A <a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method. A second hash function is used to generate a
value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
on the key. That value is then used by this key as the step size in <a
href="section-14.html#linear-probing-by-steps" class="term"
title="In hashing, this collision resolution method is a variation on simple linear probing. Some constant $c$ is defined such that term $i$ of the probe sequence is $ci$. This means that collision resolution works by moving sequentially through the hash table from the home slot in steps of size $c$. While not much improvement on linear probing, it forms the basis of another collision resolution method called double hashing, where each key uses a value for $c$ defined by a second hash function.">linear
probing by steps</a>. Since different keys use different step sizes (as
generated by the second hash function), this process avoids the
clustering caused by standard linear probing by steps.</p>
</dd>
<dt><dfn id="double-rotation">double rotation</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#rebalancing-operation"
class="term"
title="An operation performed on balanced search trees, such as the AVL tree or splay tree, for the purpose of keeping the tree height balanced.">rebalancing
operation</a> used by the <a href="section-14.html#splay-tree"
class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a> and <a href="section-14.html#avl-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a splay tree in that it uses the concept of rotations in the insert and remove operations.">AVL
tree</a>.</p>
</dd>
<dt><dfn id="doubly-linked-list">doubly linked list</dfn></dt>
<dd>
<p>A <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> implementation variant where each list node contains access
pointers to both the previous element and the next element on the
list.</p>
</dd>
<dt><dfn id="dsa">DSA</dfn></dt>
<dd>
<p>Abbreviation for <em>data structures and algorithms</em>.</p>
</dd>
<dt><dfn id="dynamic-allocation">dynamic allocation</dfn></dt>
<dd>
<p>The act of creating an object from <a
href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>. In C++, Java, and Javascript, this is done using the
<code>new</code> operator.</p>
</dd>
<dt><dfn id="dynamic-array">dynamic array</dfn></dt>
<dd>
<p>Arrays, once allocated, are of fixed size. A dynamic array puts an <a
href="section-14.html#interface" class="term"
title="An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members.">interface</a>
around the array so as to appear to allow the array to grow and shrink
in size as necessary. Typically this is done by allocating a new copy,
copying the contents of the old array, and then returning the old array
to <a href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>. If done correctly, the <a
href="section-14.html#amortised-cost" class="term"
title="The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortised analysis, gives rise to the notion of amortised complexity.">amortised
cost</a> for dynamically resizing the array can be made constant. In
some programming languages such as Java, the term <a
href="section-14.html#vector" class="term"
title="In set notation, another term for a sequence. As a data structure, the term vector usually used as a snyonym for a dynamic array.">vector</a>
is used as a synonym for dynamic array.</p>
</dd>
<dt><dfn id="dynamic-memory-allocation">dynamic memory
allocation</dfn></dt>
<dd>
<p>A programming technique where linked objects in a data structure are
created from <a href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a> as needed. When no longer needed, the object is either
returned to <a href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a> or left as <a href="section-14.html#garbage" class="term"
title="In memory management, any memory that was previously (dynamically) allocated by the program during runtime, but which is no longer accessible since all pointers to the memory have been deleted or overwritten. In some languages, garbage can be recovered by garbage collection. In languages such as C and C++ that do not support garbage collection, so creating garbage is considered a memory leak.">garbage</a>,
depending on the programming language.</p>
</dd>
<dt><dfn id="dynamic-programming">dynamic programming</dfn></dt>
<dd>
<p>An approach to designing algorithms that works by storing a table of
results for subproblems. A typical cause for excessive cost in <a
href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursive</a>
algorithms is that different branches of the recursion might solve the
same subproblem. Dynamic programming uses a table to store information
about which subproblems have already been solved, and uses the stored
information to immediately give the answer for any repeated attempts to
solve that subproblem.</p>
</dd>
<dt><dfn id="dynamic">dynamic</dfn></dt>
<dd>
<p>Something that is changes (in contrast to <a
href="section-14.html#static" class="term"
title="Something that is not changing (in contrast to dynamic). In computer programming, static normally refers to something that happens at compile time. For example, static analysis is analysis of the program&#39;s text or structure, as opposed to its run-time behaviour. Static binding or static memory allocation occurs at compile time.">static</a>).
In computer programming, dynamic normally refers to something that
happens at run time. For example, run-time analysis is analysis of the
program’s behaviour, as opposed to its (static) text or structure
dynamic binding or dynamic memory allocation occurs at run time.</p>
</dd>
<dt><dfn id="edge">edge</dfn></dt>
<dd>
<p>The connection that links two <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
in a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>,
<a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a>, or <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.</p>
</dd>
<dt><dfn id="edit-distance">edit distance</dfn></dt>
<dd>
<p>Given strings
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>,
the edit distance is a measure for the number of editing steps required
to convert
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="efficient">efficient</dfn></dt>
<dd>
<p>A solution is said to be efficient if it solves the problem within
the required <a href="section-14.html#resource-constraints" class="term"
title="Examples of resource constraints include the total space available to store the data (possibly divided into separate main memory and disk space constraints) and the time allowed to perform each subtask.">resource
constraints</a>. A solution is sometimes said to be efficient if it
requires fewer resources than known alternatives, regardless of whether
it meets any particular requirements.</p>
</dd>
<dt><dfn id="element">element</dfn></dt>
<dd>
<p>One value or <a href="section-14.html#member" class="term"
title="In set notation, this is a synonym for element. In abstract design, a data item is a member of a type. In an object-oriented language, data members are data fields in an object.">member</a>
in a set.</p>
</dd>
<dt><dfn id="empirical-comparison">empirical comparison</dfn></dt>
<dd>
<p>An approach to comparing to things by actually seeing how they
perform. Most typically, we are referring to the comparison of two
programs by running each on a suite of test data and measuring the
actual running times. Empirical comparison is subject to many possible
complications, including unfair selection of test data, and inaccuracies
in the time measurements due to variations in the computing environment
between various executions of the programs.</p>
</dd>
<dt><dfn id="empty">empty</dfn></dt>
<dd>
<p>For a <a href="section-14.html#container" class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a>
class, the state of containing no <a href="section-14.html#element"
class="term" title="One value or member in a set.">elements</a>.</p>
</dd>
<dt><dfn id="encapsulation">encapsulation</dfn></dt>
<dd>
<p>In programming, the concept of hiding implementation details from the
user of an ADT, and protecting <a href="section-14.html#data-member"
class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
members</a> of an object from outside access.</p>
</dd>
<dt><dfn id="enqueue">enqueue</dfn></dt>
<dd>
<p>A specialised term used to indicate inserting an element onto a
queue.</p>
</dd>
<dt><dfn id="entry-sequenced-file">entry-sequenced file</dfn></dt>
<dd>
<p>A file that stores records in the order that they were added to the
file.</p>
</dd>
<dt><dfn id="enumeration">enumeration</dfn></dt>
<dd>
<p>The process by which a <a href="section-14.html#traversal"
class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
lists every object in the <a href="section-14.html#container"
class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a>
exactly once. Thus, a traversal that prints the <a
href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
is said to enumerate the nodes. An enumeration can also refer to the
actual listing that is produced by the traversal (as well as the process
that created that listing).</p>
</dd>
<dt><dfn id="equidistribution-property">equidistribution
property</dfn></dt>
<dd>
<p>In random number theory, this means that a given series of random
numbers cannot be described more briefly than simply listing it out.</p>
</dd>
<dt><dfn id="equivalence-class">equivalence class</dfn></dt>
<dd>
<p>An <a href="section-14.html#equivalence-relation" class="term"
title="Relation $R$ is an equivalence relation on set $\mathbf{S}$ if it is reflexive, symmetric, and transitive.">equivalence
relation</a> can be used to partition a set into equivalence
classes.</p>
</dd>
<dt><dfn id="equivalence-relation">equivalence relation</dfn></dt>
<dd>
<p>Relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
is an equivalence relation on set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
if it is <a href="section-14.html#reflexive" class="term"
title="In set notation, binary relation $R$ on set $S$ is reflexive if $aRa$ for all $a \in \mathbf{S}$.">reflexive</a>,
<a href="section-14.html#symmetric" class="term"
title="In set notation, relation $R$ is symmetric if whenever $aRb$, then $bRa$, for all $a, b \in \mathbf{S}$.">symmetric</a>,
and <a href="section-14.html#transitive" class="term"
title="In set notation, relation $R$ is transitive if whenever $aRb$ and $bRc$, then $aRc$, for all $a, b, c \in \mathbf{S}$.">transitive</a>.</p>
</dd>
<dt><dfn id="estimation">estimation</dfn></dt>
<dd>
<p>As a technical skill, this is the process of generating a rough
estimate in order to evaluate the feasibility of a proposed solution.
This is sometimes known as “back of the napkin” or “back of the
envelope” calculation. The estimation process can be formalised as (1)
determine the major parameters that affect the problem, (2) derive an
equation that relates the parameters to the problem, then (3) select
values for the parameters and apply the equation to yield an estimated
solution.</p>
</dd>
<dt><dfn id="evaluation">evaluation</dfn></dt>
<dd>
<p>The act of finding the value for a polynomial at a given point.</p>
</dd>
<dt><dfn id="exact-match-query">exact-match query</dfn></dt>
<dd>
<p>Records are accessed by unique identifier.</p>
</dd>
<dt><dfn id="exceptions">exceptions</dfn></dt>
<dd>
<p>Exceptions are techniques used to predict possible runtime errors and
handle them properly.</p>
</dd>
<dt><dfn id="exchange-sort">exchange sort</dfn></dt>
<dd>
<p>A sort that relies solely on exchanges (swaps of adjacent records) to
reorder the list. <a href="section-14.html#insertion-sort" class="term"
title="A sorting algorithm with $O(n^2)$ average and worst case cost, and $O(n)$ best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted.">Insertion
sort</a> and <a href="section-14.html#bubble-sort" class="term"
title="A simple sort that requires $O(n^2)$ time in best, average, and worst cases. Even an optimised version will normally run slower than Insertion sort, so it has little to recommend it.">Bubble
sort</a> are examples of exchange sorts. All exchange sorts require
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
time in the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>.</p>
</dd>
<dt><dfn id="exchange">exchange</dfn></dt>
<dd>
<p>A swap of adjacent records in an <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>.</p>
</dd>
<dt><dfn id="expanding-the-recurrence">expanding the
recurrence</dfn></dt>
<dd>
<p>A technique for solving a <a
href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>. The idea is to replace the recursive part of the
recurrence with a copy of recurrence.</p>
</dd>
<dt><dfn id="exponential-growth-rate">exponential growth rate</dfn></dt>
<dd>
<p>A <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> function where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
(the input size) appears in the exponent. For example,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mi>n</mi></msup><annotation encoding="application/x-tex">2^n</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="expression-tree">expression tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
structure meant to represent a mathematical expression. <a
href="section-14.html#internal-node" class="term"
title="In a tree, any node that has at least one non-empty child is an internal node.">Internal
nodes</a> of the expression tree are operators in the expression, with
the subtrees being the sub-expressions that are its operand. All <a
href="section-14.html#leaf-node" class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaf
nodes</a> are operands.</p>
</dd>
<dt><dfn id="extent">extent</dfn></dt>
<dd>
<p>A physically contiguous block of <a href="section-14.html#sector"
class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sectors</a>
on a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> that are all part of a given disk file. The fewer extents
needed to store the data for a disk file, generally the fewer <a
href="section-14.html#seek" class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operations that will be required to process a series of <a
href="section-14.html#disk-access" class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
access</a> operations on that file.</p>
</dd>
<dt><dfn id="external-fragmentation">external fragmentation</dfn></dt>
<dd>
<p>A condition that arises when a series of <a
href="section-14.html#memory-request" class="term"
title="In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there.">memory
requests</a> result in lots of small <a
href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free blocks</a>, no
one of which is useful for servicing typical requests.</p>
</dd>
<dt><dfn id="external-sort">external sort</dfn></dt>
<dd>
<p>A sorting algorithm that is applied to data stored in <a
href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> such as on a <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. This is in contrast to an <a
href="section-14.html#internal-sort" class="term"
title="A sorting algorithm that is applied to data stored in main memory. This is in contrast to an external sort that is meant to work on data stored in peripheral storage such as on a disk drive.">internal
sort</a> that works on data stored in <a
href="section-14.html#main-memory" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">main
memory</a>.</p>
</dd>
<dt><dfn id="factorial">factorial</dfn></dt>
<dd>
<p>The factorial function is defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>n</mi><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) = n f(n-1)</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n &gt; 0</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="failure-policy">failure policy</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, a failure policy is the response that takes place when
there is no way to satisfy a <a href="section-14.html#memory-request"
class="term"
title="In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there.">memory
request</a> from the current <a href="section-14.html#free-block"
class="term" title="A block of unused space in a memory pool.">free
blocks</a> in the <a href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. Possibilities include rejecting the request, expanding the
memory pool, collecting <a href="section-14.html#garbage" class="term"
title="In memory management, any memory that was previously (dynamically) allocated by the program during runtime, but which is no longer accessible since all pointers to the memory have been deleted or overwritten. In some languages, garbage can be recovered by garbage collection. In languages such as C and C++ that do not support garbage collection, so creating garbage is considered a memory leak.">garbage</a>,
and reorganising the memory pool (to collect together free space).</p>
</dd>
<dt><dfn id="family-of-languages">family of languages</dfn></dt>
<dd>
<p>Given some class or type of <a href="section-14.html#finite-automata"
class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a> (for example, the <a
href="section-14.html#deterministic-finite-automata" class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a>), the set of languages accepted by that class of
finite automata is called a family. For example, the <a
href="section-14.html#regular-language" class="term"
title="A language $L$ is a regular language if and only if there exists a deterministic finite automata $M$ such that $L = L(M)$.">regular
languages</a> is a family defined by the DFAs.</p>
</dd>
<dt><dfn id="fifo">FIFO</dfn></dt>
<dd>
<p>Abbreviation for “first-in, first-out”. This is the access paradigm
for a <a href="section-14.html#queue" class="term"
title="A list-like structure in which elements are inserted only at one end, and removed only from the other one end.">queue</a>,
and an old terminology for the queue is “FIFO list”.</p>
</dd>
<dt><dfn id="file-allocation-table">file allocation table</dfn></dt>
<dd>
<p>A legacy file system architecture orginially developed for DOS and
then used in Windows. It is still in use in many small-scale peripheral
devices such as USB memory sticks and digital camera memory.</p>
</dd>
<dt><dfn id="file-manager">file manager</dfn></dt>
<dd>
<p>A part of the <a href="section-14.html#operating-system" class="term"
title="The control program for a computer. Its purpose is to control hardware, manage resources, and present a standard interface to these to other software components.">operating
system</a> responsible for taking requests for data from a <a
href="section-14.html#logical-file" class="term"
title="In file processing, the programmer&#39;s view of a random access file stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. This is in contrast to the physical file.">logical
file</a> and mapping those requests to the physical location of the data
on disk.</p>
</dd>
<dt><dfn id="file-processing">file processing</dfn></dt>
<dd>
<p>The domain with computer science that deals with processing data
stored on a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> (in a file), or more broadly, dealing with data stored on any
<a href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> device. Two fundamental properties make dealing with data on
a peripheral device different from dealing with data in main memory: (1)
Reading/writing data on a peripheral storage device is far slower than
reading/writing data to main memory (for example, a typical disk drive
is about a million times slower than <a href="section-14.html#ram"
class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>).
(2) All I/O to a peripheral device is typically in terms of a <a
href="section-14.html#block" class="term"
title="A unit of storage, usually referring to storage on a disk drive or other peripheral storage device. A block is the basic unit of I/O for that device.">block</a>
of data (for example, nearly all disk drives do all I/O in terms of
blocks of 512 bytes).</p>
</dd>
<dt><dfn id="file-structure">file structure</dfn></dt>
<dd>
<p>The organisation of data on <a
href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a>, such as a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> or DVD drive.</p>
</dd>
<dt><dfn id="final-state">final state</dfn></dt>
<dd>
<p>A required element of any <a
href="section-14.html#finite-state-acceptor" class="term"
title="A simple type of finite state automata, an acceptor&#39;s only ability is to accept or reject a string. So, a finite state acceptor does not have the ability to modify the input tape. If computation on the string ends in a final state, then the the string is accepted, otherwise it is rejected.">acceptor</a>.
When computation on a string ends in a final state, then the machine
accepts the string. Otherwise the machine rejects the string.</p>
</dd>
<dt><dfn id="find">FIND</dfn></dt>
<dd>
<p>One half of the <a href="section-14.html#union-find" class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
algorithm for managing <a href="section-14.html#disjoint-sets"
class="term"
title="A collection of sets, any pair of which share no elements in common. A collection of disjoint sets partitions some objects such that every object is in exactly one of the disjoint sets.">disjoint
sets</a>. It is the process of moving upwards in a tree to find the
tree’s root.</p>
</dd>
<dt><dfn>finite automata</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn id="finite-state-acceptor">finite state acceptor</dfn></dt>
<dd>
<p>A simple type of <a href="section-14.html#finite-state-automata"
class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state automata</a>, an acceptor’s only ability is to accept or reject a
string. So, a finite state acceptor does not have the ability to modify
the input tape. If computation on the string ends in a <a
href="section-14.html#final-state" class="term"
title="A required element of any acceptor. When computation on a string ends in a final state, then the machine accepts the string. Otherwise the machine rejects the string.">final
state</a>, then the the string is accepted, otherwise it is
rejected.</p>
</dd>
<dt><dfn>finite state automata</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn id="finite-state-machine">finite state machine</dfn> (<dfn
id="fsm">FSM</dfn>, <dfn id="finite-state-automata">finite state
automata</dfn>, <dfn id="fsa">FSA</dfn>, <dfn
id="finite-automata">finite automata</dfn>, <dfn
id="state-machine">state machine</dfn>, <dfn
id="automata">automata</dfn>)</dt>
<dd>
<p>Any abstract state machine, generally represented as a graph where
the nodes are the <a href="section-14.html#state" class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">states</a>,
and the edges represent transitions between nodes that take place when
the machine is in that node (state) and sees an appropriate input. See,
as an example, <a href="section-14.html#deterministic-finite-automata"
class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a>.</p>
</dd>
<dt><dfn id="first-fit">first fit</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, first fit is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
for deciding which <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> to use
when allocating memory from a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. First fit will always allocate the first <a
href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> on the
<a href="section-14.html#free-block-list" class="term"
title="In a memory manager, the list that stores the necessary information about the current free blocks. Generally, this is done with some sort of linked list, where each node of the linked list indicates the start position and length of the free block in the memory pool.">free
block list</a> that is large enough to service the memory request. The
advantage of this approach is that it is typically not necessary to look
at all free blocks on the free block list to find a suitable free block.
The disadvantage is that it is not “intelligently” selecting what might
be a better choice of free block.</p>
</dd>
<dt><dfn id="fixed-length-coding">fixed-length coding</dfn></dt>
<dd>
<p>Given a collection of objects, a fixed-length coding scheme assigns a
code to each object in the collection using codes that are all of the
same length. Standard ASCII and Unicode representations for characters
are both examples of fixed-length coding schemes. This is in contrast to
<a href="section-14.html#variable-length-coding" class="term"
title="Given a collection of objects, a variable-length coding scheme assigns a code to each object in the collection using codes that can be of different lengths. Typically this is done in a way such that the objects that are most likely to be used have the shortest codes, with the goal of minimising the total space needed to represent a sequence of objects, such as when representing the characters in a document. Huffman coding is an example of a variable-length coding scheme. This is in contrast to fixed-length coding.">variable-length
coding</a>.</p>
</dd>
<dt><dfn id="floor">floor</dfn></dt>
<dd>
<p>Written
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⌊</mo><mi>x</mi><mo stretchy="false" form="postfix">⌋</mo></mrow><annotation encoding="application/x-tex">\lfloor x \rfloor</annotation></semantics></math>,
for real value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
the floor is the greatest integer
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\leq x</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="floyds-algorithm">Floyd’s algorithm</dfn></dt>
<dd>
<p>An algorithm to solve the <a
href="section-14.html#all-pairs-shortest-paths-problem" class="term"
title="Given a graph with weights or distances on the edges, find the shortest paths between every pair of vertices in the graph. One approach to solving this problem is Floyd&#39;s algorithm, which uses the dynamic programming algorithmic technique.">all-pairs
shortest paths problem</a>. It uses the <a
href="section-14.html#dynamic-programming" class="term"
title="An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem.">dynamic
programming</a> algorithmic technique, and runs in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math>
time. As with any <a href="section-14.html#dynamic-programming"
class="term"
title="An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem.">dynamic
programming</a> algorithm, the key issue is to avoid duplicating work by
using proper bookkeeping on the algorithm’s progress through the
solution space. The basic idea is to first find all the direct edge
costs, then improving those costs by allowing paths through <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> 0, then the
cheapest paths involving paths going through vertices 0 and 1, and so
on.</p>
</dd>
<dt><dfn id="flush">flush</dfn></dt>
<dd>
<p>The act of removing data from a <a href="section-14.html#caching"
class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">cache</a>,
most typically because other data considered of higher future value must
replace it in the cache. If the data being flushed has been modified
since it was first read in from <a
href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a> (and the changes are meant to be saved), then it must be
written back to that secondary storage.</p>
</dd>
<dt><dfn id="flyweight">flyweight</dfn></dt>
<dd>
<p>A <a href="section-14.html#design-pattern" class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a> that is meant to solve the following problem: You have an
application with many objects. Some of these objects are identical in
the information that they contain, and the role that they play. But they
must be reached from various places, and conceptually they really are
distinct objects. Because there is so much duplication of the same
information, we want to reduce memory cost by sharing that space. For
example, in document layout, the letter “C” might be represented by an
object that describes that character’s strokes and bounding box.
However, we do not want to create a separate “C” object everywhere in
the document that a “C” appears. The solution is to allocate a single
copy of the shared representation for “C” objects. Then, every place in
the document that needs a “C” in a given font, size, and typeface will
reference this single copy. The various instances of <a
href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">references</a>
to a specific form of “C” are called flyweights. Flyweights can also be
used to implement the empty leaf nodes of the <a
href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>
and <a href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a>.</p>
</dd>
<dt><dfn id="folding-method">folding method</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
an approach to implementing a <a href="section-14.html#hash-function"
class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>. Most typically used when the key is a string, the folding
method breaks the string into pieces (perhaps each letter is a piece, or
a small series of letters is a piece), converts the letter(s) to an
integer value (typically by using its underlying encoding value), and
summing up the pieces.</p>
</dd>
<dt><dfn id="ford-and-johnson-sort">Ford and Johnson sort</dfn></dt>
<dd>
<p>A sorting algorithm that is close to the theoretical minimum number
of key comparisons necessary to sort. Generally not considered practical
in practice due to the fact that it is not efficient in terms of the
number of records that need to be moved. It consists of first sorting
pairs of nodes into winners and losers (of the pairs comparisons), then
(recursively) sorting the winners of the pairs, and then finally
carefully selecting the order in which the losers are added to the chain
of sorted items.</p>
</dd>
<dt><dfn id="forest">forest</dfn></dt>
<dd>
<p>A collection of one or more <a href="section-14.html#tree"
class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">trees</a>.</p>
</dd>
<dt><dfn id="free-block-list">free block list</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, the list that stores the necessary information about the
current <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free blocks</a>.
Generally, this is done with some sort of <a
href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a>, where each node of the linked list indicates the start
position and length of the free block in the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>.</p>
</dd>
<dt><dfn id="free-block">free block</dfn></dt>
<dd>
<p>A block of unused space in a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>.</p>
</dd>
<dt><dfn id="free-store">free store</dfn></dt>
<dd>
<p>Space available to a program during runtime to be used for <a
href="section-14.html#dynamic-allocation" class="term"
title="The act of creating an object from free store. In C++, Java, and Javascript, this is done using the `new` operator.">dynamic
allocation</a> of objects. The free store is distinct from the <a
href="section-14.html#runtime-stack" class="term"
title="The place where an activation record is stored when a subroutine is called during a program&#39;s runtime.">runtime
stack</a>. The free store is sometimes referred to as the <a
href="section-14.html#heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>,
which can be confusing because <a href="section-14.html#heap"
class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
more often refers to a specific data structure. Most programming
languages provide functions to allocate (and maybe to deallocate)
objects from the free store, such as <code>new</code> in C++ and
Java.</p>
</dd>
<dt><dfn id="free-tree">free tree</dfn></dt>
<dd>
<p>A connected, <a href="section-14.html#undirected-graph" class="term"
title="A graph whose edges do not have a direction.">undirected
graph</a> with no simple cycles. An equivalent definition is that a free
tree is connected and has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mi>𝐕</mi><mo stretchy="false" form="prefix">|</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">|\mathbf{V}| - 1</annotation></semantics></math>
edges.</p>
</dd>
<dt><dfn id="freelist">freelist</dfn></dt>
<dd>
<p>A simple and faster alternative to using <a
href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a> when the objects being dynamically allocated are all of the
same size (and thus are interchangeable). Typically implemented as a <a
href="section-14.html#linked-stack" class="term"
title="Analogous to a linked list, this uses dynamic allocation of nodes to store the elements when implementing the stack ADT.">linked
stack</a>, released objects are put on the front of the freelist. When a
request is made to allocate an object, the freelist is checked first and
it provides the object if possible. If the freelist is empty, then a new
object is allocated from <a href="section-14.html#free-store"
class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>.</p>
</dd>
<dt><dfn id="frequency-count">frequency count</dfn></dt>
<dd>
<p>A <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
used to maintain a <a href="section-14.html#self-organising-list"
class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>. Under this heuristic, a count is maintained for every record.
When a record access is made, its count is increased. If this makes its
count greater than that of another record in the list, it moves up
toward the front of the list accordingly so as to keep the list sorted
by frequency. Analogous to the <a
href="section-14.html#least-frequently-used" class="term"
title="Abbreviated LFU, it is a heuristic that can be used to decide which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. However, least recently used is more popular than LFU. Analogous to the frequency count heuristic for maintaining a self-organising list.">least
frequently used</a> heuristic for maintaining a <a
href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>.</p>
</dd>
<dt><dfn>FSA</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn>FSM</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn id="full-binary-tree-theorem">full binary tree
theorem</dfn></dt>
<dd>
<p>This theorem states that the number of leaves in a non-empty full
binary tree is one more than the number of internal nodes. Equivalently,
then number of null pointers in a standard <a
href="section-14.html#pointer-based-implementation-for-binary-tree-nodes"
class="term"
title="A common way to implement binary tree nodes. Each node stores a data value (or a reference to a data value), and pointers to the left and right children. If either or both of the children does not exist, then a null pointer is stored.">pointer-based
implementation for binary tree nodes</a> is one more than the number of
nodes in the binary tree.</p>
</dd>
<dt><dfn id="full-tree">full tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a> is full if every <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
is either a <a href="section-14.html#leaf-node" class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaf
node</a> or else it is an <a href="section-14.html#internal-node"
class="term"
title="In a tree, any node that has at least one non-empty child is an internal node.">internal
node</a> with two non-empty <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.</p>
</dd>
<dt><dfn id="function">function</dfn></dt>
<dd>
<p>In mathematics, a matching between inputs (the <a
href="section-14.html#domain" class="term"
title="The set of possible inputs to a function.">domain</a>) and
outputs (the <a href="section-14.html#range" class="term"
title="The set of possible outputs for a function.">range</a>). In
programming, a subroutine that takes input parameters and uses them to
compute and return a value. In this case, it is usually considered bad
practice for a function to change any global variables (doing so is
called a side effect).</p>
</dd>
<dt><dfn id="garbage-collection">garbage collection</dfn></dt>
<dd>
<p>Languages with garbage collection such Java, Javascript, Lisp, and
Scheme will periodically reclaim <a href="section-14.html#garbage"
class="term"
title="In memory management, any memory that was previously (dynamically) allocated by the program during runtime, but which is no longer accessible since all pointers to the memory have been deleted or overwritten. In some languages, garbage can be recovered by garbage collection. In languages such as C and C++ that do not support garbage collection, so creating garbage is considered a memory leak.">garbage</a>
and return it to <a href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>.</p>
</dd>
<dt><dfn id="garbage">garbage</dfn></dt>
<dd>
<p>In <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
management</a>, any memory that was previously (dynamically) allocated
by the program during runtime, but which is no longer accessible since
all pointers to the memory have been deleted or overwritten. In some
languages, garbage can be recovered by <a
href="section-14.html#garbage-collection" class="term"
title="Languages with garbage collection such Java, Javascript, Lisp, and Scheme will periodically reclaim garbage and return it to free store.">garbage
collection</a>. In languages such as C and C++ that do not support
garbage collection, so creating garbage is considered a <a
href="section-14.html#memory-leak" class="term"
title="In programming, the act of creating garbage. In languages such as C and C++ that do not support garbage collection, repeated memory leaks will evenually cause the program to terminate.">memory
leak</a>.</p>
</dd>
<dt><dfn id="general-tree">general tree</dfn></dt>
<dd>
<p>A tree in which any given node can have any number of <a
href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.
This is in contrast to, for example, a <a
href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a> where each node has a fixed number of children (some of which
might be <code>null</code>). General tree nodes tend to be harder to
implement for this reason.</p>
</dd>
<dt><dfn id="grammar">grammar</dfn></dt>
<dd>
<p>A formal definition for what strings make up a <a
href="section-14.html#language" class="term"
title="A set of strings.">language</a>, in terms of a set of <a
href="section-14.html#production-rule" class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production
rules</a>.</p>
</dd>
<dt><dfn id="graph">graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐆</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mi>𝐕</mi><mo>,</mo><mi>𝐄</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{G} = (\mathbf{V}, \mathbf{E})</annotation></semantics></math>
consists of a set of <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>
and a set of <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐄</mi><annotation encoding="application/x-tex">\mathbf{E}</annotation></semantics></math>,
such that each edge in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐄</mi><annotation encoding="application/x-tex">\mathbf{E}</annotation></semantics></math>
is a connection between a pair of vertices in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐕</mi><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="greedy-algorithm">greedy algorithm</dfn></dt>
<dd>
<p>An algorithm that makes locally optimal choices at each step.</p>
</dd>
<dt><dfn id="growth-rate">growth rate</dfn> (<dfn
id="order-of-growth">order of growth</dfn>)</dt>
<dd>
<p>The rate at which a function grows. How quickly the function grows
when its input grows. Also called its <em>order of growth</em>.</p>
<p>A function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
has growth rate bounded by a function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
if the values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
are eventually bounded by those of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
up to some constant factor. We often shorten this (somewhat confusingly)
by saying that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
has growth rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
or that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
has order of growth
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>.</p>
<p>Formally, there are constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mn>0</mn></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n_0 \geq 0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c &gt; 0</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>≤</mo><mi>c</mi><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \leq c g(n)</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≥</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">n \geq n_0</annotation></semantics></math>.
We then say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
has growth rate less or equal that of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
and write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f \in O(g)</annotation></semantics></math>
(big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>
notation). This defines the preorder of growth rates.</p>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, we sometimes speak of the growth rate of an <a
href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>.
By that, we mean the <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a> of the <a href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of the algorithm, the rate at which the cost of the <a
href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
grows as the size of its input grows. This is also called the <a
href="section-14.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortised complexity.">asymptotic
complexity</a> of that algorithm.</p>
</dd>
<dt><dfn id="guess-and-test">guess-and-test</dfn></dt>
<dd>
<p>A technique used when trying to determine the <a
href="section-14.html#closed-form-solution" class="term"
title="An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence.">closed-form
solution</a> for a <a href="section-14.html#summation" class="term"
title="The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to $n$ can be written as $\sum_{i=1}^{n} i$.">summation</a>
or <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>. Given a hypothesis for the closed-form solution, if it is
correct, then it is often relatively easy to prove that using <a
href="section-14.html#proof-by-induction" class="term"
title="A mathematical proof technique similar to recursion. It is used to prove a parameterised theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction.">induction</a>.</p>
</dd>
<dt><dfn id="guided-traversal">guided traversal</dfn></dt>
<dd>
<p>A <a href="section-14.html#tree-traversal" class="term"
title="A traversal performed on a tree. Traditional tree traversals include preorder and postorder traversals for both binary and general trees, and inorder traversal that is most appropriate for a BST.">tree
traversal</a> that does not need to visit every node in the tree. An
example would be a <a href="section-14.html#range-query" class="term"
title="Records are returned if their relevant key value falls within a specified range.">range
query</a> in a <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>.</p>
</dd>
<dt><dfn id="halt-state">halt state</dfn></dt>
<dd>
<p>In a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a>, a designated <a href="section-14.html#state" class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">state</a>
which causes the machine to immediately halt when it is entered.</p>
</dd>
<dt><dfn id="halted-configuration">halted configuration</dfn></dt>
<dd>
<p>A halted configuration occurs in a <a
href="section-14.html#turing-machine" class="term"
title="A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer.">Turing
machine</a> when the machine transitions into the <a
href="section-14.html#halt-state" class="term"
title="In a finite automata, a designated state which causes the machine to immediately halt when it is entered.">halt
state</a>.</p>
</dd>
<dt><dfn id="halting-problem">halting problem</dfn></dt>
<dd>
<p>The halting problem is to answer this question: Given a computer
program
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and an input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math>,
will program
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
halt when executed on input
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math>?
This problem has been proved impossible to solve in the general case.
Thus, it is an example of an <a
href="section-14.html#unsolvable-problem" class="term"
title="A problem that can proved impossible to solve on a computer. The classic example is the halting problem.">unsolvable
problem</a>.</p>
</dd>
<dt><dfn id="handle">handle</dfn></dt>
<dd>
<p>When using a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a> to store data, the <a href="section-14.html#client"
class="term"
title="The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool.">client</a>
will pass data to be stored (the <a href="section-14.html#message"
class="term"
title="In a memory manager implementation (particularly a memory manager implemented with a message passing style of interface), the message is the data that the client of the memory manager wishes to have stored in the memory pool. The memory manager will reply to the client by returning a handle that defines the location and size of the message as stored in the memory pool. The client can later recover the message by passing the handle back to the memory manager.">message</a>)
to the memory manager, and the memory manager will return to the client
a handle. The handle encodes the necessary information that the memory
manager can later use to recover and return the message to the client.
This is typically the location and length of the message within the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>.</p>
</dd>
<dt><dfn id="hanging-configuration">hanging configuration</dfn></dt>
<dd>
<p>A hanging configuration occurs in a <a
href="section-14.html#turing-machine" class="term"
title="A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer.">Turing
machine</a> when the I/O head moves to the left from the left-most
square of the tape, or when the machine goes into an infinite loop.</p>
</dd>
<dt><dfn id="hard-algorithm">hard algorithm</dfn></dt>
<dd>
<p>“Hard” is traditionally defined in relation to running time, and a
“hard” algorithm is defined to be an algorithm with exponential running
time.</p>
</dd>
<dt><dfn id="hard-problem">hard problem</dfn></dt>
<dd>
<p>“Hard” is traditionally defined in relation to running time, and a
“hard” problem is defined to be one whose best known algorithm requires
exponential running time.</p>
</dd>
<dt><dfn id="harmonic-series">harmonic series</dfn></dt>
<dd>
<p>The sum of reciprocals from 1 to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is called the harmonic series, and is written
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ℋ</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\mathcal{H}_n</annotation></semantics></math>.
This sum has a value between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>e</mi></msub><mi>n</mi></mrow><annotation encoding="application/x-tex">\log_e n</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>e</mi></msub><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\log_e n + 1</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="hash-function">hash function</dfn></dt>
<dd>
<p>In a <a href="section-14.html#hash-system" class="term"
title="The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key.">hash
system</a>, the function that converts a <a href="section-14.html#key"
class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value to a position in the <a href="section-14.html#hash-table"
class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. The hope is that this position in the hash table contains the
record that matches the key value.</p>
</dd>
<dt><dfn id="hash-system">hash system</dfn></dt>
<dd>
<p>The implementation for search based on hash lookup in a <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. The <a href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> is processed by a <a href="section-14.html#hash-function"
class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>, which returns a position in a <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>, which hopefully is the correct position in which to find the
record corresponding to the search key.</p>
</dd>
<dt><dfn id="hash-table">hash table</dfn></dt>
<dd>
<p>The data structure (usually an <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>)
that stores data records for lookup using <a
href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>.</p>
</dd>
<dt><dfn id="hashing">hashing</dfn></dt>
<dd>
<p>A search method that uses a <a href="section-14.html#hash-function"
class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a> to convert a <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> value into a position within a <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. In a properly implemented <a
href="section-14.html#hash-system" class="term"
title="The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key.">hash
system</a>, that position in the table will have high probability of
containing the record that matches the key value. Sometimes, the hash
function will return a position that does not store the desired key, due
to a process called <a href="section-14.html#collision" class="term"
title="In a hash system, this refers to the case where two search keys are mapped by the hash function to the same slot in the hash table. This can happen on insertion or search when another record has already been hashed to that slot. In this case, a closed hash system will require a process known as collision resolution to find the location of the desired record.">collision</a>.
In that case, the desired record is found through a process known as <a
href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a>.</p>
</dd>
<dt><dfn id="head">head</dfn></dt>
<dd>
<p>The beginning of a <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>.</p>
</dd>
<dt><dfn id="header-node">header node</dfn></dt>
<dd>
<p>Commonly used in implementations for a <a
href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> or related structure, this <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
preceeds the first element of the list. Its purpose is to simplify the
code implementation by reducing the number of special cases that must be
programmed for.</p>
</dd>
<dt><dfn>heap</dfn></dt>
<dd>
<p>See <a href="section-14.html#binary-heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">binary
heap</a></p>
</dd>
<dt><dfn id="heapsort">heapsort</dfn></dt>
<dd>
<p>A sorting algorithm that costs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>
time in the <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best</a>,
<a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>,
and <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst</a>
cases. It tends to be slower than <a href="section-14.html#mergesort"
class="term"
title="A sorting algorithm that requires $O(n \log n)$ in the best, average, and worst cases. Conceptually it is simple: Split the list in half, sort the halves, then merge them together. It is a bit complicated to implement efficiently on an array.">Mergesort</a>
and <a href="section-14.html#quicksort" class="term"
title="A sort that is $O(n \log n)$ in the best and average cases, though $O(n^2)$ in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts.">Quicksort</a>.
It works by building a <a href="section-14.html#max-heap" class="term"
title="A heap where every node has a key value greater than its children. As a consequence, the node with maximum key value is at the root.">max
heap</a>, and then repeatedly removing the item with maximum <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value (moving it to the end of the heap) until all elements have been
removed (and replaced at their proper location in the array).</p>
</dd>
<dt><dfn id="height-balanced">height balanced</dfn></dt>
<dd>
<p>The condition the <a href="section-14.html#depth" class="term"
title="The depth of a node $M$ in a tree is the length of the path from the root of the tree to $M$.">depths</a>
of each <a href="section-14.html#subtree" class="term"
title="A subtree is a subset of the nodes of a binary tree that includes some node $R$ of the tree as the subtree root along with all the descendants of $R$.">subtree</a>
in a tree are roughly the same.</p>
</dd>
<dt><dfn id="height">height</dfn></dt>
<dd>
<p>The height of a tree is one more than the <a
href="section-14.html#depth" class="term"
title="The depth of a node $M$ in a tree is the length of the path from the root of the tree to $M$.">depth</a>
of the deepest <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
in the tree.</p>
</dd>
<dt><dfn id="heuristic-algorithm">heuristic algorithm</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#approximation-algorithm"
class="term"
title="An algorthm for an optimisation problem that finds a good, but not necessarily cheapest, solution.">approximation
algorithm</a>, that uses a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
to find a good, but not necessarily cheapest, solution to an <a
href="section-14.html#optimisation-problem" class="term"
title="Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the *traveling salesman problem*, where visiting $n$ cities in some order has a cost, and the goal is to visit in the cheapest order.">optimisation
problem</a>.</p>
</dd>
<dt><dfn id="heuristic">heuristic</dfn></dt>
<dd>
<p>A way to solve a problem that is not guarenteed to be optimal. While
it might not be guarenteed to be optimal, it is generally expected (by
the agent employing the heuristic) to provide a reasonably efficient
solution.</p>
</dd>
<dt><dfn id="home-position">home position</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
a synonym for <a href="section-14.html#home-slot" class="term"
title="In hashing, this is the slot in the hash table determined for a given key by the hash function.">home
slot</a>.</p>
</dd>
<dt><dfn id="home-slot">home slot</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
this is the <a href="section-14.html#slot" class="term"
title="In hashing, a position in a hash table.">slot</a> in the <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> determined for a given key by the <a
href="section-14.html#hash-function" class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>.</p>
</dd>
<dt><dfn id="homogeneity">homogeneity</dfn></dt>
<dd>
<p>In a <a href="section-14.html#container" class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a>
class, this is the property that all objects stored in the ncontainer
are of the same class. For example, if you have a list intended to store
payroll records, is it possible for the programmer to insert an integer
onto the list instead?</p>
</dd>
<dt><dfn id="huffman-codes">Huffman codes</dfn></dt>
<dd>
<p>The codes given to a collection of letters (or other symbols) through
the process of Huffman coding. Huffman coding uses a <a
href="section-14.html#huffman-coding-tree" class="term"
title="A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie.">Huffman
coding tree</a> to generate the codes. The codes can be of variable
length, such that the letters which are expected to appear most
frequently are shorter. Huffman coding is optimal whenever the true
frequencies are known, and the frequency of a letter is independent of
the context of that letter in the message.</p>
</dd>
<dt><dfn id="huffman-coding-tree">Huffman coding tree</dfn></dt>
<dd>
<p>A Huffman coding tree is a <a href="section-14.html#full-tree"
class="term"
title="A binary tree is full if every node is either a leaf node or else it is an internal node with two non-empty children.">full
binary tree</a> that is used to represent letters (or other symbols)
efficiently. Each letter is associated with a node in the tree, and is
then given a <a href="section-14.html#huffman-codes" class="term"
title="The codes given to a collection of letters (or other symbols) through the process of Huffman coding. Huffman coding uses a Huffman coding tree to generate the codes. The codes can be of variable length, such that the letters which are expected to appear most frequently are shorter. Huffman coding is optimal whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message.">Huffman
code</a> based on the position of the associated node. A Huffman coding
tree is an example of a binary <a href="section-14.html#trie"
class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>.</p>
</dd>
<dt><dfn id="huffman-tree">Huffman tree</dfn></dt>
<dd>
<p>Shorter form of the term <a
href="section-14.html#huffman-coding-tree" class="term"
title="A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie.">Huffman
coding tree</a>.</p>
</dd>
<dt><dfn id="i-o-head">I/O head</dfn> (<dfn
id="read-write-head">read/write head</dfn>)</dt>
<dd>
<p>On a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> (or similar device), the part of the machinery that actually
reads data from the disk.</p>
</dd>
<dt><dfn id="image-space-decomposition">image-space
decomposition</dfn></dt>
<dd>
<p>A from of <a href="section-14.html#key-space-decomposition"
class="term"
title="The idea that the range for a search key will be split into pieces. There are two general approaches to this: object-space decomposition and image-space decomposition.">key-space
decomposition</a> where the <a href="section-14.html#key-space"
class="term"
title="The range of values that a key value may take on.">key space</a>
splitting points is predetermined (typically by splitting in half). For
example, a <a href="section-14.html#huffman-coding-tree" class="term"
title="A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie.">Huffman
coding tree</a> splits the letters being coded into those with codes
that start with 0 on the left side, and those with codes that start with
1 on the right side. This regular decomposition of the key space is the
basis for a <a href="section-14.html#trie" class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>
data structure. An image-space decomposition is in opposition to an <a
href="section-14.html#object-space-decomposition" class="term"
title="A from of key-space decomposition where the key space is determined by the actual values of keys that are found. For example, a BST stores a key value in its root, and all other values in the tree with lesser value are in the left subtree. Thus, the root value has split (or decomposed) the key space for that key based on its value into left and right parts. An object-space decomposition is in opposition to an image-space decomposition.">object-space
decomposition</a>.</p>
</dd>
<dt><dfn id="in-degree">in degree</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, the in degree for a <a href="section-14.html#vertex"
class="term" title="Another name for a node in a graph.">vertex</a> is
the number of edges directed into the vertex.</p>
</dd>
<dt><dfn id="incident">incident</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, an edge connecting two vertices is said to be incident with
those vertices. The two vertices are said to be <a
href="section-14.html#adjacent" class="term"
title="Two nodes of a tree or two vertices of a graph are said to be adjacent if they have an edge connecting them. If the edge is directed from $a$ to $b$, then we say that $a$ is adjacent to $b$, and $b$ is adjacent from $a$.">adjacent</a>.</p>
</dd>
<dt><dfn id="index-file">index file</dfn></dt>
<dd>
<p>A file whose records consist of <a
href="section-14.html#key-value-pair" class="term"
title="A standard solution for solving the problem of how to relate a key value to a record (or how to find the key for a given record) within the context of a particular index. The idea is to simply store as records in the index pairs of keys and records. Specifically, the index will typically store a copy of the key along with a reference to the record. The other standard solution to this problem is to pass a comparator function to the index.">key-value
pairs</a> where the pointers are referencing the complete records stored
in another file.</p>
</dd>
<dt><dfn id="indexed-sequential-access-method">indexed sequential access
method</dfn> (<dfn id="isam">ISAM</dfn>)</dt>
<dd>
<p>An obsolete method for indexing data for (at the time) fast
retrieval. More generally, the term is used also to generically refer to
an <a href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">index</a>
that supports both sequential and <a href="section-14.html#key"
class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">keyed</a>
access to data records. Today, that would nearly always be implemented
using a <a href="section-14.html#b-tree" class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>.</p>
</dd>
<dt><dfn id="indexing">indexing</dfn></dt>
<dd>
<p>The process of associating a <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> with the location of a corresponding data record. The two
defining points to the concept of an index is the association of a key
with a record, and the fact that the index does not actually store the
record itself but rather it stores a <a href="section-14.html#reference"
class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to the record. In this way, a collection of records can be supported by
multiple indices, typically a separate index for each key field in the
record.</p>
</dd>
<dt><dfn id="induction-hypothesis">induction hypothesis</dfn></dt>
<dd>
<p>The key assumption used in a <a
href="section-14.html#proof-by-induction" class="term"
title="A mathematical proof technique similar to recursion. It is used to prove a parameterised theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction.">proof
by induction</a>, that the theorem to be proved holds for smaller
instances of the theorem. The induction hypothesis is equivalent to the
<a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursive</a>
call in a recursive function.</p>
</dd>
<dt><dfn id="induction-step">induction step</dfn></dt>
<dd>
<p>Part of a <a href="section-14.html#proof-by-induction" class="term"
title="A mathematical proof technique similar to recursion. It is used to prove a parameterised theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction.">proof
by induction</a>. In its simplest form, this is a proof of the
implication that if the theorem holds for $n-1$, then it holds for $n$.
As an alternative, see <a href="section-14.html#strong-induction"
class="term"
title="An alternative formulation for the induction step in a proof by induction. The induction step for strong induction is: If **Thrm** holds for all $k, c \leq k &lt; n$, then **Thrm** holds for $n$.">strong
induction</a>.</p>
</dd>
<dt><dfn id="induction-variable">induction variable</dfn></dt>
<dd>
<p>The variable used to parameterise the theorem being proved by
induction. For example, if we seek to prove that the sum of the integers
from 1 to $n$ is $n(n+1)/2$, then $n$ is the induction variable. An
induction variable must be an integer.</p>
</dd>
<dt><dfn id="information-theoretic-lower-bound">information theoretic
lower bound</dfn></dt>
<dd>
<p>A <a href="section-14.html#lower-bound" class="term"
title="An lower bound for a growth rate $f$ is any growth rate $g$ that is less than or equal to it. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \geq c g(n)$ for all $n \geq n_0$. We also write $f \in \Omega(g)$ or slightly imprecisely $f(n) \in \Omega(g(n))$ (this is Omega notation). Usually, we are interested in finding a lower bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, a lower bound for an algorithm is a lower bound for the asymptotic complexity of the algorithm, the growth rate of its complexity.">lower
bound</a> on the amount of resources needed to solve a <a
href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
based on the number of bits of information needed to uniquely specify
the answer. Sometimes referred to as a “Shannon theoretic lower bound”
due to Shannon’s work on information theory and entropy. An example is
that sorting has a lower bound of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>2</mn></msub><mi>n</mi><mi>!</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(\log_2 n!)</annotation></semantics></math>
because there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math>
possible orderings for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
values. This observation alone does not make the lower bound tight,
because it is possible that no algorithm could actually reach the
information theory lower limit.</p>
</dd>
<dt><dfn id="inherit">inherit</dfn></dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, the process by which a <a
href="section-14.html#subclass" class="term"
title="In object-oriented programming, any class within a class hierarchy that inherits from some other class.">subclass</a>
gains <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
members</a> and <a href="section-14.html#method" class="term"
title="In the object-oriented programming paradigm, a method is an operation on a class. A synonym for member function.">methods</a>
from a <a href="section-14.html#base-class" class="term"
title="In object-oriented programming, a class from which another class inherits. The class that inherits is called a subclass.">base
class</a>.</p>
</dd>
<dt><dfn id="initial-state">initial state</dfn></dt>
<dd>
<p>A synonym for <a href="section-14.html#start-state" class="term"
title="In a finite automata, the designated state in which the machine will always begin a computation.">start
state</a>.</p>
</dd>
<dt><dfn id="inode">inode</dfn></dt>
<dd>
<p>Short for “index node”. In UNIX-style file systems, specific disk <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sectors</a>
that hold indexing information to define the layout of the file
system.</p>
</dd>
<dt><dfn id="inorder-traversal">inorder traversal</dfn></dt>
<dd>
<p>In a <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a>, a <a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
that first <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursively</a>
<a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visits</a>
the left <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">child</a>,
then visits the <a href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>,
and then recursively visits the right child. In a <a
href="section-14.html#binary-search-tree" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">binary
search tree</a>, this traversal will <a
href="section-14.html#enumeration" class="term"
title="The process by which a traversal lists every object in the container exactly once. Thus, a traversal that prints the nodes is said to enumerate the nodes. An enumeration can also refer to the actual listing that is produced by the traversal (as well as the process that created that listing).">enumerate</a>
the nodes in sorted order.</p>
</dd>
<dt><dfn id="insertion-sort">Insertion sort</dfn></dt>
<dd>
<p>A sorting algorithm with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
<a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>
and <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a> cost, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
<a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best
case</a> cost. This best case cost makes it useful when we have reason
to expect the input to be nearly sorted.</p>
</dd>
<dt><dfn>instance variable</dfn></dt>
<dd>
<p>See <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
member</a></p>
</dd>
<dt><dfn id="integer-function">integer function</dfn></dt>
<dd>
<p>Any function whose input is an integer and whose output is an
integer. It can be proved by <a
href="section-14.html#diagonalisation-argument" class="term"
title="A proof technique for proving that a set is uncountably infinite. The approach is to show that, no matter what order the elements of the set are put in, a new element of the set can be constructed that is not in that ordering. This is done by changing the $i$ th value or position of the element to be different from that of the $i$ th element in the proposed ordering.">diagonalisation</a>
that the set of integer functions is <a
href="section-14.html#uncountably-infinite" class="term"
title="An infinite set is uncountably infinite if there does not exist any mapping from it to the set of integers. This is often proved using a diagonalisation argument. The real numbers is an example of an uncountably infinite set.">uncountably
infinite</a>.</p>
</dd>
<dt><dfn id="inter-sector-gap">inter-sector gap</dfn></dt>
<dd>
<p>On a disk drive, a physical gap in the data that occurs between the
<a href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sectors</a>.
This allows the <a href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a> detect the end of the sector.</p>
</dd>
<dt><dfn id="interface">interface</dfn></dt>
<dd>
<p>An interface is a class-like structure that only contains method
signatures and fields. An interface does not contain an implementation
of the methods or any <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
members</a>.</p>
</dd>
<dt><dfn id="intermediate-code-generation">intermediate code
generation</dfn></dt>
<dd>
<p>A phase in a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>,
that walks through a <a href="section-14.html#parse-tree" class="term"
title="A tree that represents the syntactic structure of an input string, making it easy to compare against a grammar to see if it is syntactically correct.">parse
tree</a> to produce simple <a href="section-14.html#assembly-code"
class="term"
title="A form of intermediate code created by a compiler that is easy to convert into the final form that the computer can execute. An assembly language is typically a direct mapping of one or a few instructions that the CPU can execute into a mnemonic form that is relatively easy for a human to read.">assembly
code</a>.</p>
</dd>
<dt><dfn id="intermediate-code">intermediate code</dfn></dt>
<dd>
<p>A step in a typical <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
is to transform the original high-level language into a form on which it
is easier to do other stages of the process. For example, some compilers
will transform the original high-level source code into <a
href="section-14.html#assembly-code" class="term"
title="A form of intermediate code created by a compiler that is easy to convert into the final form that the computer can execute. An assembly language is typically a direct mapping of one or a few instructions that the CPU can execute into a mnemonic form that is relatively easy for a human to read.">assembly
code</a> on which it can do <a href="section-14.html#code-optimisation"
class="term"
title="A phase in a compiler that makes changes in the code (typically assembly code) with the goal of replacing it with a version of the code that will run faster while performing the same computation.">code
optimisation</a>, before translating it into its final executable
form.</p>
</dd>
<dt><dfn id="internal-fragmentation">internal fragmentation</dfn></dt>
<dd>
<p>A condition that occurs when more than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
bytes are allocated to service a <a
href="section-14.html#memory-request" class="term"
title="In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there.">memory
request</a> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
bytes, wasting free storage. This is often done to simplify <a
href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
management</a>.</p>
</dd>
<dt><dfn id="internal-node">internal node</dfn></dt>
<dd>
<p>In a tree, any node that has at least one non-empty <a
href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">child</a>
is an internal node.</p>
</dd>
<dt><dfn id="internal-sort">internal sort</dfn></dt>
<dd>
<p>A sorting algorithm that is applied to data stored in <a
href="section-14.html#main-memory" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">main
memory</a>. This is in contrast to an <a
href="section-14.html#external-sort" class="term"
title="A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory.">external
sort</a> that is meant to work on data stored in <a
href="section-14.html#peripheral-storage" class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> such as on a <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>.</p>
</dd>
<dt><dfn id="interpolation-search">interpolation search</dfn></dt>
<dd>
<p>Given a sorted array, and knowing the first and last <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values stored in some subarray known to contain <a
href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
interpolation search will compute the expected location of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
in the subarray as a fraction of the distance between the known key
values. So it will next check that computed location, thus narrowing the
search for the next iteration. Given reasonable key value distribution,
the <a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average
case</a> for interpolation search will be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log \log n)</annotation></semantics></math>,
or better than the expected cost of <a
href="section-14.html#binary-search" class="term"
title="A standard recursive algorithm for finding the record with a given search key value within a sorted list. It runs in $O(\log n)$ time. At each step, look at the middle of the current sublist, and throw away the half of the records whose keys are either too small or too large.">binary
search</a>. Nonetheless, binary search is expected to be faster in
nearly all practical situations due to the small difference between the
two costs, combined with the higher constant factors required to
implement interpolation search as compared to binary search.</p>
</dd>
<dt><dfn id="interpolation">interpolation</dfn></dt>
<dd>
<p>The act of finding the coefficients of a polynomial, given the values
at some points. A polynomal of degree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>
requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
points to interpolate the coefficients.</p>
</dd>
<dt><dfn id="interpreter">interpreter</dfn></dt>
<dd>
<p>In contrast to a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that translates a high-level program into something that can be
repeatedly executed to perform a computation, an interpreter directly
performs computation on the high-level langauge. This tends to make the
computation much slower than if it were performed on the directly
executable version produced by a compiler.</p>
</dd>
<dt><dfn id="inversion">inversion</dfn></dt>
<dd>
<p>A measure of how disordered a series of values is. For each element
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
in the series, count one inversion for each element to left of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
that is greater than the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
(and so must ultimately be moved to the right of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
during a sorting process).</p>
</dd>
<dt><dfn id="inverted-file">inverted file</dfn></dt>
<dd>
<p>Synonym for <a href="section-14.html#inverted-list" class="term"
title="An index which links secondary keys to either the associated primary key or the actual record in the database.">inverted
list</a> when the inverted list is stored in a disk file.</p>
</dd>
<dt><dfn id="inverted-list">inverted list</dfn></dt>
<dd>
<p>An <a href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">index</a>
which links <a href="section-14.html#secondary-key" class="term"
title="A key field in a record such as salary, where a particular key value might be duplicated in multiple records. A secondary key is more likely to be used by a user as a search key than is the record&#39;s primary key.">secondary
keys</a> to either the associated <a href="section-14.html#primary-key"
class="term" title="A unique identifier for a record.">primary key</a>
or the actual record in the database.</p>
</dd>
<dt><dfn id="irreflexive">irreflexive</dfn></dt>
<dd>
<p>In set notation, binary relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
on set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is irreflexive if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">aRa</annotation></semantics></math>
is never in the relation for any
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">a \in \mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn>ISAM</dfn></dt>
<dd>
<p>See <a href="section-14.html#indexed-sequential-access-method"
class="term"
title="An obsolete method for indexing data for (at the time) fast retrieval. More generally, the term is used also to generically refer to an index that supports both sequential and keyed access to data records. Today, that would nearly always be implemented using a B-tree.">indexed
sequential access method</a></p>
</dd>
<dt><dfn id="iterator">iterator</dfn></dt>
<dd>
<p>In a <a href="section-14.html#container" class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a>
such as a list, a separate class that indicates position within the
container, with support for <a href="section-14.html#traversal"
class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversing</a>
through all <a href="section-14.html#element" class="term"
title="One value or member in a set.">elements</a> in the container.</p>
</dd>
<dt><dfn id="job">job</dfn></dt>
<dd>
<p>Common name for processes or tasks to be run by an operating system.
They typically need to be processed in order of importance, and so are
kept organised by a <a href="section-14.html#priority-queue"
class="term"
title="An ADT whose primary operations of insert of records, and deletion of the greatest (or, in an alternative implementation, the least) valued record. Most often implemented using the heap data structure. The name comes from a common application where the records being stored represent tasks, with the ordering values based on the priorities of the tasks.">priority
queue</a>. Another common use for this term is for a collection of tasks
to be ordered by a <a href="section-14.html#topological-sort"
class="term"
title="The process of laying out the vertices of a DAG in a linear order such that no vertex $A$ in the order is preceded by a vertex that can be reached by a (directed) path from $A$. Usually the (directed) edges in the graph define a prerequisite system, and the goal of the topological sort is to list the vertices in an order such that no prerequisites are violated.">topological
sort</a>.</p>
</dd>
<dt><dfn id="jump-search">jump search</dfn></dt>
<dd>
<p>An algorithm for searching a sorted list, that falls between <a
href="section-14.html#sequential-search" class="term"
title="The simplest search algorithm: In an array, simply look at the array elements in the order that they appear.">sequential
search</a> and <a href="section-14.html#binary-search" class="term"
title="A standard recursive algorithm for finding the record with a given search key value within a sorted list. It runs in $O(\log n)$ time. At each step, look at the middle of the current sublist, and throw away the half of the records whose keys are either too small or too large.">binary
search</a> in both computational cost and conceptual complexity. The
idea is to keep jumping by some fixed number of positions until a value
is found that is bigger than <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
then do a sequential search over the subarray that is now known to
contain the search key. The optimal number of steps to jump will be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mi>n</mi></msqrt><annotation encoding="application/x-tex">\sqrt{n}</annotation></semantics></math>
for an array of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
and the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a> cost will be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msqrt><mi>n</mi></msqrt><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\sqrt{n})</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="k-ary-tree">K-ary tree</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#full-tree" class="term"
title="A binary tree is full if every node is either a leaf node or else it is an internal node with two non-empty children.">full
tree</a> where every internal node has exactly
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
<a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.</p>
</dd>
<dt><dfn id="k-path">k-path</dfn></dt>
<dd>
<p>In <a href="section-14.html#floyds-algorithm" class="term"
title="An algorithm to solve the all-pairs shortest paths problem. It uses the dynamic programming algorithmic technique, and runs in $O(n^3)$ time. As with any dynamic programming algorithm, the key issue is to avoid duplicating work by using proper bookkeeping on the algorithm&#39;s progress through the solution space. The basic idea is to first find all the direct edge costs, then improving those costs by allowing paths through vertex 0, then the cheapest paths involving paths going through vertices 0 and 1, and so on.">Floyd’s
algorithm</a>, a k-path is a path between two vertices
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
that can only go through vertices with an index value less than or equal
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="kd-tree">kd tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#spatial-data-structure" class="term"
title="A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree.">spatial
data structure</a> that uses a binary tree to store a collection of data
records based on their (point) location in space. It uses the concept of
a <a href="section-14.html#discriminator" class="term"
title="A part of a multi-dimensional search key. Certain tree data structures such as the bintree and the kd tree operate by making branching decisions at nodes of the tree based on a single attribute of the multi-dimensional key, with the attribute determined by the level of the node in the tree. For example, in 2 dimensions, nodes at the odd levels in the tree might branch based on the $x$ value of a coordinate, while at the even levels the tree would branch based on the $y$ value of the coordinate. Thus, the $x$ coordinate is the discriminator for the odd levels, while the $y$ coordinate is the discriminator for the even levels.">discriminator</a>
at each level to decide which single component of the <a
href="section-14.html#multi-dimensional-search-key" class="term"
title="A search key containing multiple parts, that works in conjunction with a multi-dimensional search structure. Most typically, a spatial search key representing a position in multi-dimensional (2 or 3 dimensions) space. But a multi-dimensional key could be used to organise data within non-spatial dimensions, such as temperature and time.">multi-dimensional
search key</a> to branch on at that level. It uses a <a
href="section-14.html#key-space-decomposition" class="term"
title="The idea that the range for a search key will be split into pieces. There are two general approaches to this: object-space decomposition and image-space decomposition.">key-space
decomposition</a>, meaning that all data records in the left subtree of
a node have a value on the corresponding discriminator that is less than
that of the node, while all data records in the right subtree have a
greater value. The <a href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>
is the <a href="section-14.html#image-space-decomposition" class="term"
title="A from of key-space decomposition where the key space splitting points is predetermined (typically by splitting in half). For example, a Huffman coding tree splits the letters being coded into those with codes that start with 0 on the left side, and those with codes that start with 1 on the right side. This regular decomposition of the key space is the basis for a trie data structure. An image-space decomposition is in opposition to an object-space decomposition.">image-space
decomposition</a> analog of the kd tree.</p>
</dd>
<dt><dfn id="key-sort">key sort</dfn></dt>
<dd>
<p>Any sorting operation applied to a collection of <a
href="section-14.html#key-value-pair" class="term"
title="A standard solution for solving the problem of how to relate a key value to a record (or how to find the key for a given record) within the context of a particular index. The idea is to simply store as records in the index pairs of keys and records. Specifically, the index will typically store a copy of the key along with a reference to the record. The other standard solution to this problem is to pass a comparator function to the index.">key-value
pairs</a> where the value in this case is a <a
href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to a complete record (that is, a pointer to the record in memory or a
position for a record on disk). This is in contrast to a sorting
operation that works directly on a collection of records. The intention
is that the collection of key-value pairs is far smaller than the
collection of records themselves. As such, this might allow for an <a
href="section-14.html#internal-sort" class="term"
title="A sorting algorithm that is applied to data stored in main memory. This is in contrast to an external sort that is meant to work on data stored in peripheral storage such as on a disk drive.">internal
sort</a> when sorting the records directly would require an <a
href="section-14.html#external-sort" class="term"
title="A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory.">external
sort</a>. The collection of key-value pairs can also act as an <a
href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">index</a>.</p>
</dd>
<dt><dfn id="key-space">key space</dfn></dt>
<dd>
<p>The range of values that a <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value may take on.</p>
</dd>
<dt><dfn id="key-space-decomposition">key-space decomposition</dfn></dt>
<dd>
<p>The idea that the range for a <a href="section-14.html#search-key"
class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a> will be split into pieces. There are two general approaches to
this: <a href="section-14.html#object-space-decomposition" class="term"
title="A from of key-space decomposition where the key space is determined by the actual values of keys that are found. For example, a BST stores a key value in its root, and all other values in the tree with lesser value are in the left subtree. Thus, the root value has split (or decomposed) the key space for that key based on its value into left and right parts. An object-space decomposition is in opposition to an image-space decomposition.">object-space
decomposition</a> and <a
href="section-14.html#image-space-decomposition" class="term"
title="A from of key-space decomposition where the key space splitting points is predetermined (typically by splitting in half). For example, a Huffman coding tree splits the letters being coded into those with codes that start with 0 on the left side, and those with codes that start with 1 on the right side. This regular decomposition of the key space is the basis for a trie data structure. An image-space decomposition is in opposition to an object-space decomposition.">image-space
decomposition</a>.</p>
</dd>
<dt><dfn id="key-value-pair">key-value pair</dfn></dt>
<dd>
<p>A standard solution for solving the problem of how to relate a <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value to a record (or how to find the key for a given record) within the
context of a particular <a href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">index</a>.
The idea is to simply store as records in the index pairs of keys and
records. Specifically, the index will typically store a copy of the key
along with a <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to the record. The other standard solution to this problem is to pass a
<a href="section-14.html#comparator" class="term"
title="A function given as a parameter to a method of a library (or alternatively, a parameter for a C++ template or a Java generic). The comparator function concept provides a generic way to encapulate the process of performing a comparison between two objects of a specific type. For example, if we want to write a generic sorting routine, that can handle any record type, we can require that the user of the sorting routine passes in a comparator function to define how records in the collection are to be compared.">comparator</a>
function to the index.</p>
</dd>
<dt><dfn id="key">key</dfn></dt>
<dd>
<p>A field or part of a larger record used to represent that record for
the purpose of searching or comparing. Another term for <a
href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a>.</p>
</dd>
<dt><dfn id="knapsack-problem">knapsack problem</dfn></dt>
<dd>
<p>While there are many variations of this problem, here is a typical
version: Given knapsack of a fixed size, and a collection of objects of
various sizes, is there a subset of the objects that exactly fits into
the knapsack? This problem is known to be <a
href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-complete</a>,
but can be solved for problem instances in practical time relatively
quickly using <a href="section-14.html#dynamic-programming" class="term"
title="An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem.">dynamic
programming</a>. Thus, it is considered to have <a
href="section-14.html#pseudo-polynomial" class="term"
title="In complexity analysis, refers to the time requirements of an algorithm for an NP-Complete problem that still runs acceptably fast for practical application. An example is the standard dynamic programming algorithm for the knapsack problem.">pseudo-polynomial</a>
cost. An <a href="section-14.html#optimisation-problem" class="term"
title="Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the *traveling salesman problem*, where visiting $n$ cities in some order has a cost, and the goal is to visit in the cheapest order.">optimisation
problem</a> version is to find the subset that can fit with the greatest
amount of items, either in terms of their total size, or in terms of the
sum of values associated with each item.</p>
</dd>
<dt><dfn id="kruskals-algorithm">Kruskal’s algorithm</dfn></dt>
<dd>
<p>An algorithm for computing the <a href="section-14.html#mst"
class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">MST</a>
of a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.
During processing, it makes use of the <a
href="section-14.html#union-find" class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
process to efficiently determine of two vertices are within the same <a
href="section-14.html#subgraph" class="term"
title="A subgraph $\mathbf{S}$ is formed from graph $\mathbf{G}$ by selecting a subset $\mathbf{V}_s$ of $\mathbf{G}$&#39;s vertices and a subset $\mathbf{E}_s$ of $\mathbf{G}$&#39;s edges such that for every edge $e \in \mathbf{E}_s$, both vertices of $e$ are in $\mathbf{V}_s$.">subgraph</a>.</p>
</dd>
<dt><dfn id="labeled-graph">labeled graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
with labels associated with the <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>.</p>
</dd>
<dt><dfn id="language">language</dfn></dt>
<dd>
<p>A set of strings.</p>
</dd>
<dt><dfn id="las-vegas-algorithms">Las Vegas algorithms</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#randomised-algorithm" class="term"
title="An algorithm that involves some form of randomness to control its behaviour. The ultimate goal of a randomised algorithm is to improve performance over a deterministic algorithm to solve the same problem. There are a number of variations on this theme. A &#39;Las Vegas algorithm&#39; returns a correct result, but the amount of time required might or might not improve over a deterministic algorithm. A &#39;Monte Carlo algorithm&#39; is a form of probabilistic algorithm that is not guarenteed to return a correct result, but will return a result relatively quickly.">randomised
algorithm</a>. We always find the maximum value, and “usually” we find
it fast. Such algorithms have a guaranteed result, but do not guarantee
fast running time.</p>
</dd>
<dt><dfn id="leaf-node">leaf node</dfn></dt>
<dd>
<p>In a <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a>, leaf node is any node that has two empty <a
href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.
(Note that a binary tree is defined so that every node has two children,
and that is why the leaf node has to have two empty children, rather
than no children.) In a general tree, any node is a leaf node if it has
no children.</p>
</dd>
<dt><dfn id="least-frequently-used">least frequently used</dfn> (<dfn
id="lfu">LFU</dfn>)</dt>
<dd>
<p>Abbreviated LFU, it is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
that can be used to decide which <a href="section-14.html#buffer"
class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
in a <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a> to <a href="section-14.html#flush" class="term"
title="The act of removing data from a cache, most typically because other data considered of higher future value must replace it in the cache. If the data being flushed has been modified since it was first read in from secondary storage (and the changes are meant to be saved), then it must be written back to that secondary storage.">flush</a>
when data in the buffer pool must be replaced by new data being read
into a <a href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">cache</a>.
However, <a href="section-14.html#least-recently-used" class="term"
title="Abbreviated LRU, it is a popular heuristic to use for deciding which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. Analogous to the move-to-front heuristic for maintaining a self-organising list.">least
recently used</a> is more popular than LFU. Analogous to the <a
href="section-14.html#frequency-count" class="term"
title="A heuristic used to maintain a self-organising list. Under this heuristic, a count is maintained for every record. When a record access is made, its count is increased. If this makes its count greater than that of another record in the list, it moves up toward the front of the list accordingly so as to keep the list sorted by frequency. Analogous to the least frequently used heuristic for maintaining a buffer pool.">frequency
count</a> heuristic for maintaining a <a
href="section-14.html#self-organising-list" class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>.</p>
</dd>
<dt><dfn id="least-recently-used">least recently used</dfn> (<dfn
id="lru">LRU</dfn>)</dt>
<dd>
<p>Abbreviated LRU, it is a popular <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
to use for deciding which <a href="section-14.html#buffer" class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
in a <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a> to <a href="section-14.html#flush" class="term"
title="The act of removing data from a cache, most typically because other data considered of higher future value must replace it in the cache. If the data being flushed has been modified since it was first read in from secondary storage (and the changes are meant to be saved), then it must be written back to that secondary storage.">flush</a>
when data in the buffer pool must be replaced by new data being read
into a <a href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">cache</a>.
Analogous to the <a href="section-14.html#move-to-front" class="term"
title="A heuristic used to maintain a self-organising list. Under this heuristic, whenever a record is accessed it is moved to the front of the list. Analogous to the least recently used heuristic for maintaining a buffer pool.">move-to-front</a>
heuristic for maintaining a <a
href="section-14.html#self-organising-list" class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>.</p>
</dd>
<dt><dfn id="left-recursive">left recursive</dfn></dt>
<dd>
<p>In automata theory, a <a href="section-14.html#production"
class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production</a>
is left recursive if it is of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">A \rightarrow Ax</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><mi>V</mi><mo>,</mo><mi>x</mi><mo>∈</mo><mo stretchy="false" form="prefix">(</mo><mi>V</mi><mo>∪</mo><mi>T</mi><msup><mo stretchy="false" form="postfix">)</mo><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">A \in V, x \in (V \cup T)^*</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>
is the set of <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminals</a>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
is the set of <a href="section-14.html#terminal" class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminals</a>
in the <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>.</p>
</dd>
<dt><dfn id="length">length</dfn></dt>
<dd>
<p>In a <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>,
the number of elements. In a string, the number of characters.</p>
</dd>
<dt><dfn id="level">level</dfn></dt>
<dd>
<p>In a tree, all nodes of <a href="section-14.html#depth" class="term"
title="The depth of a node $M$ in a tree is the length of the path from the root of the tree to $M$.">depth</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
are at level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
in the tree. The root is the only node at level 0, and its depth is
0.</p>
</dd>
<dt><dfn id="lexical-analysis">lexical analysis</dfn></dt>
<dd>
<p>A phase of a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
or <a href="section-14.html#interpreter" class="term"
title="In contrast to a compiler that translates a high-level program into something that can be repeatedly executed to perform a computation, an interpreter directly performs computation on the high-level langauge. This tends to make the computation much slower than if it were performed on the directly executable version produced by a compiler.">interpreter</a>
responsible for reading in characters of the program or language and
grouping them into <a href="section-14.html#token" class="term"
title="The basic logical units of a program, as deterimined by lexical analysis. These are things like arithmetic operators, language keywords, variable or function names, or numbers.">tokens</a>.</p>
</dd>
<dt><dfn id="lexical-scoping">lexical scoping</dfn></dt>
<dd>
<p>Within programming languages, the convention of allowing access to a
variable only within the block of code in which the variable is defined.
A synonym for <a href="section-14.html#static-scoping" class="term"
title="A synonym for lexical scoping.">static scoping</a>.</p>
</dd>
<dt><dfn>LFU</dfn></dt>
<dd>
<p>See <a href="section-14.html#least-frequently-used" class="term"
title="Abbreviated LFU, it is a heuristic that can be used to decide which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. However, least recently used is more popular than LFU. Analogous to the frequency count heuristic for maintaining a self-organising list.">least
frequently used</a></p>
</dd>
<dt><dfn id="lifetime">lifetime</dfn></dt>
<dd>
<p>For a variable, lifetime is the amount of time it will exist before
it is destroyed.</p>
</dd>
<dt><dfn id="lifo">LIFO</dfn></dt>
<dd>
<p>Abbreviation for “last-in, first-out”. This is the access paradigm
for a <a href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>,
and an old terminolgy for the stack is “LIFO list”.</p>
</dd>
<dt><dfn id="linear-congruential-method">linear congruential
method</dfn></dt>
<dd>
<p>In random number theory, a process for computing the next number in a
<a href="section-14.html#pseudo-random" class="term"
title="In random number theory this means that, given all past terms in the series, no future term of the series can be accurately predicted in polynomial time.">pseudo-random</a>
sequence. Starting from a <a href="section-14.html#seed" class="term"
title="In random number theory, the starting value for a random number series. Typically used with any linear congruential method.">seed</a>,
the next term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(i)</annotation></semantics></math>
in the series is calculated from term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">r(i-1)</annotation></semantics></math>
by the equation</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mi>r</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mo>×</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo><mrow><mspace width="0.222em"></mspace><mrow><mi>mod</mi><mo>&#8289;</mo></mrow><mspace width="0.222em"></mspace><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">r(i) = (r(i-1)\times b) \bmod t</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>
are constants. These constants must be well chosen for the resulting
series of numbers to have desirable properties as a random number
sequence.</p>
</dd>
<dt><dfn id="linear-growth-rate">linear growth rate</dfn></dt>
<dd>
<p>For input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
a growth rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">cn</annotation></semantics></math>
(for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
any positive constant). In other words, the cost of the associated
function is linear on the input size.</p>
</dd>
<dt><dfn id="linear-index">linear index</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#indexing" class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">indexing</a>
that stores <a href="section-14.html#key-value-pair" class="term"
title="A standard solution for solving the problem of how to relate a key value to a record (or how to find the key for a given record) within the context of a particular index. The idea is to simply store as records in the index pairs of keys and records. Specifically, the index will typically store a copy of the key along with a reference to the record. The other standard solution to this problem is to pass a comparator function to the index.">key-value
pairs</a> in a sorted array. Typically this is used for an index to a
large collection of records stored on disk, where the linear index
itself might be on disk or in <a href="section-14.html#main-memory"
class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">main
memory</a>. It allows for efficient search (including for <a
href="section-14.html#range-query" class="term"
title="Records are returned if their relevant key value falls within a specified range.">range
queries</a>), but it is not good for inserting and deleting entries in
the array. Therefore, it is an ideal indexing structure for when the
system needs to do range queries but the collection of records never
changes once the linear index has been created.</p>
</dd>
<dt><dfn id="linear-order">linear order</dfn></dt>
<dd>
<p>Another term for <a href="section-14.html#total-order" class="term"
title="A binary relation on a set where every pair of distinct elements in the set are comparable (that is, one can determine which of the two is greater than the other).">total
order</a>.</p>
</dd>
<dt><dfn id="linear-probing-by-steps">linear probing by steps</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
this <a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method is a variation on simple <a
href="section-14.html#linear-probing" class="term"
title="In hashing, this is the simplest collision resolution method. Term $i$ of the probe sequence is simply $i$, meaning that collision resolution works by moving sequentially through the hash table from the home slot. While simple, it is also inefficient, since it quickly leads to certain free slots in the hash table having higher probability of being selected during insertion or search.">linear
probing</a>. Some constant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
is defined such that term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the <a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">ci</annotation></semantics></math>.
This means that collision resolution works by moving sequentially
through the hash table from the <a href="section-14.html#home-slot"
class="term"
title="In hashing, this is the slot in the hash table determined for a given key by the hash function.">home
slot</a> in steps of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>.
While not much improvement on linear probing, it forms the basis of
another collision resolution method called <a
href="section-14.html#double-hashing" class="term"
title="A collision resolution method. A second hash function is used to generate a value $c$ on the key. That value is then used by this key as the step size in linear probing by steps. Since different keys use different step sizes (as generated by the second hash function), this process avoids the clustering caused by standard linear probing by steps.">double
hashing</a>, where each key uses a value for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
defined by a second <a href="section-14.html#hash-function" class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>.</p>
</dd>
<dt><dfn id="linear-probing">linear probing</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
this is the simplest <a href="section-14.html#collision-resolution"
class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method. Term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the <a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> is simply
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
meaning that collision resolution works by moving sequentially through
the hash table from the <a href="section-14.html#home-slot" class="term"
title="In hashing, this is the slot in the hash table determined for a given key by the hash function.">home
slot</a>. While simple, it is also inefficient, since it quickly leads
to certain free <a href="section-14.html#slot" class="term"
title="In hashing, a position in a hash table.">slots</a> in the hash
table having higher probability of being selected during insertion or
search.</p>
</dd>
<dt><dfn id="linear-search">linear search</dfn></dt>
<dd>
<p>Another name for <a href="section-14.html#sequential-search"
class="term"
title="The simplest search algorithm: In an array, simply look at the array elements in the order that they appear.">sequential
search</a>.</p>
</dd>
<dt><dfn id="linearithmic-growth-rate">linearithmic growth
rate</dfn></dt>
<dd>
<p>For input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
a growth rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">cn \log n</annotation></semantics></math>
(for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
any positive constant). In other words, the cost of the associated
function is slightly larger than linear on the input size.</p>
</dd>
<dt><dfn id="link-node">link node</dfn></dt>
<dd>
<p>A widely used supporting object that forms the basic building block
for a <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> and similar <a href="section-14.html#data-structure"
class="term" title="The implementation for an ADT.">data structures</a>.
A link node contains one or more fields that store data, and a <a
href="section-14.html#pointer" class="term"
title="A variable whose value is the address of another variable; a link.">pointer</a>
or <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to another link node.</p>
</dd>
<dt><dfn id="linked-list">linked list</dfn></dt>
<dd>
<p>An implementation for the list ADT that uses <a
href="section-14.html#dynamic-allocation" class="term"
title="The act of creating an object from free store. In C++, Java, and Javascript, this is done using the `new` operator.">dynamic
allocation</a> of <a href="section-14.html#link-node" class="term"
title="A widely used supporting object that forms the basic building block for a linked list and similar data structures. A link node contains one or more fields that store data, and a pointer or reference to another link node.">link
nodes</a> to store the list elements. Common variants are the <a
href="section-14.html#singly-linked-list" class="term"
title="A linked list implementation variant where each list node contains access a pointer only to the next element in the list.">singly
linked list</a>, <a href="section-14.html#doubly-linked-list"
class="term"
title="A linked list implementation variant where each list node contains access pointers to both the previous element and the next element on the list.">doubly
linked list</a> and <a href="section-14.html#circular-list" class="term"
title="A list ADT implementation variant where the last element of the list provides access to the first element of the list.">circular
list</a>. The <a href="section-14.html#overhead" class="term"
title="All information stored by a data structure aside from the actual data. For example, the pointer fields in a linked list or BST, or the unused positions in an array-based list.">overhead</a>
required is the pointers in each link node.</p>
</dd>
<dt><dfn id="linked-queue">linked queue</dfn></dt>
<dd>
<p>Analogous to a <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a>, this uses <a href="section-14.html#dynamic-allocation"
class="term"
title="The act of creating an object from free store. In C++, Java, and Javascript, this is done using the `new` operator.">dynamic
allocation</a> of nodes to store the elements when implementing the <a
href="section-14.html#queue" class="term"
title="A list-like structure in which elements are inserted only at one end, and removed only from the other one end.">queue</a>
ADT.</p>
</dd>
<dt><dfn id="linked-stack">linked stack</dfn></dt>
<dd>
<p>Analogous to a <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a>, this uses <a href="section-14.html#dynamic-allocation"
class="term"
title="The act of creating an object from free store. In C++, Java, and Javascript, this is done using the `new` operator.">dynamic
allocation</a> of nodes to store the elements when implementing the <a
href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>
ADT.</p>
</dd>
<dt><dfn id="list">list</dfn></dt>
<dd>
<p>A finite, ordered sequence of <a href="section-14.html#data-item"
class="term"
title="A piece of information or a record whose value is drawn from a type.">data
items</a> known as <a href="section-14.html#element" class="term"
title="One value or member in a set.">elements</a>. This is close to the
mathematical concept of a <a href="section-14.html#sequence"
class="term"
title="In set notation, a collection of elements with an order, and which may contain duplicate-valued elements. A sequence is also sometimes called a tuple or a vector.">sequence</a>.
Note that “ordered” in this definition means that the list elements have
position. It does not refer to the relationship between <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values for the list elements (that is, “ordered” does not mean
“sorted”).</p>
</dd>
<dt><dfn id="literal">literal</dfn></dt>
<dd>
<p>In a <a href="section-14.html#boolean-expression" class="term"
title="A Boolean expression is comprised of Boolean variable combined using the operators AND ($\cdot$), OR ($+$), and NOT (to negate Boolean variable $x$ we write $\overline{x}$).">Boolean
expression</a>, a <a href="section-14.html#literal" class="term"
title="In a Boolean expression, a literal is a Boolean variable or its negation. In the context of compilers, it is any constant value. Similar to a terminal.">literal</a>
is a <a href="section-14.html#boolean-variable" class="term"
title="A variable that takes on one of the two values `True` and `False`.">Boolean
variable</a> or its negation. In the context of compilers, it is any
constant value. Similar to a <a href="section-14.html#terminal"
class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminal</a>.</p>
</dd>
<dt><dfn id="load-factor">load factor</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>
this is the fraction of the <a href="section-14.html#hash-table"
class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> <a href="section-14.html#slot" class="term"
title="In hashing, a position in a hash table.">slots</a> that contain a
record. Hash systems usually try to keep the load factor below 50%.</p>
</dd>
<dt><dfn id="local-storage">local storage</dfn></dt>
<dd>
<p>local storage.</p>
</dd>
<dt><dfn id="local-variable">local variable</dfn> (<dfn
id="automatic-variable">automatic variable</dfn>, <dfn
id="stack-variable">stack variable</dfn>)</dt>
<dd>
<p>A variable declared within a function or method. It exists only from
the time when the function is called to when the function exits. When a
function is suspended (due to calling another function), the function’s
local variables are stored in an <a
href="section-14.html#activation-record" class="term"
title="The entity that is stored on the runtime stack during program execution. It stores any active local variable and the return address from which a new subroutine is being called, so that this information can be recovered when the subroutine terminates.">activation
record</a> on the <a href="section-14.html#runtime-stack" class="term"
title="The place where an activation record is stored when a subroutine is called during a program&#39;s runtime.">runtime
stack</a>.</p>
</dd>
<dt><dfn id="locality-of-reference">locality of reference</dfn></dt>
<dd>
<p>The concept that accesses within a collection of records is not
evenly distributed. This can express itself as some small fraction of
the records receiving the bulk of the accesses (<a
href="section-14.html#80-20-rule" class="term"
title="Given a typical application where there is a collection of records and a series of search operations for records, the 80/20 rule is an empirical observation that 80% of the record accessess typically go to 20% of the records. The exact values varies between data collections, and is related to the concept of locality of reference.">80/20
rule</a>). Alternatively, it can express itself as an increased
probability that the next or future accesses will come close to the most
recent access. This is the fundamental property for success of <a
href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">caching</a>.</p>
</dd>
<dt><dfn id="logarithm">logarithm</dfn></dt>
<dd>
<p>The <em>logarithm</em> of base
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
for value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the power to which
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is raised to get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="logarithmic-growth-rate">logarithmic growth rate</dfn></dt>
<dd>
<p>For input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
a growth rate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">c \log n</annotation></semantics></math>
(for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
any positive constant). In other words, the cost of the associated
function is logarithmic on the input size.</p>
</dd>
<dt><dfn id="logical-file">logical file</dfn></dt>
<dd>
<p>In <a href="section-14.html#file-processing" class="term"
title="The domain with computer science that deals with processing data stored on a disk drive (in a file), or more broadly, dealing with data stored on any peripheral storage device. Two fundamental properties make dealing with data on a peripheral device different from dealing with data in main memory: (1) Reading/writing data on a peripheral storage device is far slower than reading/writing data to main memory (for example, a typical disk drive is about a million times slower than RAM). (2) All I/O to a peripheral device is typically in terms of a block of data (for example, nearly all disk drives do all I/O in terms of blocks of 512 bytes).">file
processing</a>, the programmer’s view of a <a
href="section-14.html#random-access" class="term"
title="In file processing terminology, a disk access to a random position within the file. More generally, the ability to access an arbitrary record in the file.">random
access</a> file stored on <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk</a>
as a contiguous series of bytes, with those bytes possibly combining to
form data records. This is in contrast to the <a
href="section-14.html#physical-file" class="term"
title="The collection of sectors that comprise a file on a disk drive. This is in contrast to the logical file.">physical
file</a>.</p>
</dd>
<dt><dfn id="logical-form">logical form</dfn></dt>
<dd>
<p>The definition for a data type in terms of an ADT. Contrast to the <a
href="section-14.html#physical-form" class="term"
title="The implementation of a data type as a data structure. Contrast to the logical form for the data type.">physical
form</a> for the data type.</p>
</dd>
<dt><dfn id="lookup-table">lookup table</dfn></dt>
<dd>
<p>A table of pre-calculated values, used to speed up processing time
when the values are going to be viewed many times. The costs to this
approach are the space required for the table and the time required to
compute the table. This is an example of a <a
href="section-14.html#space-time-tradeoff" class="term"
title="Many programs can be designed to either speed processing at the cost of additional storage, or reduce storage at the cost of additional processing time.">space/time
tradeoff</a>.</p>
</dd>
<dt><dfn id="lower-bound">lower bound</dfn></dt>
<dd>
<p>An lower bound for a <a href="section-14.html#growth-rate"
class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
is any growth rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
that is less than or equal to it. Formally, there are constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mn>0</mn></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n_0 \geq 0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c &gt; 0</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>≥</mo><mi>c</mi><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \geq c g(n)</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≥</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">n \geq n_0</annotation></semantics></math>.
We also write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f \in \Omega(g)</annotation></semantics></math>
or slightly imprecisely
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>∈</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \in \Omega(g(n))</annotation></semantics></math>
(this is <a href="section-14.html#omega-notation" class="term"
title="For growth rates $f$ and $g$, we write $f \in \Omega(g)$ to say that $g$ is a lower bound for $f$. The notation can be made sense of by defining $\Omega(g)$ as the set of functions with growth rate greater than or equal to that of $g$. The notation is often somewhat imprecisely used as $f(n) \in \Omega(g(n))$ or even $f(n) = \Omega(g(n))$.">Omega
notation</a>).</p>
<p>Usually, we are interested in finding a lower bound
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
that has a simple expression compared to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>,
but is still sharp (there is not much room for improvement).</p>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, a lower bound for an algorithm is a lower bound for the <a
href="section-14.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortised complexity.">asymptotic
complexity</a> of the algorithm, the growth rate of its <a
href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>.</p>
</dd>
<dt><dfn id="lower-bounds-proof">lower bounds proof</dfn></dt>
<dd>
<p>A proof regarding the lower bound, with this term most typically
referring to the lower bound for any possible algorithm to solve a given
<a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
Many problems have a simple lower bound based on the concept that the
minimum amount of processing is related to looking at all of the
problem’s input. However, some problems have a higher lower bound than
that. For example, the lower bound for the problem of sorting
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>)
is greater than the input size to sorting
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>).
Proving such “non-trivial” lower bounds for problems is notoriously
difficult.</p>
</dd>
<dt><dfn>LRU</dfn></dt>
<dd>
<p>See <a href="section-14.html#least-recently-used" class="term"
title="Abbreviated LRU, it is a popular heuristic to use for deciding which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. Analogous to the move-to-front heuristic for maintaining a self-organising list.">least
recently used</a></p>
</dd>
<dt><dfn>main memory</dfn></dt>
<dd>
<p>See <a href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a></p>
</dd>
<dt><dfn id="map">map</dfn></dt>
<dd>
<p>A <a href="section-14.html#data-structure" class="term"
title="The implementation for an ADT.">data structure</a> that relates a
<a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
to a <a href="section-14.html#record" class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">record</a>.</p>
</dd>
<dt><dfn id="mapping">mapping</dfn></dt>
<dd>
<p>A <a href="section-14.html#function" class="term"
title="In mathematics, a matching between inputs (the domain) and outputs (the range). In programming, a subroutine that takes input parameters and uses them to compute and return a value. In this case, it is usually considered bad practice for a function to change any global variables (doing so is called a side effect).">function</a>
that maps every element of a given <a href="section-14.html#set"
class="term"
title="A collection of distinguishable members or elements.">set</a> to
a unique element of another set; a correspondence.</p>
</dd>
<dt><dfn id="mark-array">mark array</dfn></dt>
<dd>
<p>It is typical in <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
algorithms that there is a need to track which nodes have been visited
at some point in the algorithm. An <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
of bits or values called the <a href="section-14.html#mark-array"
class="term"
title="It is typical in graph algorithms that there is a need to track which nodes have been visited at some point in the algorithm. An array of bits or values called the mark array is often maintained for this purpose.">mark
array</a> is often maintained for this purpose.</p>
</dd>
<dt><dfn id="mark-sweep-algorithm">mark/sweep algorithm</dfn></dt>
<dd>
<p>An algorithm for <a href="section-14.html#garbage-collection"
class="term"
title="Languages with garbage collection such Java, Javascript, Lisp, and Scheme will periodically reclaim garbage and return it to free store.">garbage
collection</a>. All accessible variables, and any space that is
reachable by a chain of pointers from any accessible variable, is
“marked”. Then a sequential sweep of all memory in the pool is made. Any
unmarked memory locations are assumed to not be needed by the program
and can be considered as free to be reused.</p>
</dd>
<dt><dfn id="master-theorem">master theorem</dfn></dt>
<dd>
<p>A theorem that makes it easy to solve <a
href="section-14.html#divide-and-conquer-recurrences" class="term"
title="A common form of recurrence relation that have the form \begin{align*} T(n) &amp;= a T(n/b) + cn^k \ T(1) &amp;= c \end{align*} where $a$, $b$, $c$, and $k$ are constants. In general, this recurrence describes a problem of size $n$ divided into $a$ subproblems of size $n/b$, while $cn^k$ is the amount of work necessary to combine the partial solutions.">divide-and-conquer
recurrences</a>.</p>
</dd>
<dt><dfn id="matching-problem">matching problem</dfn></dt>
<dd>
<p>Any problem that involves finding a <a
href="section-14.html#matching" class="term"
title="In graph theory, a pairing (or match) of various nodes in a graph.">matching</a>
in a graph with some desired property. For example, a well-known <a
href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-complete</a>
problem is to find a <a href="section-14.html#maximum-match"
class="term" title="In a graph, the largest possible matching.">maximum
match</a> for an undirected graph.</p>
</dd>
<dt><dfn id="matching">matching</dfn></dt>
<dd>
<p>In graph theory, a pairing (or match) of various nodes in a
graph.</p>
</dd>
<dt><dfn id="max-heap">max heap</dfn></dt>
<dd>
<p>A <a href="section-14.html#heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
where every <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
has a <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value greater than its <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.
As a consequence, the node with maximum key value is at the <a
href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>.</p>
</dd>
<dt><dfn id="maximal-match">maximal match</dfn></dt>
<dd>
<p>In a graph, any <a href="section-14.html#matching" class="term"
title="In graph theory, a pairing (or match) of various nodes in a graph.">matching</a>
that leaves no pair of unmatched vertices that are connected. A maximal
matching is not necessarily a <a href="section-14.html#maximum-match"
class="term" title="In a graph, the largest possible matching.">maximum
match</a>. In other words, there might be a larger matching than the
maximal matching that was found.</p>
</dd>
<dt><dfn id="maximum-lower-bound">maximum lower bound</dfn></dt>
<dd>
<p>The <a href="section-14.html#lower-bound" class="term"
title="An lower bound for a growth rate $f$ is any growth rate $g$ that is less than or equal to it. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \geq c g(n)$ for all $n \geq n_0$. We also write $f \in \Omega(g)$ or slightly imprecisely $f(n) \in \Omega(g(n))$ (this is Omega notation). Usually, we are interested in finding a lower bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, a lower bound for an algorithm is a lower bound for the asymptotic complexity of the algorithm, the growth rate of its complexity.">lower
bound</a> for the <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
of finding the maximum value in an unsorted list is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="maximum-match">maximum match</dfn></dt>
<dd>
<p>In a graph, the largest possible <a href="section-14.html#matching"
class="term"
title="In graph theory, a pairing (or match) of various nodes in a graph.">matching</a>.</p>
</dd>
<dt><dfn>MCST</dfn></dt>
<dd>
<p>See <a href="section-14.html#minimum-spanning-tree" class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">minimum
spanning tree</a></p>
</dd>
<dt><dfn id="measure-of-cost">measure of cost</dfn></dt>
<dd>
<p>When comparing two things, such as two algorithms, some event or unit
must be used as the basic unit of comparison. It might be number of
milliseconds needed or machine instructions expended by a program, but
it is usually desirable to have a way to do comparison between two
algorithms without writing a program. Thus, some other measure of cost
might be used as a basis for comparison between the algorithms. For
example, when comparing two sorting algorthms it is traditional to use
as a measure of cost the number of <a href="section-14.html#comparison"
class="term"
title="The act of comparing two keys or records. For many data types, a comparison has constant time cost. The number of comparisons required is often used as a measure of cost for sorting and searching algorithms.">comparisons</a>
made between the key values of record pairs.</p>
</dd>
<dt><dfn id="member-function">member function</dfn></dt>
<dd>
<p>Each operation associated with the ADT is implemented by a member
function or <a href="section-14.html#method" class="term"
title="In the object-oriented programming paradigm, a method is an operation on a class. A synonym for member function.">method</a>.</p>
</dd>
<dt><dfn id="member">member</dfn></dt>
<dd>
<p>In set notation, this is a synonym for <a
href="section-14.html#element" class="term"
title="One value or member in a set.">element</a>. In abstract design, a
<a href="section-14.html#data-item" class="term"
title="A piece of information or a record whose value is drawn from a type.">data
item</a> is a member of a <a href="section-14.html#type" class="term"
title="A collection of values.">type</a>. In an object-oriented
language, <a href="section-14.html#data-member" class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
members</a> are data fields in an object.</p>
</dd>
<dt><dfn id="memory-allocation">memory allocation</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, the act of honoring a request for memory.</p>
</dd>
<dt><dfn id="memory-deallocation">memory deallocation</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, the act of freeing a block of memory, which should create
or add to a <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a>.</p>
</dd>
<dt><dfn id="memory-hierarchy">memory hierarchy</dfn></dt>
<dd>
<p>The concept that a computer system stores data in a range of storage
types that range from fast but expensive (<a
href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a>) to slow but cheap (<a
href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a>). When there is too much data to store in <a
href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a>, the goal is to have the data that is needed soon or most
often in the primary storage as much as possible, by using <a
href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">caching</a>
techniques.</p>
</dd>
<dt><dfn id="memory-leak">memory leak</dfn></dt>
<dd>
<p>In programming, the act of creating <a href="section-14.html#garbage"
class="term"
title="In memory management, any memory that was previously (dynamically) allocated by the program during runtime, but which is no longer accessible since all pointers to the memory have been deleted or overwritten. In some languages, garbage can be recovered by garbage collection. In languages such as C and C++ that do not support garbage collection, so creating garbage is considered a memory leak.">garbage</a>.
In languages such as C and C++ that do not support <a
href="section-14.html#garbage-collection" class="term"
title="Languages with garbage collection such Java, Javascript, Lisp, and Scheme will periodically reclaim garbage and return it to free store.">garbage
collection</a>, repeated memory leaks will evenually cause the program
to terminate.</p>
</dd>
<dt><dfn id="memory-manager">memory manager</dfn></dt>
<dd>
<p>Functionality for managing a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. Typically, the memory pool is viewed as an <a
href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
of bytes by the memory manager. The <a href="section-14.html#client"
class="term"
title="The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool.">client</a>
of the memory manager will request a collection of (adjacent) bytes of
some size, and release the bytes for reuse when the space is no longer
needed. The memory manager should not know anything about the
interpretation of the data that is being stored by the client into the
memory pool. Depending on the precise implementation, the client might
pass in the data to be stored, in which case the memory manager will
deal with the actual copy of the data into the memory pool. The memory
manager will return to the client a <a href="section-14.html#handle"
class="term"
title="When using a memory manager to store data, the client will pass data to be stored (the message) to the memory manager, and the memory manager will return to the client a handle. The handle encodes the necessary information that the memory manager can later use to recover and return the message to the client. This is typically the location and length of the message within the memory pool.">handle</a>
that can later be used by the client to retrieve the data.</p>
</dd>
<dt><dfn id="memory-pool">memory pool</dfn></dt>
<dd>
<p>Memory (usually in <a href="section-14.html#ram" class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>
but possibly on disk or <a href="section-14.html#peripheral-storage"
class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a> device) that is logically viewed as an array of memory
positions. A memory pool is usually managed by a <a
href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>.</p>
</dd>
<dt><dfn id="memory-request">memory request</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, a request from some <a href="section-14.html#client"
class="term"
title="The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool.">client</a>
to the memory manager to reserve a block of memory and store some bytes
there.</p>
</dd>
<dt><dfn id="merge-insert-sort">merge insert sort</dfn></dt>
<dd>
<p>A synonym for the <a href="section-14.html#ford-and-johnson-sort"
class="term"
title="A sorting algorithm that is close to the theoretical minimum number of key comparisons necessary to sort. Generally not considered practical in practice due to the fact that it is not efficient in terms of the number of records that need to be moved. It consists of first sorting pairs of nodes into winners and losers (of the pairs comparisons), then (recursively) sorting the winners of the pairs, and then finally carefully selecting the order in which the losers are added to the chain of sorted items.">Ford
and Johnson sort</a>.</p>
</dd>
<dt><dfn id="mergesort">Mergesort</dfn></dt>
<dd>
<p>A sorting algorithm that requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>
in the <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best</a>,
<a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>,
and <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst</a>
cases. Conceptually it is simple: Split the list in half, sort the
halves, then merge them together. It is a bit complicated to implement
efficiently on an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>.</p>
</dd>
<dt><dfn id="message-passing">message passing</dfn></dt>
<dd>
<p>A common approach to implementing the <a href="section-14.html#adt"
class="term"
title="Abbreviated ADT. The specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behaviour of each operation is determined by its inputs and outputs. An ADT does not specify *how* the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation.">ADT</a>
for a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a> or <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>, where the contents of a <a href="section-14.html#message"
class="term"
title="In a memory manager implementation (particularly a memory manager implemented with a message passing style of interface), the message is the data that the client of the memory manager wishes to have stored in the memory pool. The memory manager will reply to the client by returning a handle that defines the location and size of the message as stored in the memory pool. The client can later recover the message by passing the handle back to the memory manager.">message</a>
to be stored is explicitly passed between the client and the memory
manager. This is in contrast to a <a
href="section-14.html#buffer-passing" class="term"
title="An approach to implementing the ADT for a buffer pool, where a pointer to a buffer is passed between the client and the buffer pool. This is in contrast to a message passing approach, it is most likely to be used for long messages or when the message size is always the same as the buffer size, such as when implementing a B-tree.">buffer
passing</a> approach.</p>
</dd>
<dt><dfn id="message">message</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a> implementation (particularly a memory manager implemented
with a <a href="section-14.html#message-passing" class="term"
title="A common approach to implementing the ADT for a memory manager or buffer pool, where the contents of a message to be stored is explicitly passed between the client and the memory manager. This is in contrast to a buffer passing approach.">message
passing</a> style of <a href="section-14.html#interface" class="term"
title="An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members.">interface</a>),
the message is the data that the <a href="section-14.html#client"
class="term"
title="The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool.">client</a>
of the memory manager wishes to have stored in the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. The memory manager will reply to the client by returning a <a
href="section-14.html#handle" class="term"
title="When using a memory manager to store data, the client will pass data to be stored (the message) to the memory manager, and the memory manager will return to the client a handle. The handle encodes the necessary information that the memory manager can later use to recover and return the message to the client. This is typically the location and length of the message within the memory pool.">handle</a>
that defines the location and size of the message as stored in the
memory pool. The client can later recover the message by passing the
handle back to the memory manager.</p>
</dd>
<dt><dfn id="metaphor">metaphor</dfn></dt>
<dd>
<p>Humans deal with complexity by assigning a label to an assembly of
objects or concepts and then manipulating the label in place of the
assembly. Cognitive psychologists call such a label a metaphor.</p>
</dd>
<dt><dfn id="method">method</dfn></dt>
<dd>
<p>In the <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming paradigm</a>, a method is an operation on a <a
href="section-14.html#class" class="term"
title="In the object-oriented programming paradigm an ADT and its implementation together make up a class. An instantiation of a class within a program is termed an object.">class</a>.
A synonym for <a href="section-14.html#member-function" class="term"
title="Each operation associated with the ADT is implemented by a member function or method.">member
function</a>.</p>
</dd>
<dt><dfn id="mid-square-method">mid-square method</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
an approach to implementing a <a href="section-14.html#hash-function"
class="term"
title="In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value.">hash
function</a>. The key value is squared, and some number of bits from the
middle of the resulting value are extracted as the hash code. Some care
must be taken to extract bits that tend to actually be in the middle of
the resulting value, which requires some understanding of the typical
key values. When done correctly, this has the advantage of having the
hash code be affected by all bits of the key</p>
</dd>
<dt><dfn id="min-heap">min heap</dfn></dt>
<dd>
<p>A <a href="section-14.html#heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
where every <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
has a <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value less than its <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.
As a consequence, the node with minimum key value is at the <a
href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>.</p>
</dd>
<dt><dfn>minimal-cost spanning tree</dfn></dt>
<dd>
<p>See <a href="section-14.html#minimum-spanning-tree" class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">minimum
spanning tree</a></p>
</dd>
<dt><dfn id="minimum-external-path-weight">minimum external path
weight</dfn></dt>
<dd>
<p>Given a collection of objects, each associated with a <a
href="section-14.html#leaf-node" class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaf
node</a> in a tree, the binary tree with minimum external path weight is
the one with the minimum sum of <a
href="section-14.html#weighted-path-length" class="term"
title="Given a tree, and given a weight for each leaf in the tree, the weighted path length for a leaf is its weight times its depth.">weighted
path lengths</a> for the given set of leaves. This concept is used to
create a <a href="section-14.html#huffman-coding-tree" class="term"
title="A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie.">Huffman
coding tree</a>, where a letter with high weight should have low depth,
so that it will count the least against the total path length. As a
result, another letter might be pushed deeper in the tree if it has less
weight.</p>
</dd>
<dt><dfn id="minimum-spanning-tree">minimum spanning tree</dfn> (<dfn
id="minimal-cost-spanning-tree">minimal-cost spanning tree</dfn>, <dfn
id="mst">MST</dfn>, <dfn id="mcst">MCST</dfn>)</dt>
<dd>
<p>Abbreviated as MST, or sometimes as MCST. Derived from a <a
href="section-14.html#weighted-graph" class="term"
title="A graph whose edges each have an associated weight or cost.">weighted
graph</a>, the MST is the <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
of the graph’s <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
that maintains the connectivitiy of the graph while having lowest total
cost (as defined by the sum of the <a href="section-14.html#weight"
class="term"
title="A cost or distance most often associated with an edge in a graph.">weights</a>
of the edges in the MST). The result is referred to as a <a
href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
because it would never have a <a href="section-14.html#cycle"
class="term"
title="In graph terminology, a cycle is a path of length three or more that connects some vertex $v_1$ to itself.">cycle</a>
(since an edge could be removed from the cycle and still preserve
connectivity). Two algorithms to solve this problem are <a
href="section-14.html#prims-algorithm" class="term"
title="A greedy algorithm for computing the MST of a graph. It is nearly identical to Dijkstra&#39;s algorithm for solving the single-source shortest paths problem, with the only difference being the calculation done to update the best-known distance.">Prim’s
algorithm</a> and <a href="section-14.html#kruskals-algorithm"
class="term"
title="An algorithm for computing the MST of a graph. During processing, it makes use of the UNION/FIND process to efficiently determine of two vertices are within the same subgraph.">Kruskal’s
algorithm</a>.</p>
</dd>
<dt><dfn>mod</dfn></dt>
<dd>
<p>See <a href="section-14.html#modulus" class="term"
title="The modulus function returns the remainder of an integer division. Sometimes written $n \bmod m$ in mathematical expressions, the syntax in many programming languages is `n % m`.">modulus</a></p>
</dd>
<dt><dfn id="model">model</dfn></dt>
<dd>
<p>A simplification of reality that preserves only the essential
elements. With a model, we can more easily focus on and reason about
these essentials. In <a href="section-14.html#algorithm-analysis"
class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, we are especially concerned with the <a
href="section-14.html#cost-model" class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a> for measuring the cost of an algorithm.</p>
</dd>
<dt><dfn id="modulus">modulus</dfn> (<dfn id="mod">mod</dfn>)</dt>
<dd>
<p>The modulus function returns the remainder of an integer division.
Sometimes written
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mspace width="0.222em"></mspace><mrow><mi>mod</mi><mo>&#8289;</mo></mrow><mspace width="0.222em"></mspace><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">n \bmod m</annotation></semantics></math>
in mathematical expressions, the syntax in many programming languages is
<code>n % m</code>.</p>
</dd>
<dt><dfn id="monte-carlo-algorithms">Monte Carlo algorithms</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#randomised-algorithm" class="term"
title="An algorithm that involves some form of randomness to control its behaviour. The ultimate goal of a randomised algorithm is to improve performance over a deterministic algorithm to solve the same problem. There are a number of variations on this theme. A &#39;Las Vegas algorithm&#39; returns a correct result, but the amount of time required might or might not improve over a deterministic algorithm. A &#39;Monte Carlo algorithm&#39; is a form of probabilistic algorithm that is not guarenteed to return a correct result, but will return a result relatively quickly.">randomised
algorithm</a>. We find the maximum value fast, or we don’t get an answer
at all (but fast). While such algorithms have good running time, their
result is not guaranteed.</p>
</dd>
<dt><dfn id="move-to-front">move-to-front</dfn></dt>
<dd>
<p>A <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
used to maintain a <a href="section-14.html#self-organising-list"
class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>. Under this heuristic, whenever a record is accessed it is
moved to the front of the list. Analogous to the <a
href="section-14.html#least-recently-used" class="term"
title="Abbreviated LRU, it is a popular heuristic to use for deciding which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. Analogous to the move-to-front heuristic for maintaining a self-organising list.">least
recently used</a> heuristic for maintaining a <a
href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>.</p>
</dd>
<dt><dfn>MST</dfn></dt>
<dd>
<p>See <a href="section-14.html#minimum-spanning-tree" class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">minimum
spanning tree</a></p>
</dd>
<dt><dfn id="multi-dimensional-search-key">multi-dimensional search
key</dfn></dt>
<dd>
<p>A search key containing multiple parts, that works in conjunction
with a <a href="section-14.html#multi-dimensional-search-structure"
class="term"
title="A data structure used to support efficient search on a multi-dimensional search key. The main concept here is that a multi-dimensional search structure works more efficiently by considering the multiple parts of the search key as a whole, rather than making independent searches on each one-dimensional component of the key. A primary example is a spatial data structure that can efficiently represent and search for records in multi-dimensional space.">multi-dimensional
search structure</a>. Most typically, a <a
href="section-14.html#spatial" class="term"
title="Referring to a position in space.">spatial</a> search key
representing a position in multi-dimensional (2 or 3 dimensions) space.
But a multi-dimensional key could be used to organise data within
non-spatial dimensions, such as temperature and time.</p>
</dd>
<dt><dfn id="multi-dimensional-search-structure">multi-dimensional
search structure</dfn></dt>
<dd>
<p>A data structure used to support efficient search on a <a
href="section-14.html#multi-dimensional-search-key" class="term"
title="A search key containing multiple parts, that works in conjunction with a multi-dimensional search structure. Most typically, a spatial search key representing a position in multi-dimensional (2 or 3 dimensions) space. But a multi-dimensional key could be used to organise data within non-spatial dimensions, such as temperature and time.">multi-dimensional
search key</a>. The main concept here is that a multi-dimensional search
structure works more efficiently by considering the multiple parts of
the search key as a whole, rather than making independent searches on
each one-dimensional component of the key. A primary example is a <a
href="section-14.html#spatial-data-structure" class="term"
title="A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree.">spatial
data structure</a> that can efficiently represent and search for records
in multi-dimensional space.</p>
</dd>
<dt><dfn id="multilist">multilist</dfn></dt>
<dd>
<p>A list that may contain sublists. This term is sometimes used as a
synonym to the term <a href="section-14.html#bag" class="term"
title="In set notation, a bag is a collection of elements with no order (like a set), but which allows for duplicate-valued elements (unlike a set).">bag</a>.</p>
</dd>
<dt><dfn id="natural-numbers">natural numbers</dfn></dt>
<dd>
<p>Zero and the positive integers.</p>
</dd>
<dt><dfn id="necessary-fallacy">necessary fallacy</dfn></dt>
<dd>
<p>A common mistake in a <a href="section-14.html#lower-bounds-proof"
class="term"
title="A proof regarding the lower bound, with this term most typically referring to the lower bound for any possible algorithm to solve a given problem. Many problems have a simple lower bound based on the concept that the minimum amount of processing is related to looking at all of the problem&#39;s input. However, some problems have a higher lower bound than that. For example, the lower bound for the problem of sorting ($\Omega(n \log n)$) is greater than the input size to sorting ($n$). Proving such &#39;non-trivial&#39; lower bounds for problems is notoriously difficult.">lower
bounds proof</a> for a problem, where the proof makes an inappropriate
assumption that any algorithm must operate in some manner (typically in
the way that some known algorithm behaves).</p>
</dd>
<dt><dfn id="neighbour">neighbour</dfn></dt>
<dd>
<p>In a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>,
a <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>
is said to be a neighbour of <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
if there is an <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="node">node</dfn></dt>
<dd>
<p>The objects that make up a linked structure such as a linked list or
binary tree. Typically, nodes are allocated using <a
href="section-14.html#dynamic-memory-allocation" class="term"
title="A programming technique where linked objects in a data structure are created from free store as needed. When no longer needed, the object is either returned to free store or left as garbage, depending on the programming language.">dynamic
memory allocation</a>. In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, the nodes are more commonly called <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a>.</p>
</dd>
<dt><dfn id="non-deterministic-algorithm">non-deterministic
algorithm</dfn></dt>
<dd>
<p>An algorithm that may operate using a <a
href="section-14.html#non-deterministic-choice" class="term"
title="An operation that captures the concept of nondeterminism. A nondeterministic choice can be viewed as either &#39;correctly guessing&#39; between a set of choices, or implementing each of the choices in parallel. In the parallel view, the nondeterminism was successful if at least one of the choices leads to a correct answer.">non-deterministic
choice</a> operation.</p>
</dd>
<dt><dfn id="non-deterministic-choice">non-deterministic
choice</dfn></dt>
<dd>
<p>An operation that captures the concept of nondeterminism. A
nondeterministic choice can be viewed as either “correctly guessing”
between a set of choices, or implementing each of the choices in
parallel. In the parallel view, the nondeterminism was successful if at
least one of the choices leads to a correct answer.</p>
</dd>
<dt><dfn
id="non-deterministic-polynomial-time-algorithm">non-deterministic
polynomial time algorithm</dfn> (<dfn id="np">NP</dfn>)</dt>
<dd>
<p>An algorithm that runs in polynomial time, and which may (or might
not) use <a href="section-14.html#non-deterministic-choice" class="term"
title="An operation that captures the concept of nondeterminism. A nondeterministic choice can be viewed as either &#39;correctly guessing&#39; between a set of choices, or implementing each of the choices in parallel. In the parallel view, the nondeterminism was successful if at least one of the choices leads to a correct answer.">non-deterministic
choice</a>.</p>
</dd>
<dt><dfn id="non-deterministic">non-deterministic</dfn></dt>
<dd>
<p>In a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a>, at least one <a href="section-14.html#state" class="term"
title="The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state.">state</a>
has multiple transitions on at least one symbol. This means that it is
not <a href="section-14.html#deterministic" class="term"
title="Any finite automata in which, for every pair of state and symbol, there is only a single transition. This means that whenever the machine is in a given state and sees a given symbol, only a single thing can happen. This is in contrast to a non-deterministic finite automata, which has at least one state with multiple transitions on at least one symbol.">deterministic</a>
about what transition to take in that situation. A non-deterministic
machine is said to <a href="section-14.html#accept" class="term"
title="When a finite automata executes on a string and terminates in an accepting state, it is said to accept the string. The finite automata is said to accept the language that consists of all strings for which the finite automata completes execution in an accepting state.">accept</a>
a string if it completes execution on the string in an <a
href="section-14.html#accepting-state" class="term"
title="Part of the definition of a finite automata is to designate some states as accepting states. If the finite automata executes on an input string and completes the computation in an accepting state, then the machine is said to accept the string.">accepting
state</a> under at least one choice of non-deterministic transitions.
Generally, non-determinism can be simulated with a deterministic machine
by alternating between the execution that would take place under each of
the branching choices.</p>
</dd>
<dt><dfn id="non-strict-partial-order">non-strict partial
order</dfn></dt>
<dd>
<p>In set notation, a relation that is <a
href="section-14.html#reflexive" class="term"
title="In set notation, binary relation $R$ on set $S$ is reflexive if $aRa$ for all $a \in \mathbf{S}$.">reflexive</a>,
<a href="section-14.html#antisymmetric" class="term"
title="In set notation, relation $R$ is antisymmetric if whenever $aRb$ and $bRa$, then $a = b$, for all $a, b \in \mathbf{S}$.">antisymmetric</a>,
and <a href="section-14.html#transitive" class="term"
title="In set notation, relation $R$ is transitive if whenever $aRb$ and $bRc$, then $aRc$, for all $a, b, c \in \mathbf{S}$.">transitive</a>.</p>
</dd>
<dt><dfn id="non-terminal">non-terminal</dfn></dt>
<dd>
<p>In contrast to a <a href="section-14.html#terminal" class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminal</a>,
a non-terminal is an abstract state in a <a
href="section-14.html#production-rule" class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production
rule</a>. Begining with the <a href="section-14.html#start-symbol"
class="term"
title="In a grammar, the designated non-terminal that is the intial point for deriving a string in the langauge.">start
symbol</a>, all non-terminals must be converted into terminals in order
to complete a <a href="section-14.html#derivation" class="term"
title="In formal languages, the process of executing a series of production rules from a grammar. A typical example of a derivation would be the series of productions executed to go from the start symbol to a given string.">derivation</a>.</p>
</dd>
<dt><dfn id="np-complete">NP-Complete</dfn></dt>
<dd>
<p>A class of problems that are related to each other in this way: If
ever one such problem is proved to be solvable in polynomial time, or
proved to require exponential time, then all other NP-Complete problems
will cost likewise. Since so many real-world problems have been proved
to be NP-Complete, it would be extremely useful to determine if they
have polynomial or exponential cost. But so far, nobody has been able to
determine the truth of the situation. A more technical definition is
that a problem is NP-Complete if it is in NP and is NP-hard.</p>
</dd>
<dt><dfn id="np-completeness-proof">NP-Completeness proof</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#reduction" class="term"
title="In algorithm analysis, the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another. In particular, if problem A can be used to solve problem B, and problem A is proved to be in $O(f(n))$, then problem B must also be in $O(f(n))$. Reductions are often used to show that certain problems are at least as expensive as sorting, or that certain problems are NP-Complete.">reduction</a>
used to demonstrate that a particular <a href="section-14.html#problem"
class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
is <a href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-complete</a>.
Specifically, an NP-Completeness proof must first show that the problem
is in class <a href="section-14.html#np" class="term"
title="An algorithm that runs in polynomial time, and which may (or might not) use non-deterministic choice.">NP</a>,
and then show (by using a reduction to another NP-Complete problem) that
the problem is <a href="section-14.html#np-hard" class="term"
title="A problem that is &#39;as hard as&#39; any other problem in NP. That is, problem X is NP-hard if any algorithm in NP can be reduced to X in polynomial time.">NP-hard</a>.</p>
</dd>
<dt><dfn id="np-hard">NP-hard</dfn></dt>
<dd>
<p>A problem that is “as hard as” any other problem in <a
href="section-14.html#np" class="term"
title="An algorithm that runs in polynomial time, and which may (or might not) use non-deterministic choice.">NP</a>.
That is, problem X is NP-hard if any algorithm in NP can be <a
href="section-14.html#reduction" class="term"
title="In algorithm analysis, the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another. In particular, if problem A can be used to solve problem B, and problem A is proved to be in $O(f(n))$, then problem B must also be in $O(f(n))$. Reductions are often used to show that certain problems are at least as expensive as sorting, or that certain problems are NP-Complete.">reduced</a>
to X in polynomial time.</p>
</dd>
<dt><dfn>NP</dfn></dt>
<dd>
<p>See <a
href="section-14.html#non-deterministic-polynomial-time-algorithm"
class="term"
title="An algorithm that runs in polynomial time, and which may (or might not) use non-deterministic choice.">non-deterministic
polynomial time algorithm</a></p>
</dd>
<dt><dfn id="nth-roots-of-unity">nth roots of unity</dfn></dt>
<dd>
<p>All of the points along the unit circle in the complex plane that
represent multiples of the <a
href="section-14.html#primitive-nth-root-of-unity" class="term"
title="The $n$ th root of 1. Normally a complex number. An intuitive way to view this is one $n$ th of the unit circle in the complex plain.">primitive
nth root of unity</a>.</p>
</dd>
<dt><dfn id="object-oriented-programming-paradigm">object-oriented
programming paradigm</dfn></dt>
<dd>
<p>An approach to problem-solving where all computations are carried out
using <a href="section-14.html#object" class="term"
title="An instance of a class, that is, something that is created and takes up storage during the execution of a computer program. In the object-oriented programming paradigm, objects are the basic units of operation. Objects have state in the form of data members, and they know how to perform certain actions (methods).">objects</a>.</p>
</dd>
<dt><dfn id="object-space-decomposition">object-space
decomposition</dfn></dt>
<dd>
<p>A from of <a href="section-14.html#key-space-decomposition"
class="term"
title="The idea that the range for a search key will be split into pieces. There are two general approaches to this: object-space decomposition and image-space decomposition.">key-space
decomposition</a> where the <a href="section-14.html#key-space"
class="term"
title="The range of values that a key value may take on.">key space</a>
is determined by the actual values of keys that are found. For example,
a <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>
stores a key value in its root, and all other values in the tree with
lesser value are in the left <a href="section-14.html#subtree"
class="term"
title="A subtree is a subset of the nodes of a binary tree that includes some node $R$ of the tree as the subtree root along with all the descendants of $R$.">subtree</a>.
Thus, the root value has split (or decomposed) the key space for that
key based on its value into left and right parts. An object-space
decomposition is in opposition to an <a
href="section-14.html#image-space-decomposition" class="term"
title="A from of key-space decomposition where the key space splitting points is predetermined (typically by splitting in half). For example, a Huffman coding tree splits the letters being coded into those with codes that start with 0 on the left side, and those with codes that start with 1 on the right side. This regular decomposition of the key space is the basis for a trie data structure. An image-space decomposition is in opposition to an object-space decomposition.">image-space
decomposition</a>.</p>
</dd>
<dt><dfn id="object">object</dfn></dt>
<dd>
<p>An instance of a <a href="section-14.html#class" class="term"
title="In the object-oriented programming paradigm an ADT and its implementation together make up a class. An instantiation of a class within a program is termed an object.">class</a>,
that is, something that is created and takes up storage during the
execution of a computer program. In the <a
href="section-14.html#object-oriented-programming-paradigm" class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming paradigm</a>, objects are the basic units of operation.
Objects have state in the form of <a href="section-14.html#data-member"
class="term"
title="In object-oriented programming, the variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include *data field*, *attribute*, and *instance variable*.">data
members</a>, and they know how to perform certain actions (<a
href="section-14.html#method" class="term"
title="In the object-oriented programming paradigm, a method is an operation on a class. A synonym for member function.">methods</a>).</p>
</dd>
<dt><dfn id="octree">octree</dfn></dt>
<dd>
<p>The three-dimensional equivalent of the <a
href="section-14.html#quadtree" class="term"
title="A full tree where each internal node has four children. Most typically used to store two dimensional spatial data. Related to the bintree. The difference is that the quadtree splits all dimensions simultaneously, while the bintree splits one dimension at each level. Thus, to extend the quadtree concept to more dimensions requires a rapid increase in the number of splits (for example, 8 in three dimensions).">quadtree</a>
would be a tree with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mn>3</mn></msup><annotation encoding="application/x-tex">2^3</annotation></semantics></math>
or eight branches.</p>
</dd>
<dt><dfn id="omega-notation">Omega notation</dfn></dt>
<dd>
<p>For <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rates</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>,
we write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f \in \Omega(g)</annotation></semantics></math>
to say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
is a <a href="section-14.html#lower-bound" class="term"
title="An lower bound for a growth rate $f$ is any growth rate $g$ that is less than or equal to it. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \geq c g(n)$ for all $n \geq n_0$. We also write $f \in \Omega(g)$ or slightly imprecisely $f(n) \in \Omega(g(n))$ (this is Omega notation). Usually, we are interested in finding a lower bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, a lower bound for an algorithm is a lower bound for the asymptotic complexity of the algorithm, the growth rate of its complexity.">lower
bound</a> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.
The notation can be made sense of by defining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(g)</annotation></semantics></math>
as the set of functions with growth rate greater than or equal to that
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>.
The notation is often somewhat imprecisely used as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>∈</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \in \Omega(g(n))</annotation></semantics></math>
or even
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) = \Omega(g(n))</annotation></semantics></math>.</p>
</dd>
<dt><dfn>one-way list</dfn></dt>
<dd>
<p>See <a href="section-14.html#singly-linked-list" class="term"
title="A linked list implementation variant where each list node contains access a pointer only to the next element in the list.">singly
linked list</a></p>
</dd>
<dt><dfn id="open-addressing">open addressing</dfn> (<dfn
id="closed-hash-system">closed hash system</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#hash-system" class="term"
title="The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key.">hash
system</a> where all records are stored in slots of the <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. This is in contrast to an <a
href="section-14.html#open-hash-system" class="term"
title="A hash system where multiple records might be associated with the same slot of a hash table. Typically this is done using a linked list to store the records. This is in contrast to a closed hash system.">open
hash system</a>.</p>
</dd>
<dt><dfn id="open-hash-system">open hash system</dfn></dt>
<dd>
<p>A <a href="section-14.html#hash-system" class="term"
title="The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key.">hash
system</a> where multiple records might be associated with the same slot
of a <a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. Typically this is done using a linked list to store the
records. This is in contrast to a <a
href="section-14.html#closed-hash-system" class="term"
title="A hash system where all records are stored in slots of the hash table. This is in contrast to an open hash system.">closed
hash system</a>.</p>
</dd>
<dt><dfn id="operating-system">operating system</dfn></dt>
<dd>
<p>The control program for a computer. Its purpose is to control
hardware, manage resources, and present a standard interface to these to
other software components.</p>
</dd>
<dt><dfn id="optimal-static-ordering">optimal static ordering</dfn></dt>
<dd>
<p>A theoretical construct defining the best static (non-changing) order
in which to place a collection of records so as to minimise the number
of records <a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visited</a>
by a series of sequential searches. It is a useful concept for the
purpose of defining a theoretical optimum against which to compare the
performance for a <a
href="section-14.html#self-organising-list-heuristic" class="term"
title="A heuristic to use for the purpose of maintaining a self-organising list. Commonly used heuristics include move-to-front and transpose.">self-organising
list heuristic</a>.</p>
</dd>
<dt><dfn id="optimisation-problem">optimisation problem</dfn></dt>
<dd>
<p>Any problem where there are a (typically large) collection of
potential solutions, and the goal is to find the best solution. An
example is the <em>traveling salesman problem</em>, where visiting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
cities in some order has a cost, and the goal is to visit in the
cheapest order.</p>
</dd>
<dt><dfn>order of growth</dfn></dt>
<dd>
<p>See <a href="section-14.html#growth-rate" class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a></p>
</dd>
<dt><dfn id="out-degree">out degree</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, the out degree for a <a href="section-14.html#vertex"
class="term" title="Another name for a node in a graph.">vertex</a> is
the number of edges directed out of the vertex.</p>
</dd>
<dt><dfn id="overflow-bucket">overflow bucket</dfn></dt>
<dd>
<p>In <a href="section-14.html#bucket-hashing" class="term"
title="A method of hashing where multiple slots of the hash table are grouped together to form a bucket. The hash function then either hashes to some bucket, or else it hashes to a home slot in the normal way, but this home slot is part of some bucket. Collision resolution is handled first by attempting to find a free position within the same bucket as the home slot. If the bucket if full, then the record is placed in an overflow bucket.">bucket
hashing</a>, this is the <a href="section-14.html#bucket" class="term"
title="In bucket hashing, a bucket is a sequence of slots in the hash table that are grouped together.">bucket</a>
into which a record is placed if the bucket containing the record’s <a
href="section-14.html#home-slot" class="term"
title="In hashing, this is the slot in the hash table determined for a given key by the hash function.">home
slot</a> is full. The overflow bucket is logically considered to have
infinite capacity, though in practice search and insert will become
relatively expensive if many records are stored in the overflow
bucket.</p>
</dd>
<dt><dfn id="overflow">overflow</dfn></dt>
<dd>
<p>The condition where the amount of data stored in an entity has
exceeded its capacity. For example, a node in a <a
href="section-14.html#b-tree" class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>
can store a certain number of records. If a record is attempted to be
inserted into a node that is full, then something has to be done to
handle this case.</p>
</dd>
<dt><dfn id="overhead">overhead</dfn></dt>
<dd>
<p>All information stored by a data structure aside from the actual
data. For example, the pointer fields in a <a
href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> or <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>,
or the unused positions in an <a href="section-14.html#array-based-list"
class="term"
title="An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused.">array-based
list</a>.</p>
</dd>
<dt><dfn id="page">page</dfn></dt>
<dd>
<p>A term often used to refer to the contents of a single <a
href="section-14.html#buffer" class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
within a <a href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a> or other <a href="section-14.html#virtual-memory" class="term"
title="A memory management technique for making relatively fast but small memory appear larger to the program. The large &#39;virtual&#39; data space is actually stored on a relatively slow but large backing storage device, and portions of the data are copied into the smaller, faster memory as needed by use of a buffer pool. A common example is to use RAM to manage access to a large virtual space that is actually stored on a disk drive. The programmer can implement a program as though the entire data content were stored in RAM, even if that is larger than the physical RAM available making it easier to implement.">virtual
memory</a>. This corresponds to a single <a href="section-14.html#block"
class="term"
title="A unit of storage, usually referring to storage on a disk drive or other peripheral storage device. A block is the basic unit of I/O for that device.">block</a>
or <a href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>
of data from <a href="section-14.html#backing-storage" class="term"
title="In the context of a caching system or buffer pool, backing storage is the relatively large but slower source of data that needs to be cached. For example, in a virtual memory, the disk drive would be the backing storage. In the context of a web browser, the Internet might be considered the backing storage.">backing
storage</a>, which is the fundamental unit of I/O.</p>
</dd>
<dt><dfn id="parameter">parameter</dfn></dt>
<dd>
<p>The values making up an input to a <a href="section-14.html#function"
class="term"
title="In mathematics, a matching between inputs (the domain) and outputs (the range). In programming, a subroutine that takes input parameters and uses them to compute and return a value. In this case, it is usually considered bad practice for a function to change any global variables (doing so is called a side effect).">function</a>.</p>
</dd>
<dt><dfn id="parent-pointer-representation">parent pointer
representation</dfn></dt>
<dd>
<p>For <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">trees</a>,
a <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
implementation where each node stores only a pointer to its <a
href="section-14.html#parent" class="term"
title="In a tree, the node $P$ that directly links to a node $A$ is the parent of $A$. $A$ is the child of $P$.">parent</a>,
rather than to its <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>.
This makes it easy to go up the tree toward the <a
href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>,
but not down the tree toward the <a href="section-14.html#leaf-node"
class="term"
title="In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children.">leaves</a>.
This is most appropriate for solving the <a
href="section-14.html#union-find" class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
problem.</p>
</dd>
<dt><dfn id="parent">parent</dfn></dt>
<dd>
<p>In a tree, the <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
that directly links to a node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the parent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is the <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">child</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="parity-bit">parity bit</dfn></dt>
<dd>
<p>A common method for checking if transmission of a sequence of bits
has been performed correctly. The idea is to count the number of 1 bits
in the sequence, and set the parity bit to 1 if this number is odd, and
0 if it is even. Then, the transmitted sequence of bits can be checked
to see if its parity matches the value of the parity bit. This will
catch certain types of errors, in particular if the value for a single
bit has been reversed. This was used, for example, in early versions of
<a href="section-14.html#ascii-character-coding" class="term"
title="*American Standard Code for Information Interchange*. A commonly used method for encoding characters using a binary code. Standard ASCII uses an 8-bit code to represent upper and lower case letters, digits, some punctuation, and some number of non-printing characters (such as carrage return). Now largely replaced by UTF-8 encoding.">ASCII
character coding</a>.</p>
</dd>
<dt><dfn id="parity">parity</dfn></dt>
<dd>
<p>The concept of matching even-ness or odd-ness, the basic idea behind
using a <a href="section-14.html#parity-bit" class="term"
title="A common method for checking if transmission of a sequence of bits has been performed correctly. The idea is to count the number of 1 bits in the sequence, and set the parity bit to 1 if this number is odd, and 0 if it is even. Then, the transmitted sequence of bits can be checked to see if its parity matches the value of the parity bit. This will catch certain types of errors, in particular if the value for a single bit has been reversed. This was used, for example, in early versions of ASCII character coding.">parity
bit</a> for error detection.</p>
</dd>
<dt><dfn id="parse-tree">parse tree</dfn></dt>
<dd>
<p>A tree that represents the syntactic structure of an input string,
making it easy to compare against a <a href="section-14.html#grammar"
class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>
to see if it is syntactically correct.</p>
</dd>
<dt><dfn id="parser">parser</dfn></dt>
<dd>
<p>A part of a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that takes as input the program text (or more typically, the tokens from
the <a href="section-14.html#scanner" class="term"
title="The part of a compiler that is responsible for doing lexical analysis.">scanner</a>),
and verifies that the program is syntactically correct. Typically it
will build a <a href="section-14.html#parse-tree" class="term"
title="A tree that represents the syntactic structure of an input string, making it easy to compare against a grammar to see if it is syntactically correct.">parse
tree</a> as part of the process.</p>
</dd>
<dt><dfn id="partial-order">partial order</dfn></dt>
<dd>
<p>In set notation, a binary relation is called a partial order if it is
<a href="section-14.html#antisymmetric" class="term"
title="In set notation, relation $R$ is antisymmetric if whenever $aRb$ and $bRa$, then $a = b$, for all $a, b \in \mathbf{S}$.">antisymmetric</a>
and <a href="section-14.html#transitive" class="term"
title="In set notation, relation $R$ is transitive if whenever $aRb$ and $bRc$, then $aRc$, for all $a, b, c \in \mathbf{S}$.">transitive</a>.
If the relation is also <a href="section-14.html#reflexive" class="term"
title="In set notation, binary relation $R$ on set $S$ is reflexive if $aRa$ for all $a \in \mathbf{S}$.">reflexive</a>,
then it is a <a href="section-14.html#non-strict-partial-order"
class="term"
title="In set notation, a relation that is reflexive, antisymmetric, and transitive.">non-strict
partial order</a>. Alternatively, if the relation is also <a
href="section-14.html#irreflexive" class="term"
title="In set notation, binary relation $R$ on set $S$ is irreflexive if $aRa$ is never in the relation for any $a \in \mathbf{S}$.">irreflexive</a>,
then it is a <a href="section-14.html#strict-partial-order" class="term"
title="In set notation, a relation that is irreflexive, antisymmetric, and transitive.">strict
partial order</a>.</p>
</dd>
<dt><dfn id="partially-ordered-set">partially ordered set</dfn></dt>
<dd>
<p>The set on which a <a href="section-14.html#partial-order"
class="term"
title="In set notation, a binary relation is called a partial order if it is antisymmetric and transitive. If the relation is also reflexive, then it is a non-strict partial order. Alternatively, if the relation is also irreflexive, then it is a strict partial order.">partial
order</a> is defined is called a partially ordered set.</p>
</dd>
<dt><dfn id="partition">partition</dfn></dt>
<dd>
<p>In <a href="section-14.html#quicksort" class="term"
title="A sort that is $O(n \log n)$ in the best and average cases, though $O(n^2)$ in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts.">Quicksort</a>,
the process of splitting a list into two sublists, such that one sublist
has values less than the <a href="section-14.html#pivot" class="term"
title="In Quicksort, the value that is used to split the list into sublists, one with lesser values than the pivot, the other with greater values than the pivot.">pivot</a>
value, and the other with values greater than the pivot. This process
takes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(i)</annotation></semantics></math>
time on a sublist of length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="pass-by-reference">pass by reference</dfn></dt>
<dd>
<p>A <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to the variable is passed to the called function. So, any modifications
will affect the original variable.</p>
</dd>
<dt><dfn id="pass-by-value">pass by value</dfn></dt>
<dd>
<p>A copy of a variable is passed to the called function. So, any
modifications will not affect the original variable.</p>
</dd>
<dt><dfn id="path-compression">path compression</dfn></dt>
<dd>
<p>When implementing the <a href="section-14.html#union-find"
class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
algorithm, path compression is a local optimisation step that can be
performed during the FIND step. Once the root of the tree for the
current object has been found, the path to the root can be traced a
second time, with all objects in the tree made to point directly to the
root. This reduces the depth of the tree from typically
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>
to nearly constant.</p>
</dd>
<dt><dfn id="path">path</dfn></dt>
<dd>
<p>In <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
or <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a sequence of <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>v</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">v_1, v_2, ..., v_n</annotation></semantics></math>
forms a path of length
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>
if there exist edges from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mi>i</mi></msub><annotation encoding="application/x-tex">v_i</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">v_{i+1}</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">1 \leq i &lt; n</annotation></semantics></math>.</p>
</dd>
<dt><dfn>PDA</dfn></dt>
<dd>
<p>See <a href="section-14.html#pushdown-automata" class="term"
title="A type of finite state automata that adds a stack memory to the basic deterministic finite automata machine. This extends the set of languages that can be recognise to the context-free languages.">pushdown
automata</a></p>
</dd>
<dt><dfn id="peripheral-storage">peripheral storage</dfn></dt>
<dd>
<p>Any storage device that is not part of the core processing of the
computer (that is, <a href="section-14.html#ram" class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>).
A typical example is a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>.</p>
</dd>
<dt><dfn id="permutation">permutation</dfn></dt>
<dd>
<p>A permutation of a sequence
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
is the <a href="section-14.html#element" class="term"
title="One value or member in a set.">elements</a> of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
arranged in some order.</p>
</dd>
<dt><dfn id="persistent">persistent</dfn></dt>
<dd>
<p>In the context of computer memory, this refers to a memory that does
not lose its stored information when the power is turned off.</p>
</dd>
<dt><dfn id="physical-file">physical file</dfn></dt>
<dd>
<p>The collection of sectors that comprise a file on a <a
href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. This is in contrast to the <a
href="section-14.html#logical-file" class="term"
title="In file processing, the programmer&#39;s view of a random access file stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. This is in contrast to the physical file.">logical
file</a>.</p>
</dd>
<dt><dfn id="physical-form">physical form</dfn></dt>
<dd>
<p>The implementation of a data type as a data structure. Contrast to
the <a href="section-14.html#logical-form" class="term"
title="The definition for a data type in terms of an ADT. Contrast to the physical form for the data type.">logical
form</a> for the data type.</p>
</dd>
<dt><dfn id="pigeonhole-principle">pigeonhole principle</dfn></dt>
<dd>
<p>A commonly used lemma in mathematics. A typical variant states: When
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n+1</annotation></semantics></math>
objects are stored in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
locations, at least one of the locations must store two or more of the
objects.</p>
</dd>
<dt><dfn id="pivot">pivot</dfn></dt>
<dd>
<p>In <a href="section-14.html#quicksort" class="term"
title="A sort that is $O(n \log n)$ in the best and average cases, though $O(n^2)$ in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts.">Quicksort</a>,
the value that is used to split the list into sublists, one with lesser
values than the pivot, the other with greater values than the pivot.</p>
</dd>
<dt><dfn id="platter">platter</dfn></dt>
<dd>
<p>In a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, one of a series of flat disks that comprise the storage space
for the drive. Typically, each surface (top and bottom) of each platter
stores data, and each surface has its own <a
href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a>.</p>
</dd>
<dt><dfn id="point-quadtree">point quadtree</dfn></dt>
<dd>
<p>A <a href="section-14.html#spatial-data-structure" class="term"
title="A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree.">spatial
data structure</a> for storing point data. It is similar to a <a
href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a> in that it (in two dimensions) splits the world into four
parts. However, it splits using an <a
href="section-14.html#object-space-decomposition" class="term"
title="A from of key-space decomposition where the key space is determined by the actual values of keys that are found. For example, a BST stores a key value in its root, and all other values in the tree with lesser value are in the left subtree. Thus, the root value has split (or decomposed) the key space for that key based on its value into left and right parts. An object-space decomposition is in opposition to an image-space decomposition.">object-space
decomposition</a>. That is, quadrant containing the point is split into
four parts at the point. It is similar to the <a
href="section-14.html#kd-tree" class="term"
title="A spatial data structure that uses a binary tree to store a collection of data records based on their (point) location in space. It uses the concept of a discriminator at each level to decide which single component of the multi-dimensional search key to branch on at that level. It uses a key-space decomposition, meaning that all data records in the left subtree of a node have a value on the corresponding discriminator that is less than that of the node, while all data records in the right subtree have a greater value. The bintree is the image-space decomposition analog of the kd tree.">kd
tree</a> which splits alternately in each dimension, except that it
splits in all dimensions at once.</p>
</dd>
<dt><dfn id="point-region-quadtree">point-region quadtree</dfn></dt>
<dd>
<p>Formal name for what is commonly referred to as a <a
href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a>.</p>
</dd>
<dt><dfn id="pointee">pointee</dfn></dt>
<dd>
<p>The term pointee refers to anything that is pointed to by a <a
href="section-14.html#pointer" class="term"
title="A variable whose value is the address of another variable; a link.">pointer</a>
or <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>.</p>
</dd>
<dt><dfn
id="pointer-based-implementation-for-binary-tree-nodes">pointer-based
implementation for binary tree nodes</dfn></dt>
<dd>
<p>A common way to implement <a href="section-14.html#binary-tree"
class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a> <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>.
Each node stores a data value (or a <a href="section-14.html#reference"
class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to a data value), and pointers to the left and right children. If either
or both of the children does not exist, then a null pointer is
stored.</p>
</dd>
<dt><dfn id="pointer">pointer</dfn></dt>
<dd>
<p>A variable whose value is the <a href="section-14.html#address"
class="term" title="A location in memory.">address</a> of another
variable; a link.</p>
</dd>
<dt><dfn id="polymorphism">polymorphism</dfn></dt>
<dd>
<p>An <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a> term meaning <em>one name, many forms</em>. It describes
the ability of software to change its behaviour dynamically. Two basic
forms exist: <a href="section-14.html#run-time-polymorphism"
class="term"
title="A form of polymorphism known as Overriding. Overridden methods are those which implement a new method with the same signature as a method inherited from its base class. Compare to compile-time polymorphism.">run-time
polymorphism</a> and <a href="section-14.html#compile-time-polymorphism"
class="term"
title="A form of polymorphism known as Overloading. Overloaded methods have the same names, but different signatures as a method available elsewhere in the class. Compare to run-time polymorphism.">compile-time
polymorphism</a>.</p>
</dd>
<dt><dfn id="pop">pop</dfn></dt>
<dd>
<p>A specialised term used to indicate removing an <a
href="section-14.html#element" class="term"
title="One value or member in a set.">element</a> from a <a
href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>.</p>
</dd>
<dt><dfn id="poset">poset</dfn></dt>
<dd>
<p>Another name for a <a href="section-14.html#partially-ordered-set"
class="term"
title="The set on which a partial order is defined is called a partially ordered set.">partially
ordered set</a>.</p>
</dd>
<dt><dfn id="position">position</dfn></dt>
<dd>
<p>The defining property of the list ADT, this is the concept that list
elements are in a position. Many list ADTs support access by
position.</p>
</dd>
<dt><dfn id="postorder-traversal">postorder traversal</dfn></dt>
<dd>
<p>In a <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a>, a <a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
that first <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursively</a>
<a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visits</a>
the left <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">child</a>,
then recursively visits the right child, and then visits the <a
href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>.</p>
</dd>
<dt><dfn id="potential">potential</dfn></dt>
<dd>
<p>A concept in <a href="section-14.html#amortised-complexity"
class="term"
title="A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortised cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials.">amortised
complexity</a> for operations on a data structure. We choose a
<em>potential function</em> that associates an arbitrary non-negative
value of <em>stored cost</em> (stored energy) with each state of the
data structure. We then define the <a
href="section-14.html#amortised-cost" class="term"
title="The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortised analysis, gives rise to the notion of amortised complexity.">amortised
cost</a> of a run of the operation to be its cost as given by the the <a
href="section-14.html#cost-model" class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a> plus the change in potential. The <a
href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
modified this way is called <a
href="section-14.html#amortised-complexity" class="term"
title="A modification to the notion of complexity for operations on a data structure where, for each fixed input size, one does not just look at the cost of a single run of the operation, but its amortised cost over sufficiently long series of operations of the same kind. This can be made precise without considering averages by introducing potentials.">amortised
complexity</a>.</p>
<p>An example is adding an element to a dynamic array. When the dynamic
array is not full, adding an element is quick and we store some of that
saved cost by increasing the potential. When the dynamic array is full
capacity, we perform an expensive reallocation, but compensate that cost
by resetting the potential from a high value to zero. Let us define the
potential of a dynamic array with capacity
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
and size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>n</mi><mo>−</mo><mi>c</mi><mo>,</mo><mn>0</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">max(2n-c,0)</annotation></semantics></math>.
Assuming we double the capacity on reallocation, the operation of adding
an element then has constant amortised complexity.</p>
<p>The concept comes from potential energy in physics. For example, in
the graviational field of the earth, kinetic energy may be stored as
potential energy.</p>
</dd>
<dt><dfn id="powerset">powerset</dfn></dt>
<dd>
<p>For a <a href="section-14.html#set" class="term"
title="A collection of distinguishable members or elements.">set</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>,
the power set is the set of all possible <a
href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subsets</a>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="pr-quadtree">PR quadtree</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#quadtree" class="term"
title="A full tree where each internal node has four children. Most typically used to store two dimensional spatial data. Related to the bintree. The difference is that the quadtree splits all dimensions simultaneously, while the bintree splits one dimension at each level. Thus, to extend the quadtree concept to more dimensions requires a rapid increase in the number of splits (for example, 8 in three dimensions).">quadtree</a>
that stores point data in two dimensions. The root of the PR quadtree
represents some square region of 2d space. If that space stores more
than one data point, then the region is decomposed into four equal
subquadrants, each represented <a href="section-14.html#recursion"
class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursively</a>
by a subtree of the PR quadtree. Since many leaf nodes of the PR
quadtree will contain no data points, implementation often makes use of
the <a href="section-14.html#flyweight" class="term"
title="A design pattern that is meant to solve the following problem: You have an application with many objects. Some of these objects are identical in the information that they contain, and the role that they play. But they must be reached from various places, and conceptually they really are distinct objects. Because there is so much duplication of the same information, we want to reduce memory cost by sharing that space. For example, in document layout, the letter &#39;C&#39; might be represented by an object that describes that character&#39;s strokes and bounding box. However, we do not want to create a separate &#39;C&#39; object everywhere in the document that a &#39;C&#39; appears. The solution is to allocate a single copy of the shared representation for &#39;C&#39; objects. Then, every place in the document that needs a &#39;C&#39; in a given font, size, and typeface will reference this single copy. The various instances of references to a specific form of &#39;C&#39; are called flyweights. Flyweights can also be used to implement the empty leaf nodes of the bintree and PR quadtree.">Flyweight</a>
<a href="section-14.html#design-pattern" class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a>. Related to the <a href="section-14.html#bintree"
class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>.</p>
</dd>
<dt><dfn id="prefix-property">prefix property</dfn></dt>
<dd>
<p>Given a collection of strings, the collection has the prefix property
if no string in the collection is a prefix for another string in the
collection. The significance is that, given a long string composed of
members of the collection, it can be uniquely decomposed into the
constituent members. An example of such a collection of strings with the
prefix property is a set of <a href="section-14.html#huffman-codes"
class="term"
title="The codes given to a collection of letters (or other symbols) through the process of Huffman coding. Huffman coding uses a Huffman coding tree to generate the codes. The codes can be of variable length, such that the letters which are expected to appear most frequently are shorter. Huffman coding is optimal whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message.">Huffman
codes</a>.</p>
</dd>
<dt><dfn id="preorder-traversal">preorder traversal</dfn></dt>
<dd>
<p>In a <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
tree</a>, a <a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
that first <a href="section-14.html#visit" class="term"
title="During the process of a traversal on a graph or tree the action that takes place on each node.">visits</a>
the <a href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>,
then <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursively</a>
visits the left <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">child</a>,
then recursively visits the right child.</p>
</dd>
<dt><dfn id="prims-algorithm">Prim’s algorithm</dfn></dt>
<dd>
<p>A <a href="section-14.html#greedy-algorithm" class="term"
title="An algorithm that makes locally optimal choices at each step.">greedy
algorithm</a> for computing the <a href="section-14.html#mst"
class="term"
title="Abbreviated as MST, or sometimes as MCST. Derived from a weighted graph, the MST is the subset of the graph&#39;s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim&#39;s algorithm and Kruskal&#39;s algorithm.">MST</a>
of a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.
It is nearly identical to <a href="section-14.html#dijkstras-algorithm"
class="term"
title="An algorithm to solve the single-source shortest paths problem in a graph. This is a greedy algorithm. It is nearly identical to Prim&#39;s algorithm for finding a minimum spanning tree, with the only difference being the calculation done to update the best-known distance.">Dijkstra’s
algorithm</a> for solving the <a
href="section-14.html#single-source-shortest-paths-problem" class="term"
title="Given a graph with weights or distances on the edges, and a designated start vertex $s$, find the shortest path from $s$ to every other vertex in the graph. One algorithm to solve this problem is Dijkstra&#39;s algorithm.">single-source
shortest paths problem</a>, with the only difference being the
calculation done to update the best-known distance.</p>
</dd>
<dt><dfn id="primary-clustering">primary clustering</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
the tendency in certain <a href="section-14.html#collision-resolution"
class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> methods to create clustering in sections of the hash
table. The classic example is <a href="section-14.html#linear-probing"
class="term"
title="In hashing, this is the simplest collision resolution method. Term $i$ of the probe sequence is simply $i$, meaning that collision resolution works by moving sequentially through the hash table from the home slot. While simple, it is also inefficient, since it quickly leads to certain free slots in the hash table having higher probability of being selected during insertion or search.">linear
probing</a>. This tends to happen when a group of keys follow the same
<a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> during collision resolution.</p>
</dd>
<dt><dfn>primary index</dfn></dt>
<dd>
<p>See <a href="section-14.html#primary-key-index" class="term"
title="Relates each primary key value with a pointer to the actual record on disk.">primary
key index</a></p>
</dd>
<dt><dfn id="primary-key-index">primary key index</dfn> (<dfn
id="primary-index">primary index</dfn>)</dt>
<dd>
<p>Relates each <a href="section-14.html#primary-key" class="term"
title="A unique identifier for a record.">primary key</a> value with a
pointer to the actual record on disk.</p>
</dd>
<dt><dfn id="primary-key">primary key</dfn></dt>
<dd>
<p>A unique identifier for a <a href="section-14.html#record"
class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">record</a>.</p>
</dd>
<dt><dfn id="primary-storage">primary storage</dfn> (<dfn
id="main-memory">main memory</dfn>)</dt>
<dd>
<p>The faster but more expensive memory in a computer, most often <a
href="section-14.html#ram" class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>
in modern computers. This is in contrast to <a
href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a>, which together with primary storage devices make up the
computer’s <a href="section-14.html#memory-hierarchy" class="term"
title="The concept that a computer system stores data in a range of storage types that range from fast but expensive (primary storage) to slow but cheap (secondary storage). When there is too much data to store in primary storage, the goal is to have the data that is needed soon or most often in the primary storage as much as possible, by using caching techniques.">memory
hierarchy</a>.</p>
</dd>
<dt><dfn id="primitive-data-type">primitive data type</dfn></dt>
<dd>
<p>In Java, one of a particular group of <a
href="section-14.html#simple-type" class="term"
title="A data type whose values contain no subparts. An example is the integers.">simple
types</a> that are not implemented as objects. An example is an
<code>int</code>.</p>
</dd>
<dt><dfn id="primitive-element">primitive element</dfn></dt>
<dd>
<p>In set notation, this is a single element that is a member of the
base type for the set. This is as opposed to an element of the set being
another set.</p>
</dd>
<dt><dfn id="primitive-nth-root-of-unity">primitive nth root of
unity</dfn></dt>
<dd>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
th root of 1. Normally a <a href="section-14.html#complex-number"
class="term"
title="In mathematics, an imaginary number, that is, a number with a real component and an imaginary component.">complex
number</a>. An intuitive way to view this is one
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
th of the unit circle in the complex plain.</p>
</dd>
<dt><dfn id="priority-queue">priority queue</dfn></dt>
<dd>
<p>An ADT whose primary operations of insert of records, and deletion of
the greatest (or, in an alternative implementation, the least) valued
record. Most often implemented using the <a href="section-14.html#heap"
class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
data structure. The name comes from a common application where the
records being stored represent tasks, with the ordering values based on
the <a href="section-14.html#priority" class="term"
title="A quantity assigned to each of a collection of jobs or tasks that indicate importance for order of processing. For example, in an operating system, there could be a collection of processes (jobs) ready to run. The operating system must select the next task to execute, based on their priorities.">priorities</a>
of the tasks.</p>
</dd>
<dt><dfn id="priority">priority</dfn></dt>
<dd>
<p>A quantity assigned to each of a collection of <a
href="section-14.html#job" class="term"
title="Common name for processes or tasks to be run by an operating system. They typically need to be processed in order of importance, and so are kept organised by a priority queue. Another common use for this term is for a collection of tasks to be ordered by a topological sort.">jobs</a>
or tasks that indicate importance for order of processing. For example,
in an operating system, there could be a collection of processes (jobs)
ready to run. The operating system must select the next task to execute,
based on their priorities.</p>
</dd>
<dt><dfn id="probabilistic-algorithm">probabilistic algorithm</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#randomised-algorithm" class="term"
title="An algorithm that involves some form of randomness to control its behaviour. The ultimate goal of a randomised algorithm is to improve performance over a deterministic algorithm to solve the same problem. There are a number of variations on this theme. A &#39;Las Vegas algorithm&#39; returns a correct result, but the amount of time required might or might not improve over a deterministic algorithm. A &#39;Monte Carlo algorithm&#39; is a form of probabilistic algorithm that is not guarenteed to return a correct result, but will return a result relatively quickly.">randomised
algorithm</a> that might yield an incorrect result, or that might fail
to produce a result.</p>
</dd>
<dt><dfn id="probabilistic-data-structure">probabilistic data
structure</dfn></dt>
<dd>
<p>Any data structure that uses <a
href="section-14.html#probabilistic-algorithm" class="term"
title="A form of randomised algorithm that might yield an incorrect result, or that might fail to produce a result.">probabilistic
algorithms</a> to perform its operations. A good example is the <a
href="section-14.html#skip-list" class="term"
title="A form of linked list that adds additional links to improve the cost of fundamental operations like insert, delete, and search. It is a probabilistic data structure since it adds the additional links using a probabilistic algorithm. It can implement a dictionary more efficiently than a BST, and is roughly as difficult to implement.">skip
list</a>.</p>
</dd>
<dt><dfn id="probe-function">probe function</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
the function used by a <a href="section-14.html#collision-resolution"
class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method to calculate where to look next in the <a
href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>.</p>
</dd>
<dt><dfn id="probe-sequence">probe sequence</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
the series of <a href="section-14.html#slot" class="term"
title="In hashing, a position in a hash table.">slots</a> visited by the
<a href="section-14.html#probe-function" class="term"
title="In hashing, the function used by a collision resolution method to calculate where to look next in the hash table.">probe
function</a> during <a href="section-14.html#collision-resolution"
class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a>.</p>
</dd>
<dt><dfn id="problem-instance">problem instance</dfn></dt>
<dd>
<p>A specific selection of values for the parameters to a problem. In
other words, a specific set of inputs to a problem. A given problem
instance has a size under some <a href="section-14.html#cost-model"
class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a>.</p>
</dd>
<dt><dfn id="problem-lower-bound">problem lower bound</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, the tightest <a href="section-14.html#lower-bound"
class="term"
title="An lower bound for a growth rate $f$ is any growth rate $g$ that is less than or equal to it. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \geq c g(n)$ for all $n \geq n_0$. We also write $f \in \Omega(g)$ or slightly imprecisely $f(n) \in \Omega(g(n))$ (this is Omega notation). Usually, we are interested in finding a lower bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, a lower bound for an algorithm is a lower bound for the asymptotic complexity of the algorithm, the growth rate of its complexity.">lower
bound</a> that we can prove over all <a href="section-14.html#algorithm"
class="term"
title="A method or a process followed to solve a problem.">algorithms</a>
for that <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
This is often much harder to determine than the <a
href="section-14.html#problem-upper-bound" class="term"
title="In algorithm analysis, the upper bound for the best algorithm that we know for the problem. Since the upper bound for the algorithm can be very different for different situations (such as the best case or worst case), we typically have to specify which situation we are referring to.">problem
upper bound</a>. Since the lower bound for the algorithm can be very
different for different situations (such as the <a
href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best
case</a> or <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>), we typically have to specify which situation we are referring
to.</p>
</dd>
<dt><dfn id="problem-upper-bound">problem upper bound</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, the <a href="section-14.html#upper-bound" class="term"
title="An upper bound for a growth rate $f$ is any growth rate $g$ that is greater than or equal to it. Formally, there are constants $n_0 \geq 0$ and $C &gt; 0$ such that $f(n) \leq C g(n)$ for all $n \geq n_0$. We also write $f \in O(g)$ or slightly imprecisely $f(n) \in O(g(n))$ (this is big-$O$ notation). Usually, we are interested in finding an upper bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, an upper bound for an algorithm is an upper bound for the asymptotic complexity of the algorithm, the growth rate of its complexity. In practice, we are looking for the best possible upper bound that has a simple mathematical expression. For example, we may write $T(n) \in O(n^2)$ if $T$ is the (time) complexity of the algorithm to say that the complexity is quadratic, i.e. the asymptoptic complexity of the algorithm has as upper bound the growth rate given by squaring.">upper
bound</a> for the best <a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
that we know for the <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>.
Since the upper bound for the algorithm can be very different for
different situations (such as the <a href="section-14.html#best-case"
class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best
case</a> or <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>), we typically have to specify which situation we are referring
to.</p>
</dd>
<dt><dfn id="problem">problem</dfn></dt>
<dd>
<p>A task to be performed. It is best thought of as a <a
href="section-14.html#function" class="term"
title="In mathematics, a matching between inputs (the domain) and outputs (the range). In programming, a subroutine that takes input parameters and uses them to compute and return a value. In this case, it is usually considered bad practice for a function to change any global variables (doing so is called a side effect).">function</a>
or a mapping of inputs to outputs.</p>
</dd>
<dt><dfn id="procedural-programming-paradigm">procedural programming
paradigm</dfn></dt>
<dd>
<p>Procedural programming uses a list of instructions (and procedure
calls) that define a series of computational steps to be carried out.
This is in contrast to the <a
href="section-14.html#object-oriented-programming-paradigm" class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming paradigm</a>.</p>
</dd>
<dt><dfn id="procedural">procedural</dfn></dt>
<dd>
<p>Typically referring to the <a
href="section-14.html#procedural-programming-paradigm" class="term"
title="Procedural programming uses a list of instructions (and procedure calls) that define a series of computational steps to be carried out. This is in contrast to the object-oriented programming paradigm.">procedural
programming paradigm</a>, in contrast to the <a
href="section-14.html#object-oriented-programming-paradigm" class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming paradigm</a>.</p>
</dd>
<dt><dfn>production rule</dfn></dt>
<dd>
<p>See <a href="section-14.html#production" class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production</a></p>
</dd>
<dt><dfn id="production">production</dfn> (<dfn
id="production-rule">production rule</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>
is comprised of production rules. The production rules consist of <a
href="section-14.html#terminal" class="term"
title="A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler.">terminals</a>
and <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminals</a>,
with one of the non-terminals being the <a
href="section-14.html#start-symbol" class="term"
title="In a grammar, the designated non-terminal that is the intial point for deriving a string in the langauge.">start
symbol</a>. Each production rule replaces one or more non-terminals
(perhaps with associated terminals) with one or more terminals and
non-terminals. Depending on the restrictions placed on the form of the
rules, there are classes of languages that can be represented by
specific types of grammars. A <a href="section-14.html#derivation"
class="term"
title="In formal languages, the process of executing a series of production rules from a grammar. A typical example of a derivation would be the series of productions executed to go from the start symbol to a given string.">derivation</a>
is a series of productions that results in a string (that is, all
non-terminals), and this derivation can be represented as a <a
href="section-14.html#parse-tree" class="term"
title="A tree that represents the syntactic structure of an input string, making it easy to compare against a grammar to see if it is syntactically correct.">parse
tree</a>.</p>
</dd>
<dt><dfn id="program">program</dfn></dt>
<dd>
<p>An instance, or concrete representation, of an algorithm in some
programming language.</p>
</dd>
<dt><dfn id="promotion">promotion</dfn></dt>
<dd>
<p>In the context of certain <a href="section-14.html#balanced-tree"
class="term"
title="A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree.">balanced
tree</a> structures such as the <a href="section-14.html#2-3-tree"
class="term"
title="A specialised form of the B-tree where each internal node has either 2 children or 3 children. Key values are ordered to maintain the binary search tree property. The 2-3 tree is always height balanced, and its insert, search, and remove operations all have $O(\log n)$ cost.">2-3
tree</a>, a promotion takes place when an insertion causes the node to
<a href="section-14.html#overflow" class="term"
title="The condition where the amount of data stored in an entity has exceeded its capacity. For example, a node in a B-tree can store a certain number of records. If a record is attempted to be inserted into a node that is full, then something has to be done to handle this case.">overflow</a>.
In the case of the 2-3 tree, the <a href="section-14.html#key"
class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
with the middlemost value is sent to be stored in the parent.</p>
</dd>
<dt><dfn id="proof-by-contradiction">proof by contradiction</dfn></dt>
<dd>
<p>A mathematical proof technique that proves a theorem by first
assuming that the theorem is false, and then uses a chain of reasoning
to reach a logical contradiction. Since when the theorem is false a
logical contradiction arises, the conclusion is that the theorem must be
true.</p>
</dd>
<dt><dfn id="proof-by-induction">proof by induction</dfn></dt>
<dd>
<p>A mathematical proof technique similar to <a
href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>.
It is used to prove a parameterised theorem $S(n)$, that is, a theorem
where there is a <a href="section-14.html#induction-variable"
class="term"
title="The variable used to parameterise the theorem being proved by induction. For example, if we seek to prove that the sum of the integers from 1 to $n$ is $n(n+1)/2$, then $n$ is the induction variable. An induction variable must be an integer.">induction
variable</a> involved (such as the sum of the numbers from 1 to $n$).
One first proves that the theorem holds true for a <a
href="section-14.html#base-case" class="term"
title="In recursion or proof by induction, the base case is the termination condition. This is a simple input or value that can be solved (or proved in the case of induction) without resorting to a recursive call (or the induction hypothesis).">base
case</a>, then one proves the implication that whenever $S(n)$ is true
then $S(n+1)$ is also true. Another variation is <a
href="section-14.html#strong-induction" class="term"
title="An alternative formulation for the induction step in a proof by induction. The induction step for strong induction is: If **Thrm** holds for all $k, c \leq k &lt; n$, then **Thrm** holds for $n$.">strong
induction</a>.</p>
</dd>
<dt><dfn id="proof">proof</dfn></dt>
<dd>
<p>The establishment of the truth of anything, a demonstration.</p>
</dd>
<dt><dfn id="proving-the-contrapositive">proving the
contrapositive</dfn></dt>
<dd>
<p>We can prove that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>⇒</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">P \Rightarrow Q</annotation></semantics></math>
by proving
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mrow><mi mathvariant="normal">n</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi></mrow><mspace width="0.222em"></mspace><mi>Q</mi><mo stretchy="false" form="postfix">)</mo><mo>⇒</mo><mo stretchy="false" form="prefix">(</mo><mrow><mi mathvariant="normal">n</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi></mrow><mspace width="0.222em"></mspace><mi>P</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathrm{not}\ Q) \Rightarrow (\mathrm{not}\ P)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="pseudo-polynomial">pseudo polynomial</dfn></dt>
<dd>
<p>In complexity analysis, refers to the time requirements of an
algorithm for an <a href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-Complete</a>
problem that still runs acceptably fast for practical application. An
example is the standard <a href="section-14.html#dynamic-programming"
class="term"
title="An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem.">dynamic
programming</a> algorithm for the <a
href="section-14.html#knapsack-problem" class="term"
title="While there are many variations of this problem, here is a typical version: Given knapsack of a fixed size, and a collection of objects of various sizes, is there a subset of the objects that exactly fits into the knapsack? This problem is known to be NP-complete, but can be solved for problem instances in practical time relatively quickly using dynamic programming. Thus, it is considered to have pseudo-polynomial cost. An optimisation problem version is to find the subset that can fit with the greatest amount of items, either in terms of their total size, or in terms of the sum of values associated with each item.">knapsack
problem</a>.</p>
</dd>
<dt><dfn id="pseudo-random">pseudo random</dfn></dt>
<dd>
<p>In random number theory this means that, given all past terms in the
series, no future term of the series can be accurately predicted in
polynomial time.</p>
</dd>
<dt><dfn id="pseudo-random-probing">pseudo-random probing</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
this is a <a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method that stores a random permutation of the values 1
through the size of the <a href="section-14.html#hash-table"
class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>. Term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the <a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> is simply the value of position
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
in the permuation.</p>
</dd>
<dt><dfn id="push">push</dfn></dt>
<dd>
<p>A specialised term used to indicate inserting an <a
href="section-14.html#element" class="term"
title="One value or member in a set.">element</a> onto a <a
href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>.</p>
</dd>
<dt><dfn id="pushdown-automata">pushdown automata</dfn> (<dfn
id="pda">PDA</dfn>)</dt>
<dd>
<p>A type of <a href="section-14.html#finite-state-automata"
class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state automata</a> that adds a stack memory to the basic <a
href="section-14.html#deterministic-finite-automata" class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a> machine. This extends the set of languages that can
be recognise to the <a href="section-14.html#context-free-language"
class="term"
title="The set of languages that can be defined by context-sensitive grammars.">context-free
languages</a>.</p>
</dd>
<dt><dfn id="quadratic-growth-rate">quadratic growth rate</dfn></dt>
<dd>
<p>A growth rate function of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">cn^2</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is the input size and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>c</mi><annotation encoding="application/x-tex">c</annotation></semantics></math>
is a constant.</p>
</dd>
<dt><dfn id="quadratic-probing">quadratic probing</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
this is a <a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> method that computes term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the <a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> using some quadratic equation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><msubsup><mi>i</mi><mi>b</mi><mn>2</mn></msubsup><mi>i</mi><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">ai^2 _ bi + c</annotation></semantics></math>
for suitable constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">a, b, c</annotation></semantics></math>.
The simplest form is simply to use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>i</mi><mn>2</mn></msup><annotation encoding="application/x-tex">i^2</annotation></semantics></math>
as term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the probe sequence.</p>
</dd>
<dt><dfn id="quadtree">quadtree</dfn></dt>
<dd>
<p>A <a href="section-14.html#full-tree" class="term"
title="A binary tree is full if every node is either a leaf node or else it is an internal node with two non-empty children.">full
tree</a> where each internal node has four children. Most typically used
to store two dimensional <a href="section-14.html#spatial-data"
class="term"
title="Any object or record that has a position (in space).">spatial
data</a>. Related to the <a href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>.
The difference is that the quadtree splits all dimensions
simultaneously, while the bintree splits one dimension at each level.
Thus, to extend the quadtree concept to more dimensions requires a rapid
increase in the number of splits (for example, 8 in three
dimensions).</p>
</dd>
<dt><dfn id="queue">queue</dfn></dt>
<dd>
<p>A list-like structure in which elements are inserted only at one end,
and removed only from the other one end.</p>
</dd>
<dt><dfn id="quicksort">Quicksort</dfn></dt>
<dd>
<p>A sort that is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>
in the <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best</a>
and <a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>
cases, though
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
in the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>. However, a reasonable implmentation will make the worst case
occur under exceedingly rare circumstances. Due to its tight inner loop,
it tends to run better than any other known sort in general cases. Thus,
it is a popular sort to use in code libraries. It works by divide and
conquer, by selecting a <a href="section-14.html#pivot" class="term"
title="In Quicksort, the value that is used to split the list into sublists, one with lesser values than the pivot, the other with greater values than the pivot.">pivot</a>
value, splitting the list into parts that are either less than or
greater than the pivot, and then sorting the two parts.</p>
</dd>
<dt><dfn id="radix-sort">radix sort</dfn></dt>
<dd>
<p>A sorting algorithm that works by processing records with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
digit keys in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
passes, where each pass sorts the records according to the current
digit. At the end of the process, the records will be sorted. This can
be efficient if the number of digits is small compared to the number of
records. However, if the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
records all have unique key values, than at least
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(\log n)</annotation></semantics></math>
digits are required, leading to an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
sorting algorithm that tends to be much slower than other sorting
algorithms like <a href="section-14.html#quicksort" class="term"
title="A sort that is $O(n \log n)$ in the best and average cases, though $O(n^2)$ in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts.">Quicksort</a>
or <a href="section-14.html#mergesort" class="term"
title="A sorting algorithm that requires $O(n \log n)$ in the best, average, and worst cases. Conceptually it is simple: Split the list in half, sort the halves, then merge them together. It is a bit complicated to implement efficiently on an array.">Mergesort</a>.</p>
</dd>
<dt><dfn>radix</dfn></dt>
<dd>
<p>See <a href="section-14.html#base" class="term"
title="The number of digits in a number representation. For example, we typically represent numbers in base (or radix) 10. Hexidecimal is base (or radix) 16.">base</a></p>
</dd>
<dt><dfn>RAM</dfn></dt>
<dd>
<p>See <a href="section-14.html#random-access-memory" class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">random
access memory</a></p>
</dd>
<dt><dfn id="random-access-memory">random access memory</dfn> (<dfn
id="ram">RAM</dfn>)</dt>
<dd>
<p>Abbreviated RAM, this is the principle example of <a
href="section-14.html#primary-storage" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">primary
storage</a> in a modern computer. Data access times are typically
measured in billionths of a second (microseconds), which is roughly a
million times faster than data access from a disk drive. RAM is where
data are held for immediate processing, since access times are so much
faster than for <a href="section-14.html#secondary-storage" class="term"
title="Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive.">secondary
storage</a>. RAM is a typical part of a computer’s <a
href="section-14.html#memory-hierarchy" class="term"
title="The concept that a computer system stores data in a range of storage types that range from fast but expensive (primary storage) to slow but cheap (secondary storage). When there is too much data to store in primary storage, the goal is to have the data that is needed soon or most often in the primary storage as much as possible, by using caching techniques.">memory
hierarchy</a>.</p>
</dd>
<dt><dfn id="random-access">random access</dfn></dt>
<dd>
<p>In <a href="section-14.html#file-processing" class="term"
title="The domain with computer science that deals with processing data stored on a disk drive (in a file), or more broadly, dealing with data stored on any peripheral storage device. Two fundamental properties make dealing with data on a peripheral device different from dealing with data in main memory: (1) Reading/writing data on a peripheral storage device is far slower than reading/writing data to main memory (for example, a typical disk drive is about a million times slower than RAM). (2) All I/O to a peripheral device is typically in terms of a block of data (for example, nearly all disk drives do all I/O in terms of blocks of 512 bytes).">file
processing</a> terminology, a <a href="section-14.html#disk-access"
class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
access</a> to a random position within the file. More generally, the
ability to access an arbitrary record in the file.</p>
</dd>
<dt><dfn id="random-permutation">random permutation</dfn></dt>
<dd>
<p>One of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math>
possible permutations for a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
element is selected in such a way that each permutation has equal
probability of being selected.</p>
</dd>
<dt><dfn id="randomised-algorithm">randomised algorithm</dfn></dt>
<dd>
<p>An algorithm that involves some form of randomness to control its
behaviour. The ultimate goal of a randomised algorithm is to improve
performance over a deterministic algorithm to solve the same problem.
There are a number of variations on this theme. A “Las Vegas algorithm”
returns a correct result, but the amount of time required might or might
not improve over a <a href="section-14.html#deterministic-algorithm"
class="term"
title="An algorithm that does not involve any element of randomness, and so its behaviour on a given input will always be the same. This is in contrast to a randomised algorithm.">deterministic
algorithm</a>. A “Monte Carlo algorithm” is a form of <a
href="section-14.html#probabilistic-algorithm" class="term"
title="A form of randomised algorithm that might yield an incorrect result, or that might fail to produce a result.">probabilistic
algorithm</a> that is not guarenteed to return a correct result, but
will return a result relatively quickly.</p>
</dd>
<dt><dfn id="range-query">range query</dfn></dt>
<dd>
<p>Records are returned if their relevant key value falls within a
specified range.</p>
</dd>
<dt><dfn id="range">range</dfn></dt>
<dd>
<p>The set of possible outputs for a function.</p>
</dd>
<dt><dfn>read/write head</dfn></dt>
<dd>
<p>See <a href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a></p>
</dd>
<dt><dfn id="rebalancing-operation">rebalancing operation</dfn></dt>
<dd>
<p>An operation performed on balanced search trees, such as the <a
href="section-14.html#avl-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a splay tree in that it uses the concept of rotations in the insert and remove operations.">AVL
tree</a> or <a href="section-14.html#splay-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a>, for the purpose of keeping the tree <a
href="section-14.html#height-balanced" class="term"
title="The condition the depths of each subtree in a tree are roughly the same.">height
balanced</a>.</p>
</dd>
<dt><dfn id="record">record</dfn></dt>
<dd>
<p>A collection of information, typically implemented as an <a
href="section-14.html#object" class="term"
title="An instance of a class, that is, something that is created and takes up storage during the execution of a computer program. In the object-oriented programming paradigm, objects are the basic units of operation. Objects have state in the form of data members, and they know how to perform certain actions (methods).">object</a>
in an <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming language</a>. Many data structures are organised containers
for a collection of records.</p>
</dd>
<dt><dfn id="recurrence-relation">recurrence relation</dfn></dt>
<dd>
<p>A <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a> (or less formally, recurrence) defines a function by means
of an expression that includes one or more (smaller) instances of
itself. A classic example is the <a href="section-14.html#recursion"
class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursive</a>
definition for the factorial function,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>n</mi><mo>*</mo><mi>F</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">F(n) = n*F(n-1)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="recurrence-with-full-history">recurrence with full
history</dfn></dt>
<dd>
<p>A special form of <a href="section-14.html#recurrence-relation"
class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a> that includes a summation with a copy of the recurrence
inside. The recurrence that represents the average case cost for <a
href="section-14.html#quicksort" class="term"
title="A sort that is $O(n \log n)$ in the best and average cases, though $O(n^2)$ in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts.">Quicksort</a>
is an example. This internal summation can typically be removed with
simple techniques to simplify solving the recurrence.</p>
</dd>
<dt><dfn id="recursion">recursion</dfn></dt>
<dd>
<p>The process of using recursive calls. An algorithm is recursive if it
calls itself to do part of its work. See <a
href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>.</p>
</dd>
<dt><dfn id="recursive-call">recursive call</dfn></dt>
<dd>
<p>Within a <a href="section-14.html#recursive-function" class="term"
title="A function that includes a recursive call.">recursive
function</a>, it is a call that the function makes to itself.</p>
</dd>
<dt><dfn id="recursive-data-structure">recursive data
structure</dfn></dt>
<dd>
<p>A data structure that is partially composed of smaller or simpler
instances of the same data structure. For example, <a
href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
lists</a> and <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary
trees</a> can be viewed as recursive data structures.</p>
</dd>
<dt><dfn id="recursive-function">recursive function</dfn></dt>
<dd>
<p>A function that includes a <a href="section-14.html#recursive-call"
class="term"
title="Within a recursive function, it is a call that the function makes to itself.">recursive
call</a>.</p>
</dd>
<dt><dfn id="recursively-enumerable">recursively enumerable</dfn></dt>
<dd>
<p>A language
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>
is recursively enumerable if there exists a <a
href="section-14.html#turing-machine" class="term"
title="A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer.">Turing
machine</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>M</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L = L(M)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="red-black-tree">red-black tree</dfn></dt>
<dd>
<p>A balanced variation on a <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>.</p>
</dd>
<dt><dfn id="reduction">reduction</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, the process of deriving <a
href="section-14.html#asymptotic-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">asymptotic
bounds</a> for one <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
from the asymptotic bounds of another. In particular, if problem A can
be used to solve problem B, and problem A is proved to be in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(f(n))</annotation></semantics></math>,
then problem B must also be in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(f(n))</annotation></semantics></math>.
Reductions are often used to show that certain problems are at least as
expensive as sorting, or that certain problems are <a
href="section-14.html#np-complete" class="term"
title="A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard.">NP-Complete</a>.</p>
</dd>
<dt><dfn id="reference-count-algorithm">reference count
algorithm</dfn></dt>
<dd>
<p>An algorithm for <a href="section-14.html#garbage-collection"
class="term"
title="Languages with garbage collection such Java, Javascript, Lisp, and Scheme will periodically reclaim garbage and return it to free store.">garbage
collection</a>. Whenever a reference is made from a variable to some
memory location, a counter associated with that memory location is
incremented. Whenever the reference is changed or deleted, the reference
count is decremented. If this count goes to zero, then the memory is
considered free for reuse. This approach can fail if there is a cycle in
the chain of references.</p>
</dd>
<dt><dfn id="reference-parameter">reference parameter</dfn></dt>
<dd>
<p>A <a href="section-14.html#parameter" class="term"
title="The values making up an input to a function.">parameter</a> that
has been <a href="section-14.html#pass-by-reference" class="term"
title="A reference to the variable is passed to the called function. So, any modifications will affect the original variable.">passed
by reference</a>. Such a parameter can be modified inside the function
or method.</p>
</dd>
<dt><dfn id="reference">reference</dfn></dt>
<dd>
<p>A value that enables a program to directly access some particular <a
href="section-14.html#data-item" class="term"
title="A piece of information or a record whose value is drawn from a type.">data
item</a>. An example might be a byte position within a file where the
record is stored, or a pointer to a record in memory. (Note that Java
makes a distinction between a reference and the concept of a pointer,
since it does not define a reference to necessarily be a byte position
in memory.)</p>
</dd>
<dt><dfn id="reflexive">reflexive</dfn></dt>
<dd>
<p>In set notation, binary relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
on set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is reflexive if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">aRa</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">a \in \mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="region-quadtree">region quadtree</dfn></dt>
<dd>
<p>A <a href="section-14.html#spatial-data-structure" class="term"
title="A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree.">spatial
data structure</a> for storing 2D pixel data. The idea is that the root
of the tree represents the entire image, and it is recursively divided
into four equal subquadrants if not all pixels associated with the
current node have the same value. This is structurally equivalent to a
<a href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a>, only the decomposition rule is changed.</p>
</dd>
<dt><dfn id="regular-expression">regular expression</dfn></dt>
<dd>
<p>A way to specify a set of strings that define a language using the
operators of union, contatenation, and star-closure. A regular
expression defines some <a href="section-14.html#regular-language"
class="term"
title="A language $L$ is a regular language if and only if there exists a deterministic finite automata $M$ such that $L = L(M)$.">regular
language</a>.</p>
</dd>
<dt><dfn id="regular-grammar">regular grammar</dfn></dt>
<dd>
<p>And grammar that is either right-regular or left-regular. Every
regular grammar describes a regular language.</p>
</dd>
<dt><dfn id="regular-language">regular language</dfn></dt>
<dd>
<p>A language
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>
is a regular language if and only if there exists a <a
href="section-14.html#deterministic-finite-automata" class="term"
title="An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behaviour defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can &#39;do&#39; is change state before going to the next letter to the right.">deterministic
finite automata</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>M</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">L = L(M)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="relation">relation</dfn></dt>
<dd>
<p>In set notation, a relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
over set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
is a set of ordered pairs from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="replacement-selection">replacement selection</dfn></dt>
<dd>
<p>A variant of <a href="section-14.html#heapsort" class="term"
title="A sorting algorithm that costs $O(n \log n)$ time in the best, average, and worst cases. It tends to be slower than Mergesort and Quicksort. It works by building a max heap, and then repeatedly removing the item with maximum key value (moving it to the end of the heap) until all elements have been removed (and replaced at their proper location in the array).">heapsort</a>
most often used as one phase of an <a
href="section-14.html#external-sort" class="term"
title="A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory.">external
sort</a>. Given a collection of records stored in an <a
href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>,
and a stream of additional records too large to fit into <a
href="section-14.html#working-memory" class="term"
title="The portion of main memory available to an algorithm for its use. Typically refers to main memory made available to an algorithm that is operating on large amounts of data stored in peripheral storage, the working memory represents space that can hold some subset of the total data being processed.">working
memory</a>, replacement selection will unload the <a
href="section-14.html#heap" class="term"
title="The head data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes $O(\log n)$ time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only $O(n)$ time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. Uncommonly, *heap* is a synonym for free store.">heap</a>
by sending records to an output stream, and seek to bring new records
into the heap from the input stream in preference to shrinking the heap
size whenever possible.</p>
</dd>
<dt><dfn id="reserved-block">reserved block</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, this refers to space in the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a> that has been allocated to store data received from the <a
href="section-14.html#client" class="term"
title="The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool.">client</a>.
This is in contrast to the <a href="section-14.html#free-block"
class="term" title="A block of unused space in a memory pool.">free
blocks</a> that represent space in the memory pool that is not allocated
to storing client data.</p>
</dd>
<dt><dfn id="resource-constraints">resource constraints</dfn></dt>
<dd>
<p>Examples of resource constraints include the total space available to
store the data (possibly divided into separate main memory and disk
space constraints) and the time allowed to perform each subtask.</p>
</dd>
<dt><dfn id="root">root</dfn></dt>
<dd>
<p>In a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>,
the topmost <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
of the tree. All other nodes in the tree are <a
href="section-14.html#descendant" class="term"
title="In a tree, the set of all nodes that have a node $A$ as an ancestor are the descendants of $A$. In other words, all of the nodes that can be reached from $A$ by progressing downwards in tree. Another way to say it is: The children of $A$, their children, and so on.">descendants</a>
of the root.</p>
</dd>
<dt><dfn id="rotation">rotation</dfn></dt>
<dd>
<p>In the <a href="section-14.html#avl-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a splay tree in that it uses the concept of rotations in the insert and remove operations.">AVL
tree</a> and <a href="section-14.html#splay-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a>, a rotation is a local operation performed on a node, its
children, and its grandchildren that can result in reordering their
relationship. The goal of performing a rotation is to make the tree more
<a href="section-14.html#balanced-tree" class="term"
title="A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree.">balanced</a>.</p>
</dd>
<dt><dfn id="rotational-delay">rotational delay</dfn></dt>
<dd>
<p>When processing a <a href="section-14.html#disk-access" class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
access</a>, the time that it takes for the first byte of the desired
data to move to under the <a href="section-14.html#i-o-head"
class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a>. On average, this will take one half of a disk rotation, and so
constitutes a substantial portion of the time required for the disk
access.</p>
</dd>
<dt><dfn id="rotational-latency">rotational latency</dfn></dt>
<dd>
<p>A synonym for <a href="section-14.html#rotational-delay" class="term"
title="When processing a disk access, the time that it takes for the first byte of the desired data to move to under the I/O head. On average, this will take one half of a disk rotation, and so constitutes a substantial portion of the time required for the disk access.">rotational
delay</a>.</p>
</dd>
<dt><dfn id="run-file">run file</dfn></dt>
<dd>
<p>A temporary file that is created during the operation of an <a
href="section-14.html#external-sort" class="term"
title="A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory.">external
sort</a>, the run file contains a collection of <a
href="section-14.html#run" class="term"
title="A series of sorted records. Most often this refers to a (sorted) subset of records that are being sorted by means of an external sort.">runs</a>.
A common structure for an external sort is to first create a series of
runs (stored in a run file), followed by merging the runs together.</p>
</dd>
<dt><dfn id="run-time-polymorphism">run-time polymorphism</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#polymorphism" class="term"
title="An object-oriented programming term meaning *one name, many forms*. It describes the ability of software to change its behaviour dynamically. Two basic forms exist: run-time polymorphism and compile-time polymorphism.">polymorphism</a>
known as Overriding. Overridden methods are those which implement a new
method with the same signature as a method inherited from its <a
href="section-14.html#base-class" class="term"
title="In object-oriented programming, a class from which another class inherits. The class that inherits is called a subclass.">base
class</a>. Compare to <a
href="section-14.html#compile-time-polymorphism" class="term"
title="A form of polymorphism known as Overloading. Overloaded methods have the same names, but different signatures as a method available elsewhere in the class. Compare to run-time polymorphism.">compile-time
polymorphism</a>.</p>
</dd>
<dt><dfn id="run">run</dfn></dt>
<dd>
<p>A series of sorted records. Most often this refers to a (sorted)
subset of records that are being sorted by means of an <a
href="section-14.html#external-sort" class="term"
title="A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory.">external
sort</a>.</p>
</dd>
<dt><dfn id="runtime-environment">runtime environment</dfn></dt>
<dd>
<p>The environment in which a program (of a particular programming
language) executes. The runtime environment handles such activities as
managing the <a href="section-14.html#runtime-stack" class="term"
title="The place where an activation record is stored when a subroutine is called during a program&#39;s runtime.">runtime
stack</a>, the <a href="section-14.html#free-store" class="term"
title="Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as `new` in C++ and Java.">free
store</a>, and the <a href="section-14.html#garbage-collection"
class="term"
title="Languages with garbage collection such Java, Javascript, Lisp, and Scheme will periodically reclaim garbage and return it to free store.">garbage
collector</a>, and it conducts the execution of the program.</p>
</dd>
<dt><dfn id="runtime-stack">runtime stack</dfn></dt>
<dd>
<p>The place where an <a href="section-14.html#activation-record"
class="term"
title="The entity that is stored on the runtime stack during program execution. It stores any active local variable and the return address from which a new subroutine is being called, so that this information can be recovered when the subroutine terminates.">activation
record</a> is stored when a subroutine is called during a program’s
runtime.</p>
</dd>
<dt><dfn id="scanner">scanner</dfn></dt>
<dd>
<p>The part of a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>
that is responsible for doing <a href="section-14.html#lexical-analysis"
class="term"
title="A phase of a compiler or interpreter responsible for reading in characters of the program or language and grouping them into tokens.">lexical
analysis</a>.</p>
</dd>
<dt><dfn id="scope">scope</dfn></dt>
<dd>
<p>The parts of a program that can see and access a variable.</p>
</dd>
<dt><dfn id="search-key">search key</dfn></dt>
<dd>
<p>A field or part of a record that is used to represent the record when
searching. For example, in a database of customer records, we might want
to search by name. In this case the name field is used as the search
key.</p>
</dd>
<dt><dfn id="search-lower-bound">search lower bound</dfn></dt>
<dd>
<p>The problem of searching in an <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>
has provable lower bounds for specific variations of the problem. For an
unsorted array, it is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n)</annotation></semantics></math>
<a href="section-14.html#comparison" class="term"
title="The act of comparing two keys or records. For many data types, a comparison has constant time cost. The number of comparisons required is often used as a measure of cost for sorting and searching algorithms.">comparisons</a>
in the <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a>, typically proved using an <a
href="section-14.html#adversary-argument" class="term"
title="A type of lower bounds proof for a problem where a (fictional) &#39;adversary&#39; is assumed to control access to an algorithm&#39;s input, and which yields information about that input in such a way that will drive the cost for any proposed algorithm to solve the problem as high as possible. So long as the adversary never gives an answer that conflicts with any previous answer, it is permitted to do whatever necessary to make the algorithm require as much cost as possible.">adversary
argument</a>. For a sorted array, it is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(\log n)</annotation></semantics></math>
in the worst case, typically proved using an argument similar to the <a
href="section-14.html#sorting-lower-bound" class="term"
title="The lower bound for the problem of sorting is $\Omega(n \log n)$. This is traditionally proved using a decision tree model for sorting algorithms, and recognising that the minimum depth of the decision tree for any sorting algorithm is $\Omega(n \log n)$ since there are $n!$ permutations of the $n$ input records to distinguish between during the sorting process.">sorting
lower bound</a> proof. Indeed, it is possible to search a sorted array
in the average case in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>
time.</p>
</dd>
<dt><dfn id="search-problem">search problem</dfn></dt>
<dd>
<p>Given a particular key value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>,
the search problem is to locate a <a href="section-14.html#record"
class="term"
title="A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organised containers for a collection of records.">record</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>k</mi><mi>j</mi></msub><mo>,</mo><msub><mi>I</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(k_j, I_j)</annotation></semantics></math>
in some collection of records <strong>L</strong> such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k_j = K</annotation></semantics></math>
(if one exists). <a href="section-14.html#searching" class="term"
title="Given a search key $K$ and some collection of records **L**, searching is a systematic method for locating the record (or records) in **L** with key value $k_j = K$.">Searching</a>
is a systematic method for locating the record (or records) with key
value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k_j = K</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="search-tree">search tree</dfn></dt>
<dd>
<p>A <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
data structure that makes search by <a href="section-14.html#key"
class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value more efficient. A type of <a href="section-14.html#container"
class="term"
title="A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables.">container</a>,
it is common to implement an <a href="section-14.html#indexing"
class="term"
title="The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record.">index</a>
using a search tree. A good search tree implementation will guarentee
that insertion, deletion, and search operations are all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="search-trie">search trie</dfn></dt>
<dd>
<p>Any <a href="section-14.html#search-tree" class="term"
title="A tree data structure that makes search by key value more efficient. A type of container, it is common to implement an index using a search tree. A good search tree implementation will guarentee that insertion, deletion, and search operations are all $O(\log n)$.">search
tree</a> that is a <a href="section-14.html#trie" class="term"
title="A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from &#39;retrieval&#39;, and should be pronounced as &#39;try&#39; (in contrast to &#39;tree&#39;, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term &#39;trie&#39; is also sometimes used as a synonym for the alphabet trie.">trie</a>.</p>
</dd>
<dt><dfn id="searching">searching</dfn></dt>
<dd>
<p>Given a <a href="section-14.html#search-key" class="term"
title="A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key.">search
key</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
and some collection of records <strong>L</strong>, searching is a
systematic method for locating the record (or records) in
<strong>L</strong> with key value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub><mo>=</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">k_j = K</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="secondary-clustering">secondary clustering</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
the tendency in certain <a href="section-14.html#collision-resolution"
class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> methods to create clustering in sections of the hash
table. In <a href="section-14.html#primary-clustering" class="term"
title="In hashing, the tendency in certain collision resolution methods to create clustering in sections of the hash table. The classic example is linear probing. This tends to happen when a group of keys follow the same probe sequence during collision resolution.">primary
clustering</a>, this is caused by a cluster of keys that don’t
necessarily hash to the same slot but which following significant
portions of the same <a href="section-14.html#probe-sequence"
class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> during collision resolution. Secondary clustering results
from the keys hashing to the same slot of the table (and so a collision
resolution method that is not affected by the key value must use the
same probe sequence for all such keys). This problem can be resolved by
<a href="section-14.html#double-hashing" class="term"
title="A collision resolution method. A second hash function is used to generate a value $c$ on the key. That value is then used by this key as the step size in linear probing by steps. Since different keys use different step sizes (as generated by the second hash function), this process avoids the clustering caused by standard linear probing by steps.">double
hashing</a> since its probe sequence is determined in part by a second
hash function.</p>
</dd>
<dt><dfn>secondary index</dfn></dt>
<dd>
<p>See <a href="section-14.html#secondary-key-index" class="term"
title="Associates a secondary key value with the primary key of each record having that secondary key value.">secondary
key index</a></p>
</dd>
<dt><dfn id="secondary-key-index">secondary key index</dfn> (<dfn
id="secondary-index">secondary index</dfn>)</dt>
<dd>
<p>Associates a <a href="section-14.html#secondary-key" class="term"
title="A key field in a record such as salary, where a particular key value might be duplicated in multiple records. A secondary key is more likely to be used by a user as a search key than is the record&#39;s primary key.">secondary
key</a> value with the <a href="section-14.html#primary-key"
class="term" title="A unique identifier for a record.">primary key</a>
of each record having that secondary key value.</p>
</dd>
<dt><dfn id="secondary-key">secondary key</dfn></dt>
<dd>
<p>A key field in a record such as salary, where a particular key value
might be duplicated in multiple records. A secondary key is more likely
to be used by a user as a search key than is the record’s <a
href="section-14.html#primary-key" class="term"
title="A unique identifier for a record.">primary key</a>.</p>
</dd>
<dt><dfn id="secondary-storage">secondary storage</dfn></dt>
<dd>
<p>Refers to slower but cheaper means of storing data. Typical examples
include a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, a USB memory stick, or a solid state drive.</p>
</dd>
<dt><dfn id="sector-header">sector header</dfn></dt>
<dd>
<p>On a disk drive, a piece of information at the start of a <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sector</a>
that allows the <a href="section-14.html#i-o-head" class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a> to recognise the identity (or equivalently, the address) of the
current sector.</p>
</dd>
<dt><dfn id="sector">sector</dfn></dt>
<dd>
<p>A unit of space on a <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> that is the amount of data that will be read or written at one
time by the disk drive hardware. This is typically 512 bytes.</p>
</dd>
<dt><dfn id="seed">seed</dfn></dt>
<dd>
<p>In random number theory, the starting value for a random number
series. Typically used with any <a
href="section-14.html#linear-congruential-method" class="term"
title="In random number theory, a process for computing the next number in a pseudo-random sequence. Starting from a seed, the next term $r(i)$ in the series is calculated from term $r(i-1)$ by the equation $$r(i) = (r(i-1)\times b) \bmod t$$ where $b$ and $t$ are constants. These constants must be well chosen for the resulting series of numbers to have desirable properties as a random number sequence.">linear
congruential method</a>.</p>
</dd>
<dt><dfn id="seek">seek</dfn></dt>
<dd>
<p>On a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, the act of moving the <a href="section-14.html#i-o-head"
class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a> from one <a href="section-14.html#track" class="term"
title="On a disk drive, a concentric circle representing all of the sectors that can be viewed by the I/O head as the disk rotates. The significance is that, for a given placement of the I/O head, the sectors on the track can be read without performing a (relatively expensive) seek operation.">track</a>
to another. This is usually considered the most expensive step during a
<a href="section-14.html#disk-access" class="term"
title="The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost.">disk
access</a>.</p>
</dd>
<dt><dfn id="selection-sort">Selection sort</dfn></dt>
<dd>
<p>While this sort requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
time in the <a href="section-14.html#best-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has least cost. Every input size $n$ has its own best case. We **never** consider the best case as removed from input size.">best</a>,
<a href="section-14.html#average-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the average of the costs for all problem instances of a given input size $n$. If not all problem instances have equal probability of occurring, then the average case must be calculated using a weighted average that is specified with the problem (for example, every input may be equally likely). Every input size $n$ has its own average case. We **never** consider the average case as removed from input size.">average</a>,
and <a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst</a>
cases, it requires only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
swap operations. Thus, it does relatively well in applications where
swaps are expensive. It can be viewed as an optimisation on <a
href="section-14.html#bubble-sort" class="term"
title="A simple sort that requires $O(n^2)$ time in best, average, and worst cases. Even an optimised version will normally run slower than Insertion sort, so it has little to recommend it.">Bubble
sort</a>, where a swap is deferred until the end of each iteration.</p>
</dd>
<dt><dfn id="self-organising-list-heuristic">self-organising list
heuristic</dfn></dt>
<dd>
<p>A <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
to use for the purpose of maintaining a <a
href="section-14.html#self-organising-list" class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>. Commonly used heuristics include <a
href="section-14.html#move-to-front" class="term"
title="A heuristic used to maintain a self-organising list. Under this heuristic, whenever a record is accessed it is moved to the front of the list. Analogous to the least recently used heuristic for maintaining a buffer pool.">move-to-front</a>
and <a href="section-14.html#transpose" class="term"
title="In the context of linear algebra, the transpose of a matrix $A$ is another matrix $A^T$ created by writing the rows of $A$ as the columns of $A^T$. In the context of a self-organising list, transpose is a heuristic used to maintain the list. Under this heuristic, whenever a record is accessed it is moved one position closer to the front of the list.">transpose</a>.</p>
</dd>
<dt><dfn id="self-organising-list">self-organising list</dfn></dt>
<dd>
<p>A list that, over a series of search operations, will make use of
some <a href="section-14.html#heuristic" class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
to re-order its elements in an effort to improve search times. Generally
speaking, search is done sequentially from the beginning, but the
self-organising heuristic will attempt to put the records that are most
likely to be searched for at or near the front of the list. While
typically not as efficient as <a href="section-14.html#binary-search"
class="term"
title="A standard recursive algorithm for finding the record with a given search key value within a sorted list. It runs in $O(\log n)$ time. At each step, look at the middle of the current sublist, and throw away the half of the records whose keys are either too small or too large.">binary
search</a> on a sorted list, self-organising lists do not require that
the list be sorted (and so do not pay the cost of doing the sorting
operation).</p>
</dd>
<dt><dfn id="separate-chaining">separate chaining</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
a synonym for <a href="section-14.html#open-hash-system" class="term"
title="A hash system where multiple records might be associated with the same slot of a hash table. Typically this is done using a linked list to store the records. This is in contrast to a closed hash system.">open
hashing</a></p>
</dd>
<dt><dfn id="sequence">sequence</dfn></dt>
<dd>
<p>In set notation, a collection of elements with an order, and which
may contain duplicate-valued elements. A sequence is also sometimes
called a <a href="section-14.html#tuple" class="term"
title="In set notation, another term for a sequence.">tuple</a> or a <a
href="section-14.html#vector" class="term"
title="In set notation, another term for a sequence. As a data structure, the term vector usually used as a snyonym for a dynamic array.">vector</a>.</p>
</dd>
<dt><dfn id="sequential-access">sequential access</dfn></dt>
<dd>
<p>In <a href="section-14.html#file-processing" class="term"
title="The domain with computer science that deals with processing data stored on a disk drive (in a file), or more broadly, dealing with data stored on any peripheral storage device. Two fundamental properties make dealing with data on a peripheral device different from dealing with data in main memory: (1) Reading/writing data on a peripheral storage device is far slower than reading/writing data to main memory (for example, a typical disk drive is about a million times slower than RAM). (2) All I/O to a peripheral device is typically in terms of a block of data (for example, nearly all disk drives do all I/O in terms of blocks of 512 bytes).">file
processing</a> terminology, the requirement that all records in a file
are accessed in sequential order. Alternatively, a storage device that
can only access data sequentially, such as a tape drive.</p>
</dd>
<dt><dfn id="sequential-fit">sequential fit</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, the process of searching the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a> for a <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> large
enough to service a <a href="section-14.html#memory-request"
class="term"
title="In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there.">memory
request</a>, possibly reserving the remaining space as a free block.
Examples are <a href="section-14.html#first-fit" class="term"
title="In a memory manager, first fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. First fit will always allocate the first free block on the free block list that is large enough to service the memory request. The advantage of this approach is that it is typically not necessary to look at all free blocks on the free block list to find a suitable free block. The disadvantage is that it is not &#39;intelligently&#39; selecting what might be a better choice of free block.">first
fit</a>, <a href="section-14.html#circular-first-fit" class="term"
title="In a memory manager, circular first fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Circular first fit is a minor modification on first fit memory allocation, where the last free block allocated from is remembered, and search for the next suitable free block picks up from there. Like first fit, it has the advantage that it is typically not necessary to look at all free blocks on the free block list to find a suitable free block. And it has the advantage over first fit that it spreads out memory allocations evenly across the free block list. This might help to minimise external fragmentation.">circular
first fit</a>, <a href="section-14.html#best-fit" class="term"
title="In a memory manager, best fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Best fit will always allocate from the smallest free block that is large enough to service the memory request. The rationale is that this will be the method that best preserves large blocks needed for unusually large requests. The disadvantage is that it tends to cause external fragmentation in the form of small, unusable memory blocks.">best
fit</a>, and <a href="section-14.html#worst-fit" class="term"
title="In a memory manager, worst fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Worst fit will always allocate from the largest free block. The rationale is that this will be the method least likely to cause external fragmentation in the form of small, unusable memory blocks. The disadvantage is that it tends to eliminate the availability of large freeblocks needed for unusually large requests.">worst
fit</a>.</p>
</dd>
<dt><dfn id="sequential-search">sequential search</dfn></dt>
<dd>
<p>The simplest search algorithm: In an <a href="section-14.html#array"
class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>,
simply look at the array elements in the order that they appear.</p>
</dd>
<dt><dfn id="sequential-tree-representation">sequential tree
representation</dfn></dt>
<dd>
<p>A representation that stores a series of node values with the minimum
information needed to reconstruct the tree structure. This is a
technique for <a href="section-14.html#serialisation" class="term"
title="The process of taking a data structure in memory and representing it as a sequence of bytes. This is sometimes done in order to transmit the data structure across a network or store the data structure in a stream, such as on disk. Deserialisation reconstructs the original data structure from the serialised representation.">serialising</a>
a tree.</p>
</dd>
<dt><dfn id="serialisation">serialisation</dfn></dt>
<dd>
<p>The process of taking a data structure in memory and representing it
as a sequence of bytes. This is sometimes done in order to transmit the
data structure across a network or store the data structure in a <a
href="section-14.html#stream" class="term"
title="The process of delivering content in a serialised form.">stream</a>,
such as on disk. <a href="section-14.html#deserialisation" class="term"
title="The process of returning a serialised representation for a data structure back to its original in-memory form.">Deserialisation</a>
reconstructs the original data structure from the serialised
representation.</p>
</dd>
<dt><dfn id="set-former">set former</dfn></dt>
<dd>
<p>A way to define the membership of a set, by using a text description.
Example:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><mi>x</mi><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.222em"></mspace><mi>x</mi><mspace width="0.222em"></mspace><mtext mathvariant="normal">is a positive integer</mtext><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\{x\ |\ x\ \mbox{is a positive integer}\}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="set-product">set product</dfn></dt>
<dd>
<p>Written
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐐</mi><mo>×</mo><mi>𝐏</mi></mrow><annotation encoding="application/x-tex">\mathbf{Q} \times \mathbf{P}</annotation></semantics></math>,
the set product is a set of ordered pairs such that ordered pair
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a, b)</annotation></semantics></math>
is in the product whenever
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><mi>𝐏</mi></mrow><annotation encoding="application/x-tex">a \in \mathbf{P}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>∈</mo><mi>𝐐</mi></mrow><annotation encoding="application/x-tex">b \in \mathbf{Q}</annotation></semantics></math>.
For example, when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐏</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{P} = \{2, 3, 5\}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐐</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{Q} = \{5, 10\}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐐</mi><mo>×</mo><mi>𝐏</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mo>,</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mspace width="0.222em"></mspace><mo stretchy="false" form="prefix">(</mo><mn>5</mn><mo>,</mo><mn>10</mn><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{Q} \times \mathbf{P} =
\{(2, 5),\ (2, 10),\ (3, 5),\ (3, 10),\ (5, 5),\ (5, 10)\}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="set">set</dfn></dt>
<dd>
<p>A collection of distinguishable <a href="section-14.html#member"
class="term"
title="In set notation, this is a synonym for element. In abstract design, a data item is a member of a type. In an object-oriented language, data members are data fields in an object.">members</a>
or <a href="section-14.html#element" class="term"
title="One value or member in a set.">elements</a>.</p>
</dd>
<dt><dfn id="shallow-copy">shallow copy</dfn></dt>
<dd>
<p>Copying the <a href="section-14.html#reference" class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
or <a href="section-14.html#pointer" class="term"
title="A variable whose value is the address of another variable; a link.">pointer</a>
value without copying the actual content.</p>
</dd>
<dt><dfn id="shellsort">Shellsort</dfn></dt>
<dd>
<p>A sort that relies on the best-case cost of <a
href="section-14.html#insertion-sort" class="term"
title="A sorting algorithm with $O(n^2)$ average and worst case cost, and $O(n)$ best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted.">Insertion
sort</a> to improve over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math>
<a href="section-14.html#worst-case" class="term"
title="In algorithm analysis, specifically complexity of an algorithm, the problem instance from among all problem instances for a given input size $n$ that has the greatest cost. Every input size $n$ has its own worst case. We **never** consider the worst case as removed from input size.">worst
case</a> cost.</p>
</dd>
<dt><dfn id="shifting-method">shifting method</dfn></dt>
<dd>
<p>A technique for finding a <a
href="section-14.html#closed-form-solution" class="term"
title="An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence.">closed-form
solution</a> to a <a href="section-14.html#summation" class="term"
title="The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to $n$ can be written as $\sum_{i=1}^{n} i$.">summation</a>
or <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>.</p>
</dd>
<dt><dfn id="shortest-path">shortest path</dfn></dt>
<dd>
<p>Given a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
with distances or <a href="section-14.html#weight" class="term"
title="A cost or distance most often associated with an edge in a graph.">weights</a>
on the <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>,
the shortest path between two nodes is the path with least total
distance or weight. Examples of the shortest paths problems are the <a
href="section-14.html#single-source-shortest-paths-problem" class="term"
title="Given a graph with weights or distances on the edges, and a designated start vertex $s$, find the shortest path from $s$ to every other vertex in the graph. One algorithm to solve this problem is Dijkstra&#39;s algorithm.">single-source
shortest paths problem</a> and the <a
href="section-14.html#all-pairs-shortest-paths-problem" class="term"
title="Given a graph with weights or distances on the edges, find the shortest paths between every pair of vertices in the graph. One approach to solving this problem is Floyd&#39;s algorithm, which uses the dynamic programming algorithmic technique.">all-pairs
shortest paths problem</a>.</p>
</dd>
<dt><dfn id="sibling">sibling</dfn></dt>
<dd>
<p>In a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>,
a sibling of <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is any other node with the same <a href="section-14.html#parent"
class="term"
title="In a tree, the node $P$ that directly links to a node $A$ is the parent of $A$. $A$ is the child of $P$.">parent</a>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="signature-file">signature file</dfn></dt>
<dd>
<p>In document processing, a signature file is a type of <a
href="section-14.html#bitmap" class="term"
title="An array that stores a single bit at each position. Typically these bits represent Boolean variables associated with a collection of objects, such that the $i$ th bit is the Boolean value for the $i$ th object.">bitmap</a>
used to indicate which documents in a collection contain a given
keyword, such that there is a <a href="section-14.html#bitmap"
class="term"
title="An array that stores a single bit at each position. Typically these bits represent Boolean variables associated with a collection of objects, such that the $i$ th bit is the Boolean value for the $i$ th object.">bitmap</a>
for each keyword.</p>
</dd>
<dt><dfn id="signature">signature</dfn></dt>
<dd>
<p>In a programming language, the signature for a function is its return
type and its list of parameters and their types.</p>
</dd>
<dt><dfn id="simple-cycle">simple cycle</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a <a href="section-14.html#cycle" class="term"
title="In graph terminology, a cycle is a path of length three or more that connects some vertex $v_1$ to itself.">cycle</a>
is simple if its corresponding <a href="section-14.html#path"
class="term"
title="In tree or graph terminology, a sequence of vertices $v_1, v_2, ..., v_n$ forms a path of length $n-1$ if there exist edges from $v_i$ to $v_{i+1}$ for $1 \leq i &lt; n$.">path</a>
is simple, except that the first and last <a
href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> of the cycle
are the same.</p>
</dd>
<dt><dfn id="simple-path">simple path</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
terminology, a <a href="section-14.html#path" class="term"
title="In tree or graph terminology, a sequence of vertices $v_1, v_2, ..., v_n$ forms a path of length $n-1$ if there exist edges from $v_i$ to $v_{i+1}$ for $1 \leq i &lt; n$.">path</a>
is simple if all vertices on the path are distinct.</p>
</dd>
<dt><dfn id="simple-type">simple type</dfn></dt>
<dd>
<p>A <a href="section-14.html#data-type" class="term"
title="A type together with a collection of operations to manipulate the type.">data
type</a> whose values contain no subparts. An example is the
integers.</p>
</dd>
<dt><dfn id="simulating-recursion">simulating recursion</dfn></dt>
<dd>
<p>If a programming language does not support <a
href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursion</a>,
or if you want to implement the effects of recursion more efficiently,
you can use a <a href="section-14.html#stack" class="term"
title="A list-like structure in which elements may be inserted or removed from only one end.">stack</a>
to maintain the collection of subproblems that would be waiting for
completion during the recursive process. Using a loop, whenever a
recursive call would have been made, simply add the necessary program
state to the stack. When a return would have been made from the
recursive call, pop the previous program state off of the stack.</p>
</dd>
<dt><dfn id="single-rotation">single rotation</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#rebalancing-operation"
class="term"
title="An operation performed on balanced search trees, such as the AVL tree or splay tree, for the purpose of keeping the tree height balanced.">rebalancing
operation</a> used by the <a href="section-14.html#splay-tree"
class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a> and <a href="section-14.html#avl-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a splay tree in that it uses the concept of rotations in the insert and remove operations.">AVL
tree</a>.</p>
</dd>
<dt><dfn id="single-source-shortest-paths-problem">single-source
shortest paths problem</dfn></dt>
<dd>
<p>Given a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
with <a href="section-14.html#weight" class="term"
title="A cost or distance most often associated with an edge in a graph.">weights</a>
or distances on the <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>,
and a designated start <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>,
find the shortest path from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
to every other vertex in the graph. One algorithm to solve this problem
is <a href="section-14.html#dijkstras-algorithm" class="term"
title="An algorithm to solve the single-source shortest paths problem in a graph. This is a greedy algorithm. It is nearly identical to Prim&#39;s algorithm for finding a minimum spanning tree, with the only difference being the calculation done to update the best-known distance.">Dijkstra’s
algorithm</a>.</p>
</dd>
<dt><dfn id="singly-linked-list">singly linked list</dfn> (<dfn
id="one-way-list">one-way list</dfn>)</dt>
<dd>
<p>A <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> implementation variant where each list node contains access a
pointer only to the next element in the list.</p>
</dd>
<dt><dfn id="skip-list">skip list</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> that adds additional links to improve the cost of fundamental
operations like insert, delete, and search. It is a <a
href="section-14.html#probabilistic-data-structure" class="term"
title="Any data structure that uses probabilistic algorithms to perform its operations. A good example is the skip list.">probabilistic
data structure</a> since it adds the additional links using a <a
href="section-14.html#probabilistic-algorithm" class="term"
title="A form of randomised algorithm that might yield an incorrect result, or that might fail to produce a result.">probabilistic
algorithm</a>. It can implement a <a href="section-14.html#dictionary"
class="term"
title="An abstract data type or interface for a data structure or software subsystem that supports insertion, search, and deletion of records.">dictionary</a>
more efficiently than a <a href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>,
and is roughly as difficult to implement.</p>
</dd>
<dt><dfn id="slot">slot</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
a position in a <a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a>.</p>
</dd>
<dt><dfn id="snowplow-argument">snowplow argument</dfn></dt>
<dd>
<p>An analogy used to give intuition for why <a
href="section-14.html#replacement-selection" class="term"
title="A variant of heapsort most often used as one phase of an external sort. Given a collection of records stored in an array, and a stream of additional records too large to fit into working memory, replacement selection will unload the heap by sending records to an output stream, and seek to bring new records into the heap from the input stream in preference to shrinking the heap size whenever possible.">replacement
selection</a> will generate <a href="section-14.html#run" class="term"
title="A series of sorted records. Most often this refers to a (sorted) subset of records that are being sorted by means of an external sort.">runs</a>
that are on average twice the size of working memory. Records coming
from the input stream have key values that might be of any size, whose
size is related to the position of a falling snowflake. The replacement
selection process is analogous to a snowplow that moves around a
circular track picking up snow. In steady state, given a certain amount
of snow equivalent to <a href="section-14.html#working-memory"
class="term"
title="The portion of main memory available to an algorithm for its use. Typically refers to main memory made available to an algorithm that is operating on large amounts of data stored in peripheral storage, the working memory represents space that can hold some subset of the total data being processed.">working
memory</a> size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>,
an amount of snow (incoming records from the input stream) is expected
to fall ahead of the plow as the size of the working memory during one
cycle of the plow (analogously, one run of the replacement selection
algorithm). Thus, the snowplow is expected in one pass (one run of
replacement selection) to pick up
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>m</mi></mrow><annotation encoding="application/x-tex">2 m</annotation></semantics></math>
snow.</p>
</dd>
<dt><dfn id="software-engineering">software engineering</dfn></dt>
<dd>
<p>The study and application of engineering to the design, development,
and maintenance of software.</p>
</dd>
<dt><dfn id="software-reuse">software reuse</dfn></dt>
<dd>
<p>In <a href="section-14.html#software-engineering" class="term"
title="The study and application of engineering to the design, development, and maintenance of software.">software
engineering</a>, the concept of reusing a piece of software. In
particular, using an existing piece of software (such as a function or
library) when creating new software.</p>
</dd>
<dt><dfn id="solution-space">solution space</dfn></dt>
<dd>
<p>The possible solutions to a problem. This typically refers to an <a
href="section-14.html#optimisation-problem" class="term"
title="Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the *traveling salesman problem*, where visiting $n$ cities in some order has a cost, and the goal is to visit in the cheapest order.">optimisation
problem</a>, where some solutions are more desirable than others.</p>
</dd>
<dt><dfn id="solution-tree">solution tree</dfn></dt>
<dd>
<p>An ordering imposed on the set of solutions within a <a
href="section-14.html#solution-space" class="term"
title="The possible solutions to a problem. This typically refers to an optimisation problem, where some solutions are more desirable than others.">solution
space</a> in the form of a tree, typically derived from the order that
some algorithm would visit the solutions.</p>
</dd>
<dt><dfn id="sorted-list">sorted list</dfn></dt>
<dd>
<p>A <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>
where the records stored in the list are arranged so that their <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values are in ascending order. If the list uses an <a
href="section-14.html#array-based-list" class="term"
title="An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused.">array-based
list</a> implementation, then it can use <a
href="section-14.html#binary-search" class="term"
title="A standard recursive algorithm for finding the record with a given search key value within a sorted list. It runs in $O(\log n)$ time. At each step, look at the middle of the current sublist, and throw away the half of the records whose keys are either too small or too large.">binary
search</a> for a cost of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>.
But both insertion and deletion will be require
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
time.</p>
</dd>
<dt><dfn id="sorting-lower-bound">sorting lower bound</dfn></dt>
<dd>
<p>The lower bound for the <a href="section-14.html#problem"
class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
of <a href="section-14.html#sorting-problem" class="term"
title="Given a set of records $r_1$, $r_2$, ..., $r_n$ with key values $k_1$, $k_2$, ..., $k_n$, the sorting problem is to arrange the records into any order $s$ such that records $r_{s_1}$, $r_{s_2}$, ..., $r_{s_n}$ have keys obeying the property $k_{s_1} \leq k_{s_2} \leq ... \leq k_{s_n}$. In other words, the sorting problem is to arrange a set of records so that the values of their key fields are in non-decreasing order.">sorting</a>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>.
This is traditionally proved using a <a
href="section-14.html#decision-tree" class="term"
title="A theoretical construct for modeling the behaviour of algorithms. Each point at which the algorithm makes a decision (such as an if statement) is modeled by a branch in the tree that represents the algorithms behaviour. Decision trees can be used in lower bounds proofs, such as the proof that sorting requires $\Omega(n \log n)$ comparisons in the worst case.">decision
tree</a> model for sorting algorithms, and recognising that the minimum
depth of the decision tree for any sorting algorithm is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega(n \log n)</annotation></semantics></math>
since there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math>
permutations of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
input records to distinguish between during the sorting process.</p>
</dd>
<dt><dfn id="sorting-problem">sorting problem</dfn></dt>
<dd>
<p>Given a set of records
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>1</mn></msub><annotation encoding="application/x-tex">r_1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mn>2</mn></msub><annotation encoding="application/x-tex">r_2</annotation></semantics></math>,
...,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><mi>n</mi></msub><annotation encoding="application/x-tex">r_n</annotation></semantics></math>
with <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mn>1</mn></msub><annotation encoding="application/x-tex">k_1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mn>2</mn></msub><annotation encoding="application/x-tex">k_2</annotation></semantics></math>,
...,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mi>n</mi></msub><annotation encoding="application/x-tex">k_n</annotation></semantics></math>,
the sorting problem is to arrange the records into any order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
such that records
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><msub><mi>s</mi><mn>1</mn></msub></msub><annotation encoding="application/x-tex">r_{s_1}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><msub><mi>s</mi><mn>2</mn></msub></msub><annotation encoding="application/x-tex">r_{s_2}</annotation></semantics></math>,
...,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>r</mi><msub><mi>s</mi><mi>n</mi></msub></msub><annotation encoding="application/x-tex">r_{s_n}</annotation></semantics></math>
have keys obeying the property
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><msub><mi>s</mi><mn>1</mn></msub></msub><mo>≤</mo><msub><mi>k</mi><msub><mi>s</mi><mn>2</mn></msub></msub><mo>≤</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>≤</mo><msub><mi>k</mi><msub><mi>s</mi><mi>n</mi></msub></msub></mrow><annotation encoding="application/x-tex">k_{s_1} \leq k_{s_2} \leq ... \leq k_{s_n}</annotation></semantics></math>.
In other words, the sorting problem is to arrange a set of records so
that the values of their key fields are in non-decreasing order.</p>
</dd>
<dt><dfn id="space-complexity">space complexity</dfn></dt>
<dd>
<p>The <a href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an <a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
or <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
with a <a href="section-14.html#cost-model" class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a> that approximates memory/storage usage.</p>
</dd>
<dt><dfn id="space-time-tradeoff">space/time tradeoff</dfn></dt>
<dd>
<p>Many programs can be designed to either speed processing at the cost
of additional storage, or reduce storage at the cost of additional
processing time.</p>
</dd>
<dt><dfn id="sparse-graph">sparse graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
where the actual number of <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
is much less than the possible number of edges. Generally, this is
interpreted to mean that the <a href="section-14.html#degree"
class="term"
title="In graph terminology, the degree for a vertex is its number of neighbours. In a directed graph, the in degree is the number of edges directed into the vertex, and the out degree is the number of edges directed out of the vertex. In tree terminology, the degree for a node is its number of children.">degree</a>
for any <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertex</a> in the graph is
relatively low.</p>
</dd>
<dt><dfn id="sparse-matrix">sparse matrix</dfn></dt>
<dd>
<p>A matrix whose values are mostly zero. There are a number of data
structures that have been developed to store sparse matrices, with the
goal of reducing the amount of space required to represent it as
compared to simply using a regular matrix representation that stores a
value for every matrix position.</p>
</dd>
<dt><dfn id="spatial-application">spatial application</dfn></dt>
<dd>
<p>An application what has spatial aspects. In particular, an
application that stores records that need to be searched by
location.</p>
</dd>
<dt><dfn id="spatial-attribute">spatial attribute</dfn></dt>
<dd>
<p>An attribute of a record that has a position in space, such as the
coordinate. This is typically in two or more dimensions.</p>
</dd>
<dt><dfn id="spatial-data-structure">spatial data structure</dfn></dt>
<dd>
<p>A <a href="section-14.html#data-structure" class="term"
title="The implementation for an ADT.">data structure</a> designed to
support efficient processing when a <a
href="section-14.html#spatial-attribute" class="term"
title="An attribute of a record that has a position in space, such as the coordinate. This is typically in two or more dimensions.">spatial
attribute</a> is used as the key. In particular, a data structure that
supports efficient search by location, or finds all records within a
given region in two or more dimensions. Examples of spatial data
structures to store point data include the <a
href="section-14.html#bintree" class="term"
title="A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern.">bintree</a>,
the <a href="section-14.html#pr-quadtree" class="term"
title="A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree.">PR
quadtree</a> and the <a href="section-14.html#kd-tree" class="term"
title="A spatial data structure that uses a binary tree to store a collection of data records based on their (point) location in space. It uses the concept of a discriminator at each level to decide which single component of the multi-dimensional search key to branch on at that level. It uses a key-space decomposition, meaning that all data records in the left subtree of a node have a value on the corresponding discriminator that is less than that of the node, while all data records in the right subtree have a greater value. The bintree is the image-space decomposition analog of the kd tree.">kd
tree</a>.</p>
</dd>
<dt><dfn id="spatial-data">spatial data</dfn></dt>
<dd>
<p>Any object or record that has a position (in space).</p>
</dd>
<dt><dfn id="spatial">spatial</dfn></dt>
<dd>
<p>Referring to a position in space.</p>
</dd>
<dt><dfn id="spindle">spindle</dfn></dt>
<dd>
<p>The center of a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> that holds the <a href="section-14.html#platter" class="term"
title="In a disk drive, one of a series of flat disks that comprise the storage space for the drive. Typically, each surface (top and bottom) of each platter stores data, and each surface has its own I/O head.">platters</a>
in place.</p>
</dd>
<dt><dfn id="splay-tree">splay tree</dfn></dt>
<dd>
<p>A variant implementation for the <a href="section-14.html#bst"
class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>,
which differs from the standard BST in that it uses modified insert and
remove methods in order to keep the tree <a
href="section-14.html#balanced-tree" class="term"
title="A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree.">balanced</a>.
Similar to an <a href="section-14.html#avl-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a splay tree in that it uses the concept of rotations in the insert and remove operations.">AVL
tree</a> in that it uses the concept of <a
href="section-14.html#rotation" class="term"
title="In the AVL tree and splay tree, a rotation is a local operation performed on a node, its children, and its grandchildren that can result in reordering their relationship. The goal of performing a rotation is to make the tree more balanced.">rotations</a>
in the insert and remove operations. While a splay tree does not
guarentee that the tree is balanced, it does guarentee that a series of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
operations on the tree will have a total cost of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n \log n)</annotation></semantics></math>
cost, meaning that any given operation can be viewed as having <a
href="section-14.html#amortised-cost" class="term"
title="The average cost of an operation in a sufficiently long series of operations of the same kind. This is as opposed to considering every individual operation to independently have its own cost, which might lead to an overestimate for the total cost of the series. This can be made precise without considering averages by introducing potentials. In amortised analysis, gives rise to the notion of amortised complexity.">amortised
cost</a> of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(\log n)</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="splaying">splaying</dfn></dt>
<dd>
<p>The act of performing an <a
href="section-14.html#rebalancing-operation" class="term"
title="An operation performed on balanced search trees, such as the AVL tree or splay tree, for the purpose of keeping the tree height balanced.">rebalancing
operation</a> on a <a href="section-14.html#splay-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
tree</a>.</p>
</dd>
<dt><dfn id="stable">stable</dfn></dt>
<dd>
<p>A sorting algorithm is said to be stable if it does not change the
relative ordering of records with identical <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values.</p>
</dd>
<dt><dfn id="stack-frame">stack frame</dfn></dt>
<dd>
<p>Frame of data that pushed into and poped from call stack</p>
</dd>
<dt><dfn>stack variable</dfn></dt>
<dd>
<p>See <a href="section-14.html#local-variable" class="term"
title="A variable declared within a function or method. It exists only from the time when the function is called to when the function exits. When a function is suspended (due to calling another function), the function&#39;s local variables are stored in an activation record on the runtime stack.">local
variable</a></p>
</dd>
<dt><dfn id="stack">stack</dfn></dt>
<dd>
<p>A list-like structure in which elements may be inserted or removed
from only one end.</p>
</dd>
<dt><dfn id="stale-pointer">stale pointer</dfn></dt>
<dd>
<p>Within the context of a <a href="section-14.html#buffer-pool"
class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a> or <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, this means a <a href="section-14.html#reference"
class="term"
title="A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.)">reference</a>
to a <a href="section-14.html#buffer" class="term"
title="A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive.">buffer</a>
or memory location that is no longer valid. For example, a program might
make a memory request to a buffer pool, and be given a reference to the
buffer holding the requested data. Over time, due to inactivity, the
contents of this buffer might be flushed. If the program holding the
buffer reference then tries to access the contents of that buffer again,
then the data contents will have changed. The possibility for this to
occur depends on the design of the interface to the buffer pool system.
Some designs make this impossible to occur. Other designs make it
possible in an attempt to deliver greater performance.</p>
</dd>
<dt><dfn id="start-state">start state</dfn></dt>
<dd>
<p>In a <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a>, the designated state in which the machine will always
begin a computation.</p>
</dd>
<dt><dfn id="start-symbol">start symbol</dfn></dt>
<dd>
<p>In a <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>,
the designated <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminal</a>
that is the intial point for <a href="section-14.html#derivation"
class="term"
title="In formal languages, the process of executing a series of production rules from a grammar. A typical example of a derivation would be the series of productions executed to go from the start symbol to a given string.">deriving</a>
a string in the langauge.</p>
</dd>
<dt><dfn>state machine</dfn></dt>
<dd>
<p>See <a href="section-14.html#finite-state-machine" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
state machine</a></p>
</dd>
<dt><dfn id="state">state</dfn></dt>
<dd>
<p>The condition that something is in at some point in time. In
computing, this typically means the collective values of any existing
variables at some point in time. In an <a
href="section-14.html#automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">automata</a>,
a state is an abstract condition, possibly with associated information,
that is primarily defined in terms of the conditions that the automata
may transition from its present state to another state.</p>
</dd>
<dt><dfn id="static-scoping">static scoping</dfn></dt>
<dd>
<p>A synonym for <a href="section-14.html#lexical-scoping" class="term"
title="Within programming languages, the convention of allowing access to a variable only within the block of code in which the variable is defined. A synonym for static scoping.">lexical
scoping</a>.</p>
</dd>
<dt><dfn id="static">static</dfn></dt>
<dd>
<p>Something that is not changing (in contrast to <a
href="section-14.html#dynamic" class="term"
title="Something that is changes (in contrast to static). In computer programming, dynamic normally refers to something that happens at run time. For example, run-time analysis is analysis of the program&#39;s behaviour, as opposed to its (static) text or structure dynamic binding or dynamic memory allocation occurs at run time.">dynamic</a>).
In computer programming, static normally refers to something that
happens at compile time. For example, static analysis is analysis of the
program’s text or structure, as opposed to its run-time behaviour.
Static binding or static memory allocation occurs at compile time.</p>
</dd>
<dt><dfn id="strassens-algorithm">Strassen’s algorithm</dfn></dt>
<dd>
<p>A <a href="section-14.html#recursion" class="term"
title="The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion.">recursive</a>
algorithm for matrix multiplication. When multiplying two
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrices, this algorithm runs faster than the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math>
time required by the standard matrix multiplication algorithm.
Specifically, Strassen’s algorithm requires time
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mrow><msub><mrow><mi>log</mi><mo>&#8289;</mo></mrow><mn>2</mn></msub><mn>7</mn></mrow></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n^{\log_2 7})</annotation></semantics></math>
time. This is achieved by refactoring the sub-matrix multiplication and
addition operations so as to need only 7 sub-matrix multiplications
instead of 8, at a cost of additional sub-matrix addition operations.
Thus, while the asymptotic cost is lower, the constant factor in the
growth rate equation is higher. This makes Strassen’s algorithm
inefficient in practice unless the arrays being multiplied are rather
large. Variations on Strassen’s algorithm exist that reduce the number
of sub-matrix multiplications even futher at a cost of even more
sub-matrix additions.</p>
</dd>
<dt><dfn id="strategy">strategy</dfn></dt>
<dd>
<p>An approach to accomplish a task, often encapsulated as an algorithm.
Also the name for a <a href="section-14.html#design-pattern"
class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a> that separates the algorithm for performing a task from the
control for applying that task to each member of a collection. A good
example is a generic sorting function that takes a collection of records
(such as an <a href="section-14.html#array" class="term"
title="A data type that is used to store elements in consecutive memory locations and refers to them by an index.">array</a>)
and a “strategy” in the form of an algorithm that knows how to extract
the key from a record in the array. Only subtly different from the <a
href="section-14.html#visitor" class="term"
title="A design pattern where a traversal process is given a function (known as the visitor) that is applied to every object in the collection being traversed. For example, a generic tree or graph traversal might be designed such that it takes a function parameter, where that function is applied to each node.">visitor</a>
design pattern, where the difference is primarily one of intent rather
than syntax. The strategy design pattern is focused on encapsulating an
activity that is part of a larger process, so that different ways of
performing that activity can be substituted. The visitor design pattern
is focused on encapsulating an activity that will be performed on all
members of a collection so that completely different activities can be
substituted within a generic method that accesses all of the collection
members.</p>
</dd>
<dt><dfn id="stream">stream</dfn></dt>
<dd>
<p>The process of delivering content in a <a
href="section-14.html#serialisation" class="term"
title="The process of taking a data structure in memory and representing it as a sequence of bytes. This is sometimes done in order to transmit the data structure across a network or store the data structure in a stream, such as on disk. Deserialisation reconstructs the original data structure from the serialised representation.">serialised</a>
form.</p>
</dd>
<dt><dfn id="strict-partial-order">strict partial order</dfn></dt>
<dd>
<p>In set notation, a relation that is <a
href="section-14.html#irreflexive" class="term"
title="In set notation, binary relation $R$ on set $S$ is irreflexive if $aRa$ is never in the relation for any $a \in \mathbf{S}$.">irreflexive</a>,
<a href="section-14.html#antisymmetric" class="term"
title="In set notation, relation $R$ is antisymmetric if whenever $aRb$ and $bRa$, then $a = b$, for all $a, b \in \mathbf{S}$.">antisymmetric</a>,
and <a href="section-14.html#transitive" class="term"
title="In set notation, relation $R$ is transitive if whenever $aRb$ and $bRc$, then $aRc$, for all $a, b, c \in \mathbf{S}$.">transitive</a>.</p>
</dd>
<dt><dfn id="strong-induction">strong induction</dfn></dt>
<dd>
<p>An alternative formulation for the <a
href="section-14.html#induction-step" class="term"
title="Part of a proof by induction. In its simplest form, this is a proof of the implication that if the theorem holds for $n-1$, then it holds for $n$. As an alternative, see strong induction.">induction
step</a> in a <a href="section-14.html#proof-by-induction" class="term"
title="A mathematical proof technique similar to recursion. It is used to prove a parameterised theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction.">proof
by induction</a>. The induction step for strong induction is: If
<strong>Thrm</strong> holds for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>,</mo><mi>c</mi><mo>≤</mo><mi>k</mi><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k, c \leq k &lt; n</annotation></semantics></math>,
then <strong>Thrm</strong> holds for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="subclass">subclass</dfn></dt>
<dd>
<p>In <a href="section-14.html#object-oriented-programming-paradigm"
class="term"
title="An approach to problem-solving where all computations are carried out using objects.">object-oriented
programming</a>, any class within a <a
href="section-14.html#class-hierarchy" class="term"
title="In object-oriented programming, a set of classes and their interrelationships. One of the classes is the base class, and the others are subclasses that inherit either directly or indirectly from the base class.">class
hierarchy</a> that <a href="section-14.html#inherit" class="term"
title="In object-oriented programming, the process by which a subclass gains data members and methods from a base class.">inherits</a>
from some other class.</p>
</dd>
<dt><dfn id="subgraph">subgraph</dfn></dt>
<dd>
<p>A subgraph
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐒</mi><annotation encoding="application/x-tex">\mathbf{S}</annotation></semantics></math>
is formed from <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>
by selecting a <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐕</mi><mi>s</mi></msub><annotation encoding="application/x-tex">\mathbf{V}_s</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>’s
<a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> and a subset
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐄</mi><mi>s</mi></msub><annotation encoding="application/x-tex">\mathbf{E}_s</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>’s
<a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
such that for every edge
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mo>∈</mo><msub><mi>𝐄</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">e \in \mathbf{E}_s</annotation></semantics></math>,
both vertices of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>
are in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐕</mi><mi>s</mi></msub><annotation encoding="application/x-tex">\mathbf{V}_s</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="subset">subset</dfn></dt>
<dd>
<p>In set theory, a set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is a subset of a set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
or equivalently
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
is a <a href="section-14.html#superset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">superset</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
if all elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
are also elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="subtract-and-guess">subtract-and-guess</dfn></dt>
<dd>
<p>A technique for finding a <a
href="section-14.html#closed-form-solution" class="term"
title="An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence.">closed-form
solution</a> to a <a href="section-14.html#summation" class="term"
title="The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to $n$ can be written as $\sum_{i=1}^{n} i$.">summation</a>
or <a href="section-14.html#recurrence-relation" class="term"
title="A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, $F(n) = n*F(n-1)$.">recurrence
relation</a>.</p>
</dd>
<dt><dfn id="subtree">subtree</dfn></dt>
<dd>
<p>A subtree is a <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
of the nodes of a binary tree that includes some node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
of the tree as the subtree <a href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>
along with all the <a href="section-14.html#descendant" class="term"
title="In a tree, the set of all nodes that have a node $A$ as an ancestor are the descendants of $A$. In other words, all of the nodes that can be reached from $A$ by progressing downwards in tree. Another way to say it is: The children of $A$, their children, and so on.">descendants</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="successful-search">successful search</dfn></dt>
<dd>
<p>When searching for a <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value in a collection of records, we might find it. If so, we call this
a successful search. The alternative is an <a
href="section-14.html#unsuccessful-search" class="term"
title="When searching for a key value in a collection of records, we might not find it. If so, we call this an unsuccessful search. Usually we require that this means that no record in the collection actually has that key value (though a probabilistic algorithm for search might not require this to be true). The alternative to an unsuccessful search is a successful search.">unsuccessful
search</a>.</p>
</dd>
<dt><dfn id="summation">summation</dfn></dt>
<dd>
<p>The sum of costs for some <a href="section-14.html#function"
class="term"
title="In mathematics, a matching between inputs (the domain) and outputs (the range). In programming, a subroutine that takes input parameters and uses them to compute and return a value. In this case, it is usually considered bad practice for a function to change any global variables (doing so is called a side effect).">function</a>
applied to a range of parameter values. Often written using Sigma
notation. For example, the sum of the integers from 1 to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
can be written as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>i</mi></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{n} i</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="superset">superset</dfn></dt>
<dd>
<p>In set theory, a set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is a <a href="section-14.html#subset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">subset</a>
of a <a href="section-14.html#set" class="term"
title="A collection of distinguishable members or elements.">set</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
or equivalently
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
is a <a href="section-14.html#superset" class="term"
title="In set theory, a set $A$ is a subset of a set $B$, or equivalently $B$ is a superset of $A$, if all elements of $A$ are also elements of $B$.">superset</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
if all <a href="section-14.html#element" class="term"
title="One value or member in a set.">elements</a> of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
are also elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="symbol-table">symbol table</dfn></dt>
<dd>
<p>As part of a <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>,
the symbol table stores all of the identifiers in the program, along
with any necessary information needed about the identifier to allow the
compiler to do its job.</p>
</dd>
<dt><dfn id="symmetric-matrix">symmetric matrix</dfn></dt>
<dd>
<p>A square matrix that is equal to its <a
href="section-14.html#transpose" class="term"
title="In the context of linear algebra, the transpose of a matrix $A$ is another matrix $A^T$ created by writing the rows of $A$ as the columns of $A^T$. In the context of a self-organising list, transpose is a heuristic used to maintain the list. Under this heuristic, whenever a record is accessed it is moved one position closer to the front of the list.">transpose</a>.
Equivalently, for a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">i,j &lt; n</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo stretchy="false" form="prefix">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mi>A</mi><mo stretchy="false" form="prefix">[</mo><mi>j</mi><mo>,</mo><mi>i</mi><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">A[i, j] = A[j, i]</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="symmetric">symmetric</dfn></dt>
<dd>
<p>In set notation, relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
is symmetric if whenever
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">aRb</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>R</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">bRa</annotation></semantics></math>,
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>∈</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">a, b \in \mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="syntax-analysis">syntax analysis</dfn></dt>
<dd>
<p>A phase of <a href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compilation</a>
that accepts <a href="section-14.html#token" class="term"
title="The basic logical units of a program, as deterimined by lexical analysis. These are things like arithmetic operators, language keywords, variable or function names, or numbers.">tokens</a>,
checks if program is syntactically correct, and then generates a <a
href="section-14.html#parse-tree" class="term"
title="A tree that represents the syntactic structure of an input string, making it easy to compare against a grammar to see if it is syntactically correct.">parse
tree</a>.</p>
</dd>
<dt><dfn id="tail">tail</dfn></dt>
<dd>
<p>The end of a <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>.</p>
</dd>
<dt><dfn id="terminal">terminal</dfn></dt>
<dd>
<p>A specific character or string that appears in a <a
href="section-14.html#production-rule" class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production
rule</a>. In contrast to a <a href="section-14.html#non-terminal"
class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminal</a>,
which represents an abstract state in the production. Similar to a <a
href="section-14.html#literal" class="term"
title="In a Boolean expression, a literal is a Boolean variable or its negation. In the context of compilers, it is any constant value. Similar to a terminal.">literal</a>,
but this is the term more typically used in the context of a <a
href="section-14.html#compiler" class="term"
title="A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimisation, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute.">compiler</a>.</p>
</dd>
<dt><dfn id="theta-notation">Theta notation</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math>
notation is used to indicate that the <a
href="section-14.html#upper-bound" class="term"
title="An upper bound for a growth rate $f$ is any growth rate $g$ that is greater than or equal to it. Formally, there are constants $n_0 \geq 0$ and $C &gt; 0$ such that $f(n) \leq C g(n)$ for all $n \geq n_0$. We also write $f \in O(g)$ or slightly imprecisely $f(n) \in O(g(n))$ (this is big-$O$ notation). Usually, we are interested in finding an upper bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, an upper bound for an algorithm is an upper bound for the asymptotic complexity of the algorithm, the growth rate of its complexity. In practice, we are looking for the best possible upper bound that has a simple mathematical expression. For example, we may write $T(n) \in O(n^2)$ if $T$ is the (time) complexity of the algorithm to say that the complexity is quadratic, i.e. the asymptoptic complexity of the algorithm has as upper bound the growth rate given by squaring.">upper
bound</a> and <a href="section-14.html#lower-bound" class="term"
title="An lower bound for a growth rate $f$ is any growth rate $g$ that is less than or equal to it. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \geq c g(n)$ for all $n \geq n_0$. We also write $f \in \Omega(g)$ or slightly imprecisely $f(n) \in \Omega(g(n))$ (this is Omega notation). Usually, we are interested in finding a lower bound $g$ that has a simple expression compared to $f$, but is still sharp (there is not much room for improvement). In algorithm analysis, a lower bound for an algorithm is a lower bound for the asymptotic complexity of the algorithm, the growth rate of its complexity.">lower
bound</a> for an <a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
or <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
match.</p>
</dd>
<dt><dfn id="time-complexity">time complexity</dfn></dt>
<dd>
<p>The <a href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an <a href="section-14.html#algorithm" class="term"
title="A method or a process followed to solve a problem.">algorithm</a>
or <a href="section-14.html#problem" class="term"
title="A task to be performed. It is best thought of as a function or a mapping of inputs to outputs.">problem</a>
with a <a href="section-14.html#cost-model" class="term"
title="In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the complexity of the algorithm. By default, the cost model approximates the runtime of the program. To stress this, we also speak of time complexity. It is also possible to model other kinds of costs. In the case of memory/storage, we speak of space complexity. Looking at the growth rate of the complexity function tells us the asymptotic complexity of the algorithm. A cost model would be considered &#39;good&#39; if it yields predictions that conform to our understanding of reality.">cost
model</a> that approximates runtime.</p>
</dd>
<dt><dfn id="token">token</dfn></dt>
<dd>
<p>The basic logical units of a program, as deterimined by <a
href="section-14.html#lexical-analysis" class="term"
title="A phase of a compiler or interpreter responsible for reading in characters of the program or language and grouping them into tokens.">lexical
analysis</a>. These are things like arithmetic operators, language
keywords, variable or function names, or numbers.</p>
</dd>
<dt><dfn id="tombstone">tombstone</dfn></dt>
<dd>
<p>In <a href="section-14.html#hashing" class="term"
title="A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return a position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution.">hashing</a>,
a tombstone is used to mark a <a href="section-14.html#slot"
class="term" title="In hashing, a position in a hash table.">slot</a> in
the <a href="section-14.html#hash-table" class="term"
title="The data structure (usually an array) that stores data records for lookup using hashing.">hash
table</a> where a record has been deleted. Its purpose is to allow the
<a href="section-14.html#collision-resolution" class="term"
title="The outcome of a collision resolution policy.">collision
resolution</a> process to probe through that slot (so that records
further down the <a href="section-14.html#probe-sequence" class="term"
title="In hashing, the series of slots visited by the probe function during collision resolution.">probe
sequence</a> are not unreachable after deleting the record), while also
allowing the slot to be reused by a future insert operation.</p>
</dd>
<dt><dfn id="topological-sort">topological sort</dfn></dt>
<dd>
<p>The process of laying out the <a href="section-14.html#vertex"
class="term" title="Another name for a node in a graph.">vertices</a> of
a <a href="section-14.html#dag" class="term"
title="A graph with no cycles. Abbreviated as DAG. Note that a DAG is not necessarily a tree since a given node might have multiple parents.">DAG</a>
in a <a href="section-14.html#linear-order" class="term"
title="Another term for total order.">linear order</a> such that no
vertex
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
in the order is preceded by a vertex that can be reached by a (directed)
<a href="section-14.html#path" class="term"
title="In tree or graph terminology, a sequence of vertices $v_1, v_2, ..., v_n$ forms a path of length $n-1$ if there exist edges from $v_i$ to $v_{i+1}$ for $1 \leq i &lt; n$.">path</a>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
Usually the (directed) edges in the graph define a prerequisite system,
and the goal of the topological sort is to list the vertices in an order
such that no prerequisites are violated.</p>
</dd>
<dt><dfn id="total-order">total order</dfn></dt>
<dd>
<p>A binary relation on a set where every pair of distinct elements in
the set are <a href="section-14.html#comparable" class="term"
title="The concept that two objects can be compared to determine if they are equal or not, or to determine which one is greater than the other. In set notation, elements $x$ and $y$ of a set are comparable under a given relation $R$ if either $xRy$ or $yRx$. To be reliably compared for a greater/lesser relationship, the values being compared must belong to a total order. In programming, the property of a data type such that two elements of the type can be compared to determine if they are the same (a weaker version), or which of the two is larger (a stronger version). `Comparable` is also the name of an interface in Java that asserts a comparable relationship between objects within a class, and `.compareTo()` is the `Comparable` interface method that implements the actual comparison between two objects of the class.">comparable</a>
(that is, one can determine which of the two is greater than the
other).</p>
</dd>
<dt><dfn id="total-path-length">total path length</dfn></dt>
<dd>
<p>In a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>,
the sum of the <a href="section-14.html#level" class="term"
title="In a tree, all nodes of depth $d$ are at level $d$ in the tree. The root is the only node at level 0, and its depth is 0.">levels</a>
for each <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>.</p>
</dd>
<dt><dfn id="towers-of-hanoi-problem">Towers of Hanoi problem</dfn></dt>
<dd>
<p>A standard example of a recursive algorithm. The problem starts with
a stack of disks (each with unique size) stacked decreasing order on the
left pole, and two additional poles. The problem is to move the disks to
the right pole, with the constraints that only one disk can be moved at
a time and a disk may never be on top of a smaller disk. For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
disks, this problem requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math>
moves. The standard solution is to move
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>
disks to the middle pole, move the bottom disk to the right pole, and
then move the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math>
disks on the middle pole to the right pole.</p>
</dd>
<dt><dfn id="track-to-track-seek-time">track-to-track seek
time</dfn></dt>
<dd>
<p>Expected (average) time to perform a <a href="section-14.html#seek"
class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operation from a random <a href="section-14.html#track" class="term"
title="On a disk drive, a concentric circle representing all of the sectors that can be viewed by the I/O head as the disk rotates. The significance is that, for a given placement of the I/O head, the sectors on the track can be read without performing a (relatively expensive) seek operation.">track</a>
to an adjacent track. Thus, this can be viewed as the minimum possible
seek time for the <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. This is one of two metrics commonly provided by disk drive
vendors for disk drive performance, with the other being <a
href="section-14.html#average-seek-time" class="term"
title="Expected (average) time to perform a seek operation on a disk drive, assuming that the seek is between two randomly selected tracks. This is one of two metrics commonly provided by disk drive vendors for disk drive performance, with the other being track-to-track seek time.">average
seek time</a>.</p>
</dd>
<dt><dfn id="track">track</dfn></dt>
<dd>
<p>On a <a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>, a concentric circle representing all of the <a
href="section-14.html#sector" class="term"
title="A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes.">sectors</a>
that can be viewed by the <a href="section-14.html#i-o-head"
class="term"
title="On a disk drive (or similar device), the part of the machinery that actually reads data from the disk.">I/O
head</a> as the disk rotates. The significance is that, for a given
placement of the I/O head, the sectors on the track can be read without
performing a (relatively expensive) <a href="section-14.html#seek"
class="term"
title="On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access.">seek</a>
operation.</p>
</dd>
<dt><dfn id="trailer-node">trailer node</dfn></dt>
<dd>
<p>Commonly used in implementations for a <a
href="section-14.html#linked-list" class="term"
title="An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node.">linked
list</a> or related structure, this <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
follows the last element of the list. Its purpose is to simplify the
code implementation by reducing the number of special cases that must be
programmed for.</p>
</dd>
<dt><dfn id="transducer">transducer</dfn></dt>
<dd>
<p>A machine that takes an input and creates an output. A <a
href="section-14.html#turing-machine" class="term"
title="A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer.">Turing
machine</a> is an example of a transducer.</p>
</dd>
<dt><dfn id="transitive">transitive</dfn></dt>
<dd>
<p>In set notation, relation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
is transitive if whenever
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">aRb</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>R</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">bRc</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>R</mi><mi>c</mi></mrow><annotation encoding="application/x-tex">aRc</annotation></semantics></math>,
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>∈</mo><mi>𝐒</mi></mrow><annotation encoding="application/x-tex">a, b, c \in \mathbf{S}</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="transpose">transpose</dfn></dt>
<dd>
<p>In the context of linear algebra, the transpose of a matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
is another matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mi>T</mi></msup><annotation encoding="application/x-tex">A^T</annotation></semantics></math>
created by writing the rows of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
as the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mi>T</mi></msup><annotation encoding="application/x-tex">A^T</annotation></semantics></math>.
In the context of a <a href="section-14.html#self-organising-list"
class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>, transpose is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
used to maintain the list. Under this heuristic, whenever a record is
accessed it is moved one position closer to the front of the list.</p>
</dd>
<dt><dfn id="trap-state">trap state</dfn></dt>
<dd>
<p>In a <a href="section-14.html#fsa" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">FSA</a>,
any state that has all transitions cycle back to itself. Such a state
might be <a href="section-14.html#final-state" class="term"
title="A required element of any acceptor. When computation on a string ends in a final state, then the machine accepts the string. Otherwise the machine rejects the string.">final</a>.</p>
</dd>
<dt><dfn id="traversal">traversal</dfn></dt>
<dd>
<p>Any process for visiting all of the objects in a collection (such as
a <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
or <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>)
in some order.</p>
</dd>
<dt><dfn id="tree-traversal">tree traversal</dfn></dt>
<dd>
<p>A <a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
performed on a tree. Traditional tree traversals include <a
href="section-14.html#preorder-traversal" class="term"
title="In a binary tree, a traversal that first visits the root, then recursively visits the left child, then recursively visits the right child.">preorder</a>
and <a href="section-14.html#postorder-traversal" class="term"
title="In a binary tree, a traversal that first recursively visits the left child, then recursively visits the right child, and then visits the root.">postorder</a>
traversals for both <a href="section-14.html#binary-tree" class="term"
title="A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root.">binary</a>
and <a href="section-14.html#general-tree" class="term"
title="A tree in which any given node can have any number of children. This is in contrast to, for example, a binary tree where each node has a fixed number of children (some of which might be `null`). General tree nodes tend to be harder to implement for this reason.">general</a>
trees, and <a href="section-14.html#inorder-traversal" class="term"
title="In a binary tree, a traversal that first recursively visits the left child, then visits the root, and then recursively visits the right child. In a binary search tree, this traversal will enumerate the nodes in sorted order.">inorder
traversal</a> that is most appropriate for a <a
href="section-14.html#bst" class="term"
title="A binary tree that imposes the following constraint on its node values: The search key value for any node $A$ must be greater than the (key) values for all nodes in the left subtree of $A$, and less than the key values for all nodes in the right subtree of $A$. Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree.">BST</a>.</p>
</dd>
<dt><dfn id="tree">tree</dfn></dt>
<dd>
<p>A tree
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐓</mi><annotation encoding="application/x-tex">\mathbf{T}</annotation></semantics></math>
is a finite set of one or more <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">nodes</a>
such that there is one designated node
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>,
called the <a href="section-14.html#root" class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">root</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐓</mi><annotation encoding="application/x-tex">\mathbf{T}</annotation></semantics></math>.
If the set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>𝐓</mi><mo>−</mo><mo stretchy="false" form="prefix">{</mo><mi>R</mi><mo stretchy="false" form="postfix">}</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{T} -\{R\})</annotation></semantics></math>
is not empty, these nodes are partitioned into
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n &gt; 0</annotation></semantics></math>
<a href="section-14.html#disjoint-sets" class="term"
title="A collection of sets, any pair of which share no elements in common. A collection of disjoint sets partitions some objects such that every object is in exactly one of the disjoint sets.">disjoint
sets</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐓</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\mathbf{T}_0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐓</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\mathbf{T}_1</annotation></semantics></math>,
...,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>𝐓</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\mathbf{T}_{n-1}</annotation></semantics></math>,
each of which is a tree, and whose <a href="section-14.html#root"
class="term"
title="In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root.">roots</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>R</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">R_1, R_2, ..., R_n</annotation></semantics></math>,
respectively, are <a href="section-14.html#child" class="term"
title="In a tree, the set of nodes directly pointed to by a node $R$ are the children of $R$.">children</a>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.</p>
</dd>
<dt><dfn id="trie">trie</dfn></dt>
<dd>
<p>A form of <a href="section-14.html#search-tree" class="term"
title="A tree data structure that makes search by key value more efficient. A type of container, it is common to implement an index using a search tree. A good search tree implementation will guarentee that insertion, deletion, and search operations are all $O(\log n)$.">search
tree</a> where an internal node represents a split in the <a
href="section-14.html#key-space" class="term"
title="The range of values that a key value may take on.">key space</a>
at a predetermined location, rather than split based on the actual <a
href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
values seen. For example, a simple binary search trie for key values in
the range 0 to 1023 would store all records with key values less than
512 on the left side of the tree, and all records with key values equal
to or greater than 512 on the right side of the tree. A trie is always a
<a href="section-14.html#full-tree" class="term"
title="A binary tree is full if every node is either a leaf node or else it is an internal node with two non-empty children.">full
tree</a>. Folklore has it that the term comes from “retrieval”, and
should be pronounced as “try” (in contrast to “tree”, to distinguish the
differences in the space decomposition method of a search tree versus a
search trie). The term “trie” is also sometimes used as a synonym for
the <a href="section-14.html#alphabet-trie" class="term"
title="A trie data structure for storing variable-length strings. Level $i$ of the tree corresponds to the letter in position $i$ of the string. The root will have potential branches on each intial letter of string. Thus, all strings starting with &#39;a&#39; will be stored in the &#39;a&#39; branch of the tree. At the second level, such strings will be separated by branching on the second letter.">alphabet
trie</a>.</p>
</dd>
<dt><dfn id="truth-table">truth table</dfn></dt>
<dd>
<p>In symbolic logic, a table that contains as rows all possible
combinations of the boolean variables, with a column that shows the
outcome (true or false) for the expression when given that row’s truth
assignment for the boolean variables.</p>
</dd>
<dt><dfn id="tuple">tuple</dfn></dt>
<dd>
<p>In set notation, another term for a <a
href="section-14.html#sequence" class="term"
title="In set notation, a collection of elements with an order, and which may contain duplicate-valued elements. A sequence is also sometimes called a tuple or a vector.">sequence</a>.</p>
</dd>
<dt><dfn id="turing-machine">Turing machine</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#finite-automata" class="term"
title="Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, deterministic finite automata.">finite
automata</a> that, while simple to define completely, is capable of
performing any computation that can be performed by any known
computer.</p>
</dd>
<dt><dfn id="turing-acceptable">Turing-acceptable</dfn></dt>
<dd>
<p>A language is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>u</mi><mi>r</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>−</mo><mi>a</mi><mi>c</mi><mi>c</mi><mi>e</mi><mi>p</mi><mi>t</mi><mi>a</mi><mi>b</mi><mi>l</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">Turing-acceptable</annotation></semantics></math>
if there is some <a href="section-14.html#turing-machine" class="term"
title="A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer.">Turing
machine</a> that <a href="section-14.html#accept" class="term"
title="When a finite automata executes on a string and terminates in an accepting state, it is said to accept the string. The finite automata is said to accept the language that consists of all strings for which the finite automata completes execution in an accepting state.">accepts</a>
it. That is, the machine will halt in an accepting configuration if the
string is in the language, and go into a <a
href="section-14.html#hanging-configuration" class="term"
title="A hanging configuration occurs in a Turing machine when the I/O head moves to the left from the left-most square of the tape, or when the machine goes into an infinite loop.">hanging
configuration</a> if the string is not in the language.</p>
</dd>
<dt><dfn id="turing-computable-function">Turing-computable
function</dfn></dt>
<dd>
<p>Any function for which there exists a Turing machine that can perform
the necessary work to compute the function.</p>
</dd>
<dt><dfn id="turing-decidable">Turing-decidable</dfn></dt>
<dd>
<p>A language is Turing-decidable if there exists a Turing machine that
can clearly indicate for every string whether that string is in the
language or not. Every Turing-decidable language is also
Turing-acceptable, because the Turing machine that can decide if the
string is in the language can be modified to go into a <a
href="section-14.html#hanging-configuration" class="term"
title="A hanging configuration occurs in a Turing machine when the I/O head moves to the left from the left-most square of the tape, or when the machine goes into an infinite loop.">hanging
configuration</a> if the string is not in the language.</p>
</dd>
<dt><dfn id="two-colouring">two-colouring</dfn></dt>
<dd>
<p>An assignment from two colours to regions in an image such that no
two regions sharing a side have the same colour.</p>
</dd>
<dt><dfn id="type">type</dfn></dt>
<dd>
<p>A collection of values.</p>
</dd>
<dt><dfn id="unary-notation">unary notation</dfn></dt>
<dd>
<p>A way to represent <a href="section-14.html#natural-numbers"
class="term" title="Zero and the positive integers.">natural
numbers</a>, where the value of zero is represented by the empty string,
and the value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is represented by a series of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
marks.</p>
</dd>
<dt><dfn>uncountable</dfn></dt>
<dd>
<p>See <a href="section-14.html#uncountably-infinite" class="term"
title="An infinite set is uncountably infinite if there does not exist any mapping from it to the set of integers. This is often proved using a diagonalisation argument. The real numbers is an example of an uncountably infinite set.">uncountably
infinite</a></p>
</dd>
<dt><dfn id="uncountably-infinite">uncountably infinite</dfn> (<dfn
id="uncountable">uncountable</dfn>)</dt>
<dd>
<p>An infinite set is uncountably infinite if there does not exist any
mapping from it to the set of integers. This is often proved using a <a
href="section-14.html#diagonalisation-argument" class="term"
title="A proof technique for proving that a set is uncountably infinite. The approach is to show that, no matter what order the elements of the set are put in, a new element of the set can be constructed that is not in that ordering. This is done by changing the $i$ th value or position of the element to be different from that of the $i$ th element in the proposed ordering.">diagonalisation
argument</a>. The real numbers is an example of an uncountably infinite
set.</p>
</dd>
<dt><dfn id="underflow">underflow</dfn></dt>
<dd>
<p>The condition where the amount of data stored in an entity has
dropped below some minimum threshold. For example, a node in a <a
href="section-14.html#b-tree" class="term"
title="A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (hence some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B+ tree.">B-tree</a>
is required to be at least half full. If a record deletion causes the
node to be less than half full, then it is in a condition of underflow,
and something has to be done to correct this.</p>
</dd>
<dt><dfn id="undirected-edge">undirected edge</dfn></dt>
<dd>
<p>An <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
that connects two <a href="section-14.html#vertex" class="term"
title="Another name for a node in a graph.">vertices</a> with no
direction between them. Many graph representations will represent such
an edge with two <a href="section-14.html#directed-edge" class="term"
title="An edge that goes from vertex to another. In contrast, an undirected edge simply links to vertices without a direction.">directed
edges</a>.</p>
</dd>
<dt><dfn id="undirected-graph">undirected graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
whose <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
do not have a direction.</p>
</dd>
<dt><dfn id="uninitialised">uninitialised</dfn></dt>
<dd>
<p>Uninitialised variable means it has no initial value.</p>
</dd>
<dt><dfn id="union-find">UNION/FIND</dfn></dt>
<dd>
<p>A process for mainining a collection of disjoint sets. The <a
href="section-14.html#find" class="term"
title="One half of the UNION/FIND algorithm for managing disjoint sets. It is the process of moving upwards in a tree to find the tree&#39;s root.">FIND</a>
operation determines which disjoint set a given object resides in, and
the <a href="section-14.html#union" class="term"
title="One half of the UNION/FIND algorithm for managing disjoint sets. It is the process of merging two trees that are represented using the parent pointer representation by making the root for one of the trees set its parent pointer to the root of the other tree.">UNION</a>
operation combines two disjoint sets when it is determined that they are
members of the same <a href="section-14.html#equivalence-class"
class="term"
title="An equivalence relation can be used to partition a set into equivalence classes.">equivalence
class</a> under some <a href="section-14.html#equivalence-relation"
class="term"
title="Relation $R$ is an equivalence relation on set $\mathbf{S}$ if it is reflexive, symmetric, and transitive.">equivalence
relation</a>.</p>
</dd>
<dt><dfn id="union">UNION</dfn></dt>
<dd>
<p>One half of the <a href="section-14.html#union-find" class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
algorithm for managing <a href="section-14.html#disjoint-sets"
class="term"
title="A collection of sets, any pair of which share no elements in common. A collection of disjoint sets partitions some objects such that every object is in exactly one of the disjoint sets.">disjoint
sets</a>. It is the process of merging two trees that are represented
using the <a href="section-14.html#parent-pointer-representation"
class="term"
title="For trees, a node implementation where each node stores only a pointer to its parent, rather than to its children. This makes it easy to go up the tree toward the root, but not down the tree toward the leaves. This is most appropriate for solving the UNION/FIND problem.">parent
pointer representation</a> by making the root for one of the trees set
its parent pointer to the root of the other tree.</p>
</dd>
<dt><dfn id="unit-production">unit production</dfn></dt>
<dd>
<p>A unit production is a <a href="section-14.html#production"
class="term"
title="A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree.">production</a>
in a <a href="section-14.html#grammar" class="term"
title="A formal definition for what strings make up a language, in terms of a set of production rules.">grammar</a>
of the form
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>→</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \rightarrow B</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>∈</mo></mrow><annotation encoding="application/x-tex">A, B \in</annotation></semantics></math>
the set of <a href="section-14.html#non-terminal" class="term"
title="In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation.">non-terminals</a>
for the grammar. Any grammar with unit productions can be rewritten to
remove them.</p>
</dd>
<dt><dfn id="unsolvable-problem">unsolvable problem</dfn></dt>
<dd>
<p>A problem that can proved impossible to solve on a computer. The
classic example is the <a href="section-14.html#halting-problem"
class="term"
title="The halting problem is to answer this question: Given a computer program $P$ and an input $I$, will program $P$ halt when executed on input $I$? This problem has been proved impossible to solve in the general case. Thus, it is an example of an unsolvable problem.">halting
problem</a>.</p>
</dd>
<dt><dfn id="unsorted-list">unsorted list</dfn></dt>
<dd>
<p>A <a href="section-14.html#list" class="term"
title="A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that &#39;ordered&#39; in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, &#39;ordered&#39; does not mean &#39;sorted&#39;).">list</a>
where the records stored in the list can appear in any order (as opposed
to a <a href="section-14.html#sorted-list" class="term"
title="A list where the records stored in the list are arranged so that their key values are in ascending order. If the list uses an array-based list implementation, then it can use binary search for a cost of $O(\log n)$. But both insertion and deletion will be require $O(n)$ time.">sorted
list</a>). An unsorted list can support efficient
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>)
insertion time (since you can put the record anywhere convenient), but
requires
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>
time for both search and and deletion.</p>
</dd>
<dt><dfn id="unsuccessful-search">unsuccessful search</dfn></dt>
<dd>
<p>When searching for a <a href="section-14.html#key" class="term"
title="A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key.">key</a>
value in a collection of records, we might not find it. If so, we call
this an unsuccessful search. Usually we require that this means that no
record in the collection actually has that key value (though a <a
href="section-14.html#probabilistic-algorithm" class="term"
title="A form of randomised algorithm that might yield an incorrect result, or that might fail to produce a result.">probabilistic
algorithm</a> for search might not require this to be true). The
alternative to an unsuccessful search is a <a
href="section-14.html#successful-search" class="term"
title="When searching for a key value in a collection of records, we might find it. If so, we call this a successful search. The alternative is an unsuccessful search.">successful
search</a>.</p>
</dd>
<dt><dfn id="unvisited">unvisited</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
algorithms, this refers to a node that has not been processed at the
current point in the algorithm. This information is typically maintained
by using a <a href="section-14.html#mark-array" class="term"
title="It is typical in graph algorithms that there is a need to track which nodes have been visited at some point in the algorithm. An array of bits or values called the mark array is often maintained for this purpose.">mark
array</a>.</p>
</dd>
<dt><dfn id="upper-bound">upper bound</dfn></dt>
<dd>
<p>An upper bound for a <a href="section-14.html#growth-rate"
class="term"
title="The rate at which a function grows. How quickly the function grows when its input grows. Also called its *order of growth*. A function $f$ has growth rate bounded by a function $g$ if the values of $f$ are eventually bounded by those of $g$ up to some constant factor. We often shorten this (somewhat confusingly) by saying that $f$ has growth rate $g$ or that $f$ has order of growth $g$. Formally, there are constants $n_0 \geq 0$ and $c &gt; 0$ such that $f(n) \leq c g(n)$ for all $n \geq n_0$. We then say that $f$ has growth rate less or equal that of $g$ and write $f \in O(g)$ (big-$O$ notation). This defines the preorder of growth rates. In algorithm analysis, we sometimes speak of the growth rate of an algorithm. By that, we mean the growth rate of the complexity of the algorithm, the rate at which the cost of the algorithm grows as the size of its input grows. This is also called the asymptotic complexity of that algorithm.">growth
rate</a>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>
is any growth rate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
that is greater than or equal to it. Formally, there are constants
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mn>0</mn></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">n_0 \geq 0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">C &gt; 0</annotation></semantics></math>
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>≤</mo><mi>C</mi><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \leq C g(n)</annotation></semantics></math>
for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≥</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">n \geq n_0</annotation></semantics></math>.
We also write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f \in O(g)</annotation></semantics></math>
or slightly imprecisely
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(n) \in O(g(n))</annotation></semantics></math>
(this is <a href="section-14.html#big-o-notation" class="term"
title="For growth rates $f$ and $g$, we write $f \in O(g)$ to say that $g$ is an upper bound for $f$. The notation can be made sense of by defining $O(g)$ as the set of functions with growth rate less than or equal to that of $g$. The notation is often somewhat imprecisely used as $f(n) \in O(g(n))$ or even $f(n) = O(g(n))$.">big-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>O</mi><annotation encoding="application/x-tex">O</annotation></semantics></math>
notation</a>).</p>
<p>Usually, we are interested in finding an upper bound
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>
that has a simple expression compared to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>,
but is still sharp (there is not much room for improvement).</p>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, an upper bound for an algorithm is an upper bound for the
<a href="section-14.html#asymptotic-complexity" class="term"
title="The growth rate or order of growth of the complexity of an algorithm or problem. There are several independent categories of qualifiers for (asymptotic) complexity: - time complexity (default), space complexity, complexity in some other cost, - worst case (default), average case, best case, - whether to use amortised complexity.">asymptotic
complexity</a> of the algorithm, the growth rate of its <a
href="section-14.html#complexity" class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>.
In practice, we are looking for the best possible upper bound that has a
simple mathematical expression. For example, we may write
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false" form="prefix">(</mo><mi>n</mi><mo stretchy="false" form="postfix">)</mo><mo>∈</mo><mi>O</mi><mo stretchy="false" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">T(n) \in O(n^2)</annotation></semantics></math>
if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>
is the (time) complexity of the algorithm to say that the complexity is
quadratic, i.e. the asymptoptic complexity of the algorithm has as upper
bound the growth rate given by squaring.</p>
</dd>
<dt><dfn id="value-parameter">value parameter</dfn></dt>
<dd>
<p>A <a href="section-14.html#parameter" class="term"
title="The values making up an input to a function.">parameter</a> that
has been <a href="section-14.html#pass-by-value" class="term"
title="A copy of a variable is passed to the called function. So, any modifications will not affect the original variable.">passed
by value</a>. Changing such a parameter inside the function or method
will not affect the value of the calling parameter.</p>
</dd>
<dt><dfn id="variable-length-coding">variable-length coding</dfn></dt>
<dd>
<p>Given a collection of objects, a variable-length coding scheme
assigns a code to each object in the collection using codes that can be
of different lengths. Typically this is done in a way such that the
objects that are most likely to be used have the shortest codes, with
the goal of minimising the total space needed to represent a sequence of
objects, such as when representing the characters in a document. <a
href="section-14.html#huffman-codes" class="term"
title="The codes given to a collection of letters (or other symbols) through the process of Huffman coding. Huffman coding uses a Huffman coding tree to generate the codes. The codes can be of variable length, such that the letters which are expected to appear most frequently are shorter. Huffman coding is optimal whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message.">Huffman
coding</a> is an example of a variable-length coding scheme. This is in
contrast to <a href="section-14.html#fixed-length-coding" class="term"
title="Given a collection of objects, a fixed-length coding scheme assigns a code to each object in the collection using codes that are all of the same length. Standard ASCII and Unicode representations for characters are both examples of fixed-length coding schemes. This is in contrast to variable-length coding.">fixed-length
coding</a>.</p>
</dd>
<dt><dfn id="vector">vector</dfn></dt>
<dd>
<p>In set notation, another term for a <a
href="section-14.html#sequence" class="term"
title="In set notation, a collection of elements with an order, and which may contain duplicate-valued elements. A sequence is also sometimes called a tuple or a vector.">sequence</a>.
As a data structure, the term vector usually used as a snyonym for a <a
href="section-14.html#dynamic-array" class="term"
title="Arrays, once allocated, are of fixed size. A dynamic array puts an interface around the array so as to appear to allow the array to grow and shrink in size as necessary. Typically this is done by allocating a new copy, copying the contents of the old array, and then returning the old array to free store. If done correctly, the amortised cost for dynamically resizing the array can be made constant. In some programming languages such as Java, the term vector is used as a synonym for dynamic array.">dynamic
array</a>.</p>
</dd>
<dt><dfn id="vertex">vertex</dfn></dt>
<dd>
<p>Another name for a <a href="section-14.html#node" class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>
in a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.</p>
</dd>
<dt><dfn id="virtual-memory">virtual memory</dfn></dt>
<dd>
<p>A memory management technique for making relatively fast but small
memory appear larger to the program. The large “virtual” data space is
actually stored on a relatively slow but large <a
href="section-14.html#backing-storage" class="term"
title="In the context of a caching system or buffer pool, backing storage is the relatively large but slower source of data that needs to be cached. For example, in a virtual memory, the disk drive would be the backing storage. In the context of a web browser, the Internet might be considered the backing storage.">backing
storage</a> device, and portions of the data are copied into the
smaller, faster memory as needed by use of a <a
href="section-14.html#buffer-pool" class="term"
title="A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used.">buffer
pool</a>. A common example is to use <a href="section-14.html#ram"
class="term"
title="Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer&#39;s memory hierarchy.">RAM</a>
to manage access to a large virtual space that is actually stored on a
<a href="section-14.html#disk-drive" class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a>. The programmer can implement a program as though the entire
data content were stored in RAM, even if that is larger than the
physical RAM available making it easier to implement.</p>
</dd>
<dt><dfn id="visit">visit</dfn></dt>
<dd>
<p>During the process of a <a href="section-14.html#traversal"
class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
on a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
or <a href="section-14.html#tree" class="term"
title="A tree $\mathbf{T}$ is a finite set of one or more nodes such that there is one designated node $R$, called the root of $\mathbf{T}$. If the set $(\mathbf{T} -{R})$ is not empty, these nodes are partitioned into $n &gt; 0$ disjoint sets $\mathbf{T}_0$, $\mathbf{T}_1$, ..., $\mathbf{T}_{n-1}$, each of which is a tree, and whose roots $R_1, R_2, ..., R_n$, respectively, are children of $R$.">tree</a>
the action that takes place on each <a href="section-14.html#node"
class="term"
title="The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices.">node</a>.</p>
</dd>
<dt><dfn id="visited">visited</dfn></dt>
<dd>
<p>In <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
algorithms, this refers to a node that has previously been processed at
the current point in the algorithm. This information is typically
maintained by using a <a href="section-14.html#mark-array" class="term"
title="It is typical in graph algorithms that there is a need to track which nodes have been visited at some point in the algorithm. An array of bits or values called the mark array is often maintained for this purpose.">mark
array</a>.</p>
</dd>
<dt><dfn id="visitor">visitor</dfn></dt>
<dd>
<p>A <a href="section-14.html#design-pattern" class="term"
title="An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly.">design
pattern</a> where a <a href="section-14.html#traversal" class="term"
title="Any process for visiting all of the objects in a collection (such as a tree or graph) in some order.">traversal</a>
process is given a function (known as the visitor) that is applied to
every object in the collection being traversed. For example, a generic
tree or graph traversal might be designed such that it takes a function
parameter, where that function is applied to each node.</p>
</dd>
<dt><dfn id="volatile">volatile</dfn></dt>
<dd>
<p>In the context of computer memory, this refers to a memory that loses
all stored information when the power is turned off.</p>
</dd>
<dt><dfn id="weight">weight</dfn></dt>
<dd>
<p>A cost or distance most often associated with an <a
href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edge</a>
in a <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>.</p>
</dd>
<dt><dfn id="weighted-graph">weighted graph</dfn></dt>
<dd>
<p>A <a href="section-14.html#graph" class="term"
title="A graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ consists of a set of vertices $\mathbf{V}$ and a set of edges $\mathbf{E}$, such that each edge in $\mathbf{E}$ is a connection between a pair of vertices in $\mathbf{V}$.">graph</a>
whose <a href="section-14.html#edge" class="term"
title="The connection that links two nodes in a tree, linked list, or graph.">edges</a>
each have an associated <a href="section-14.html#weight" class="term"
title="A cost or distance most often associated with an edge in a graph.">weight</a>
or cost.</p>
</dd>
<dt><dfn id="weighted-path-length">weighted path length</dfn></dt>
<dd>
<p>Given a tree, and given a <a href="section-14.html#weight"
class="term"
title="A cost or distance most often associated with an edge in a graph.">weight</a>
for each leaf in the tree, the weighted path length for a leaf is its
weight times its <a href="section-14.html#depth" class="term"
title="The depth of a node $M$ in a tree is the length of the path from the root of the tree to $M$.">depth</a>.</p>
</dd>
<dt><dfn id="weighted-union-rule">weighted union rule</dfn></dt>
<dd>
<p>When merging two disjoint sets using the <a
href="section-14.html#union-find" class="term"
title="A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation.">UNION/FIND</a>
algorithm, the weighted union rule is used to determine which subtree’s
root points to the other. The root of the subtree with fewer nodes will
be set to point to the root of the subtree with more nodes. In this way,
the average depth of nodes in the resulting tree will be less than if
the assignment had been made in the other direction.</p>
</dd>
<dt><dfn id="working-memory">working memory</dfn></dt>
<dd>
<p>The portion of <a href="section-14.html#main-memory" class="term"
title="The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer&#39;s memory hierarchy.">main
memory</a> available to an algorithm for its use. Typically refers to
main memory made available to an algorithm that is operating on large
amounts of data stored in <a href="section-14.html#peripheral-storage"
class="term"
title="Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive.">peripheral
storage</a>, the working memory represents space that can hold some
subset of the total data being processed.</p>
</dd>
<dt><dfn id="worst-case">worst case</dfn></dt>
<dd>
<p>In <a href="section-14.html#algorithm-analysis" class="term"
title="A method for estimating the efficiency of an algorithm or computer program by identifying its asymptotic complexity, the growth rate of its complexity function. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing.">algorithm
analysis</a>, specifically <a href="section-14.html#complexity"
class="term"
title="After fixing a cost model for a problem, we can calculate the complexity function of an algorithm. This sends an input size $n$ to the cost of running the algorithm on input of that size. For each fixed $n$, we consider only the worst-case input of size $n$. This defines the worst-case complexity of the algorithm. There is also the average-case and best-case complexity, which are defined similarly. We speak of time complexity, space complexity, etc. depending on what kind of cost our cost model captures. Here, time refers to runtime and space refers to memory/storage. The case of time complexity is the default, so we omit the word &#39;time&#39;.">complexity</a>
of an algorithm, the <a href="section-14.html#problem-instance"
class="term"
title="A specific selection of values for the parameters to a problem. In other words, a specific set of inputs to a problem. A given problem instance has a size under some cost model.">problem
instance</a> from among all problem instances for a given input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
that has the greatest cost.</p>
<p>Every input size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
has its own worst case. We <strong>never</strong> consider the worst
case as removed from input size.</p>
</dd>
<dt><dfn id="worst-fit">worst fit</dfn></dt>
<dd>
<p>In a <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
manager</a>, worst fit is a <a href="section-14.html#heuristic"
class="term"
title="A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution.">heuristic</a>
for deciding which <a href="section-14.html#free-block" class="term"
title="A block of unused space in a memory pool.">free block</a> to use
when allocating memory from a <a href="section-14.html#memory-pool"
class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a>. Worst fit will always allocate from the largest free block.
The rationale is that this will be the method least likely to cause <a
href="section-14.html#external-fragmentation" class="term"
title="A condition that arises when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests.">external
fragmentation</a> in the form of small, unusable memory blocks. The
disadvantage is that it tends to eliminate the availability of large
freeblocks needed for unusually large requests.</p>
</dd>
<dt><dfn id="zigzig">zigzig</dfn></dt>
<dd>
<p>A type of <a href="section-14.html#rebalancing-operation"
class="term"
title="An operation performed on balanced search trees, such as the AVL tree or splay tree, for the purpose of keeping the tree height balanced.">rebalancing
operation</a> used by <a href="section-14.html#splay-tree" class="term"
title="A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL tree in that it uses the concept of rotations in the insert and remove operations. While a splay tree does not guarentee that the tree is balanced, it does guarentee that a series of $n$ operations on the tree will have a total cost of $O(n \log n)$ cost, meaning that any given operation can be viewed as having amortised cost of $O(\log n)$.">splay
trees</a>.</p>
</dd>
<dt><dfn id="zipf-distribution">Zipf distribution</dfn></dt>
<dd>
<p>A data distribution that follows Zipf’s law, an emprical observation
that many types of data studied in the physical and social sciences
follow a power law probability distribution. That is, the frequency of
any record in the data collection is inversely proportional to its rank
when the collection is sorted by frequency. Thus, the most frequently
appearing record has a frequency much higher than the next most
frequently appearing record, which in turn has a frequency much higher
than the third (but with ratio slightly lower than that for the first
two records) and so on. The <a href="section-14.html#80-20-rule"
class="term"
title="Given a typical application where there is a collection of records and a series of search operations for records, the 80/20 rule is an empirical observation that 80% of the record accessess typically go to 20% of the records. The exact values varies between data collections, and is related to the concept of locality of reference.">80/20
rule</a> is a casual characterisation of a Zipf distribution. Adherence
to a Zipf distribution is important to the successful operation of a <a
href="section-14.html#caching" class="term"
title="The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool.">cache</a>
or <a href="section-14.html#self-organising-list" class="term"
title="A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organising heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organising lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation).">self-organising
list</a>.</p>
</dd>
<dt><dfn id="zone">zone</dfn></dt>
<dd>
<p>In <a href="section-14.html#memory-manager" class="term"
title="Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data.">memory
management</a>, the concept that different parts of the <a
href="section-14.html#memory-pool" class="term"
title="Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager.">memory
pool</a> are handled in different ways. For example, some of the memory
might be handled by a simple <a href="section-14.html#freelist"
class="term"
title="A simple and faster alternative to using free store when the objects being dynamically allocated are all of the same size (and thus are interchangeable). Typically implemented as a linked stack, released objects are put on the front of the freelist. When a request is made to allocate an object, the freelist is checked first and it provides the object if possible. If the freelist is empty, then a new object is allocated from free store.">freelist</a>,
while other portions of the memory pool might be handled by a <a
href="section-14.html#sequential-fit" class="term"
title="In a memory manager, the process of searching the memory pool for a free block large enough to service a memory request, possibly reserving the remaining space as a free block. Examples are first fit, circular first fit, best fit, and worst fit.">sequential
fit</a> memory manager. On a <a href="section-14.html#disk-drive"
class="term"
title="An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer&#39;s memory hierarchy.">disk
drive</a> the concept of a zone relates to the fact that there are
limits to the maximum data density, combined with the fact that the need
for the same angular distance to be used for a sector in each track
means that tracks further from the center of the disk will become
progressively less dense. A zone in this case is a series of adjacent
tracks whose data density is set by the maximum density of the innermost
track of that zone. The next zone can then reset the data density for
its innermost track, thereby gaining more total storage space while
preserving angular distance for each sector.</p>
</dd>
</dl>
</div>
</section>
</main>

<footer>
<nav class="sitenav">
<div class="navlink">
<a href="section-13.11.html" class="navbutton">&lt;&lt;</a>
<a href="section-13.11.html" accesskey="p" rel="previous">Review questions</a>
</div>
<div class="navlink">
<a href="section-15.html" accesskey="n" rel="next">Bibliography</a>
<a href="section-15.html" class="navbutton">&gt;&gt;</a>
</div>
</nav>
</footer>

</body>
</html>

